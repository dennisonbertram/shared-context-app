This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

## Additional Info

# Directory Structure
```
architecture/
  architecture-async-processing-2025-01-16.md
  architecture-global-context-network-2025-01-16.md
  architecture-hooks-event-capture-2025-01-16.md
  architecture-learning-extraction-2025-01-16.md
  architecture-mcp-server-2025-01-16.md
  architecture-subagent-system-2025-01-16.md
  architecture-testing-harness-2025-01-16.md
  INDEX.md
decisions/
  decision-async-processing-model-2025-01-16.md
  decision-claude-testing-harness-2025-01-16.md
  decision-data-licensing-consent-2025-01-16.md
  decision-data-model-schema-versioning-2025-01-16.md
  decision-global-sharing-architecture-2025-01-16.md
  decision-observability-cost-slo-governance-2025-01-16.md
  decision-pii-detection-strategy-2025-01-16.md
  decision-sanitize-before-storage-2025-01-16.md
  decision-security-provenance-2025-01-16.md
  decision-subagent-driven-development-2025-01-16.md
  decision-token-rewards-sybil-resistance-2025-01-16.md
  decision-use-claude-hooks-2025-01-16.md
  decision-use-sqlite-2025-01-16.md
  INDEX.md
guides/
  guide-claude-agent-sdk-integration-2025-01-16.md
  guide-database-setup-2025-01-16.md
  guide-phase-0-foundation-setup-2025-01-16.md
  guide-phase-1-hook-development-2025-01-16.md
  guide-tdd-workflow-2025-01-16.md
  guide-testing-harness-usage-2025-01-16.md
  guide-using-subagents-2025-01-16.md
  INDEX.md
plans/
  INDEX.md
  plan-global-context-network-mvp-2025-01-16.md
  plan-implementation-roadmap-2025-01-16.md
  plan-iterative-build-strategy-2025-01-16.md
  plan-original-user-vision-2025-01-16.md
  plan-phase-0-tasks-2025-01-16.md
  plan-phase-1-tasks-2025-01-16.md
  plan-phase-2-tasks-2025-01-16.md
  plan-phase-3-tasks-2025-01-16.md
  plan-phase-4-tasks-2025-01-16.md
  plan-phase-5-tasks-2025-01-16.md
  plan-phase-6-tasks-2025-01-16.md
  plan-phase-7-tasks-2025-01-16.md
  plan-sanitization-architecture-doc-2025-01-16.md
  plan-subagent-workflow-2025-01-16.md
reference/
  INDEX.md
  reference-claude-agent-sdk-api-2025-01-16.md
  reference-database-schema-2025-01-16.md
  reference-hook-configuration-2025-01-16.md
  reference-subagent-types-2025-01-16.md
  reference-testing-strategy-2025-01-16.md
reviews/
  gpt5-sanitization-plan-review-request.md
  review-of-reviews-analysis-2025-01-16.md
INDEX.md
STANDARDS.md
```

# Files

## File: architecture/architecture-async-processing-2025-01-16.md
````markdown
---
title: Async Processing Architecture
category: architecture
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [async, job-queue, workers, reliability, performance]
references:
  - docs/STANDARDS.md
  - docs/decisions/decision-async-processing-model-2025-01-16.md
  - docs/reference/reference-database-schema-2025-01-16.md
schema_version: "1.0.0"
---

# Async Processing Architecture

> Complete async processing system with SQLite-based job queue, worker model, and reliability guarantees

---

## Overview

The Global Context Network requires async processing to maintain responsive user experience while handling slow operations. This document defines the complete async architecture, aligned with canonical standards from STANDARDS.md and ADR-006.

**Core Principle**: Never block the user. All slow operations (sanitization, learning extraction, uploads) run asynchronously.

### Key Requirements

1. **Never block user** - Claude Code hooks complete < 100ms p95
2. **Persist across restarts** - Jobs survive crashes/shutdowns
3. **At-least-once delivery** - Jobs execute at least once (with idempotency)
4. **Ordered processing** - Respect job dependencies
5. **Retry with backoff** - Handle transient failures gracefully
6. **Offline tolerance** - Queue locally, sync when online
7. **Observable** - Monitor queue depth, latency, errors
8. **Graceful shutdown** - Complete in-flight jobs before exit

---

## System Architecture

### High-Level Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                         User Interaction                        │
│               (Claude Code < 100ms hook budget)                 │
└─────────────────────┬───────────────────────────────────────────┘
                      │
                      ▼
            ┌─────────────────┐
            │  Hook (< 100ms) │
            │  ┌───────────┐  │
            │  │ Sanitize  │  │ ← Fast pre-sanitization (<50ms)
            │  │  (Fast)   │  │   Rule-based, regex patterns
            │  └─────┬─────┘  │
            │        │        │
            │        ▼        │
            │  ┌───────────┐  │
            │  │  Persist  │  │ ← Write sanitized to messages table
            │  │ Sanitized │  │   SQLite WAL mode (<20ms)
            │  └─────┬─────┘  │
            │        │        │
            │        ▼        │
            │  ┌───────────┐  │
            │  │  Enqueue  │  │ ← Atomic job enqueue (same txn)
            │  │   Jobs    │  │   job_queue table
            │  └─────┬─────┘  │
            └────────┼────────┘
                     │
                     ▼
         ┌───────────────────────┐
         │  SQLite job_queue     │ ← Persistent queue (CANONICAL schema)
         │   (WAL journaling)    │   Survives crashes
         │  ULID IDs, ISO-8601   │   Status: queued → in_progress → completed/failed/dead_letter
         └───────────┬───────────┘
                     │
         ┌───────────┴───────────┐
         │                       │
         ▼                       ▼
   ┌──────────┐          ┌──────────┐
   │  Worker  │          │  Worker  │ ← Independent processes
   │ Process  │          │ Process  │   Poll queue
   │   #1     │          │   #N     │   Lease-based execution
   └────┬─────┘          └────┬─────┘
        │                     │
        ▼                     ▼
    ┌───────────────────────────┐
    │    Job Handlers           │
    │  ┌─────────────────────┐  │
    │  │ sanitize_ai_validation│ ← AI-based PII validation
    │  └─────────────────────┘  │   Claude API, <2s
    │  ┌─────────────────────┐  │
    │  │ extract_learning    │  │ ← Learning extraction
    │  └─────────────────────┘  │   Claude API, <5s
    │  ┌─────────────────────┐  │
    │  │ mine_upload         │  │ ← IPFS + blockchain upload
    │  └─────────────────────┘  │   10-30s, network I/O
    └───────────────────────────┘
```

### Components

1. **Hook** - Enqueues jobs atomically with message persistence
2. **Job Queue** - SQLite-based persistent queue with ACID guarantees
3. **Workers** - Independent processes polling for jobs
4. **Job Handlers** - Idempotent processors for each job type
5. **Dead Letter Queue** - Failed jobs for manual intervention

---

## Job Queue Design

### Canonical Schema

**CRITICAL**: This is the canonical schema from STANDARDS.md. All implementations MUST match exactly.

**Key Principles**:
- **ULID IDs**: All IDs use ULID (time-sortable, lexicographic)
- **ISO-8601 Timestamps**: All timestamp columns use TEXT with ISO-8601 format
- **Canonical Status Enums**: ONLY queued | in_progress | completed | failed | dead_letter
- **Atomic Claiming**: Single UPDATE ... RETURNING prevents race conditions

```sql
CREATE TABLE IF NOT EXISTS job_queue (
  -- Identity
  id TEXT PRIMARY KEY,                    -- ULID (time-sortable)
  type TEXT NOT NULL,                     -- 'sanitize_ai_validation', 'extract_learning', 'mine_upload'

  -- Status (CANONICAL from STANDARDS.md)
  status TEXT NOT NULL DEFAULT 'queued' CHECK (
    status IN ('queued', 'in_progress', 'completed', 'failed', 'dead_letter')
  ),

  -- Priority and Scheduling
  priority INTEGER NOT NULL DEFAULT 5 CHECK (priority >= 1 AND priority <= 10),
  scheduled_at TEXT NOT NULL,             -- ISO-8601 timestamp - when to execute

  -- Worker Coordination (Lease-based)
  lease_owner TEXT,                       -- Worker ID (hostname:pid)
  lease_until TEXT,                       -- ISO-8601 timestamp - lease expiry

  -- Payload and Idempotency
  payload TEXT NOT NULL,                  -- JSON job data
  idempotency_key TEXT UNIQUE,            -- Deduplication key

  -- Retry Management
  attempts INTEGER NOT NULL DEFAULT 0,
  max_retries INTEGER NOT NULL DEFAULT 3,
  error TEXT,                             -- Last error message

  -- Timestamps (ALL ISO-8601 TEXT)
  created_at TEXT NOT NULL,               -- ISO-8601 - when job was created
  updated_at TEXT NOT NULL,               -- ISO-8601 - last modification
  started_at TEXT,                        -- ISO-8601 - when worker claimed job
  completed_at TEXT,                      -- ISO-8601 - when job finished

  -- Result
  result TEXT                             -- JSON result data
);

-- Indexes for efficient worker queries
CREATE INDEX idx_job_queue_dequeue
  ON job_queue(status, priority, scheduled_at)
  WHERE status = 'queued';

CREATE INDEX idx_job_queue_lease_expiry
  ON job_queue(lease_until)
  WHERE status = 'in_progress';

CREATE INDEX idx_job_queue_type
  ON job_queue(type, status);

CREATE INDEX idx_job_queue_idempotency
  ON job_queue(idempotency_key);

-- Trigger for updated_at (ISO-8601)
CREATE TRIGGER job_queue_updated_at
AFTER UPDATE ON job_queue
FOR EACH ROW
BEGIN
  UPDATE job_queue SET updated_at = (datetime('now', 'localtime') || 'Z') WHERE id = NEW.id;
END;
```

### Status State Machine

**CANONICAL**: These are the ONLY valid status values from STANDARDS.md.

```typescript
type JobStatus =
  | 'queued'        // Initial state, waiting for worker
  | 'in_progress'   // Worker has claimed and is processing
  | 'completed'     // Successfully finished
  | 'failed'        // Failed but retriable (attempts < max_retries)
  | 'dead_letter';  // Failed permanently (attempts >= max_retries)

// Valid state transitions
const validTransitions: Record<JobStatus, JobStatus[]> = {
  queued: ['in_progress'],
  in_progress: ['completed', 'failed', 'queued'],  // queued = lease expired
  failed: ['queued', 'dead_letter'],
  completed: [],       // Terminal state
  dead_letter: []      // Terminal state
};
```

**State Transition Rules**:
1. `queued` → `in_progress`: Worker claims job with lease
2. `in_progress` → `completed`: Job succeeds
3. `in_progress` → `failed`: Job fails, attempts < max_retries
4. `in_progress` → `queued`: Lease expires (worker crashed)
5. `failed` → `queued`: Retry with exponential backoff
6. `failed` → `dead_letter`: Max retries exceeded
7. Terminal states (`completed`, `dead_letter`) never transition

---

## Job Types

### CANONICAL Job Types

From STANDARDS.md, these are the ONLY job types:

1. **sanitize_ai_validation** - AI-based PII validation (DOWNSTREAM from fast pre-sanitization in hook)
2. **extract_learning** - Extract learnings from conversations
3. **mine_upload** - Upload learnings to IPFS + blockchain

**CRITICAL**: The `sanitize_ai_validation` job operates on ALREADY SANITIZED content. Fast pre-sanitization happens in the hook (<50ms) BEFORE any database persistence. This job performs additional AI validation to catch edge cases the regex rules might miss.

### Job Type Specifications

#### 1. sanitize_ai_validation

**Purpose**: Downstream AI validation of pre-sanitized content

**Payload**:
```typescript
interface SanitizeAIValidationPayload {
  conversation_id: string;
  message_ids: string[];     // Messages to validate (ALREADY SANITIZED)
  sanitization_version: string;
}
```

**Behavior**:
- **CRITICAL**: Operates ONLY on pre-sanitized content from messages table
- Validates no PII leaked through fast regex rules
- Catches context-aware PII (e.g., names that look like variables)
- Updates sanitization_log with additional findings
- Does NOT access or store raw content
- Idempotent: Check if AI validation already completed for conversation

**Performance Budget**: <2s p95 (Claude API call)

**Priority**: 1 (highest) - privacy-critical

**Max Retries**: 3

**Idempotency Key**: `sanitize-ai-validation-${conversation_id}`

#### 2. extract_learning

**Purpose**: Extract learnings from completed conversations

**Payload**:
```typescript
interface ExtractLearningPayload {
  conversation_id: string;
  min_confidence: number;    // Minimum confidence threshold (0.0-1.0)
  categories: string[];      // Which learning categories to extract
}
```

**Behavior**:
- Analyzes message sequence for patterns, insights
- Generates learnings with confidence scores
- Deduplicates against existing learnings (dedupe_hash)
- Writes to learnings table with FTS indexing
- Idempotent: Check if learnings exist for conversation_id

**Performance Budget**: <5s p95 (Claude API call)

**Priority**: 5 (medium)

**Max Retries**: 3

**Idempotency Key**: `extract-learning-${conversation_id}`

#### 3. mine_upload

**Purpose**: Upload learnings to global network (IPFS + blockchain)

**Payload**:
```typescript
interface MineUploadPayload {
  learning_id: string;
  ipfs_endpoint: string;     // IPFS API endpoint
  chain_config: {
    rpc_url: string;
    contract_address: string;
    private_key_ref: string; // Reference to secure key storage
  };
}
```

**Behavior**:
- Upload learning to IPFS, get CID
- Submit blockchain transaction with CID
- Wait for transaction confirmation
- Update uploads table with CID, tx_hash, status
- Idempotent: Check uploads table for learning_id

**Performance Budget**: 10-30s p95 (network I/O)

**Priority**: 10 (lowest) - not time-sensitive

**Max Retries**: 5 (network can be flaky)

**Idempotency Key**: `upload-${learning_id}`

---

## Worker Architecture

### Worker Model

Workers are independent processes that:
1. **Poll** the job queue for available jobs
2. **Lease** jobs using optimistic locking
3. **Execute** job handlers with timeout
4. **Report** results (success/failure)
5. **Retry** failed jobs with exponential backoff
6. **Shutdown** gracefully on signals

### Worker Lifecycle

```
┌──────────────────────────────────────────────────────┐
│                   Worker Start                       │
└───────────────────┬──────────────────────────────────┘
                    │
                    ▼
         ┌──────────────────────┐
         │  Initialize Worker   │
         │  - Set worker_id     │
         │  - Open DB connection│
         │  - Register handlers │
         └──────────┬───────────┘
                    │
         ┌──────────▼───────────┐
         │   Polling Loop       │◄──────────┐
         │   (1s interval)      │           │
         └──────────┬───────────┘           │
                    │                       │
                    ▼                       │
         ┌──────────────────────┐           │
         │   Claim Job          │           │
         │   (Lease-based)      │           │
         └──────────┬───────────┘           │
                    │                       │
            ┌───────┴───────┐               │
            │               │               │
            ▼               ▼               │
      ┌─────────┐    ┌──────────┐          │
      │No Job   │    │ Job Found│          │
      │ Sleep   │    └────┬─────┘          │
      └────┬────┘         │                │
           │              ▼                │
           │    ┌──────────────────┐       │
           │    │  Execute Handler │       │
           │    │  (Idempotent)    │       │
           │    └──────┬───────────┘       │
           │           │                   │
           │    ┌──────┴──────┐            │
           │    │             │            │
           │    ▼             ▼            │
           │ ┌────────┐   ┌────────┐      │
           │ │Success │   │ Failure│      │
           │ └───┬────┘   └───┬────┘      │
           │     │            │            │
           │     ▼            ▼            │
           │ ┌────────┐   ┌────────────┐  │
           │ │ Mark   │   │ Handle     │  │
           │ │Complete│   │ Retry/DLQ  │  │
           │ └───┬────┘   └───┬────────┘  │
           │     │            │            │
           └─────┴────────────┴────────────┘
                            │
                            ▼
                    ┌───────────────┐
                    │ SIGTERM/SIGINT│
                    └───────┬───────┘
                            │
                            ▼
                  ┌──────────────────┐
                  │ Graceful Shutdown│
                  │ - Finish current │
                  │ - Release lease  │
                  │ - Close DB       │
                  └──────────────────┘
```

### Lease-Based Execution

**Why Leases**: Prevent duplicate processing and enable crash recovery.

**Lease Parameters**:
- **Duration**: 60s (adjustable per job type)
- **Renewal**: Not implemented in MVP (jobs must complete within lease)
- **Expiry Check**: Separate worker process checks for expired leases

**Claim Algorithm** (Optimistic Locking):

```typescript
function claimJob(workerId: string): Job | null {
  const now = new Date().toISOString();  // ISO-8601 format
  const leaseUntil = new Date(Date.now() + 60000).toISOString();  // +60s

  // Atomic update with RETURNING (SQLite 3.35+)
  // Single-statement UPDATE prevents race conditions with multiple workers
  const job = db.prepare(`
    UPDATE job_queue
    SET
      status = 'in_progress',
      lease_owner = ?,
      lease_until = ?,
      started_at = ?,
      attempts = attempts + 1,
      updated_at = ?
    WHERE id = (
      SELECT id
      FROM job_queue
      WHERE status = 'queued'
        AND scheduled_at <= ?
      ORDER BY priority ASC, scheduled_at ASC
      LIMIT 1
    )
    RETURNING *
  `).get(workerId, leaseUntil, now, now, now);

  return job ? deserializeJob(job) : null;
}
```

**Lease Expiry Recovery**:

```typescript
// Separate watchdog process
function releaseExpiredLeases(): number {
  const now = new Date().toISOString();  // ISO-8601 format

  const result = db.prepare(`
    UPDATE job_queue
    SET
      status = 'queued',
      lease_owner = NULL,
      lease_until = NULL,
      started_at = NULL,
      updated_at = ?
    WHERE status = 'in_progress'
      AND lease_until < ?
  `).run(now, now);

  return result.changes;
}

// Run every 30s
setInterval(() => {
  const released = releaseExpiredLeases();
  if (released > 0) {
    logger.warn(`Released ${released} expired leases`);
  }
}, 30000);
```

---

## Idempotency Strategy

### Principle

**At-least-once delivery requires idempotency**: Jobs may execute multiple times (crashes, retries, lease expiry). Handlers MUST be safe to re-run.

### Implementation Patterns

#### 1. Check-Before-Mutate

```typescript
import { ulid } from 'ulid';

async function sanitizeAIValidationHandler(payload: SanitizeAIValidationPayload) {
  const { conversation_id, message_ids } = payload;

  // Check if already validated
  const conversation = db.prepare(`
    SELECT id, metadata
    FROM conversations
    WHERE id = ?
  `).get(conversation_id);

  const metadata = conversation.metadata ? JSON.parse(conversation.metadata) : {};
  if (metadata.ai_validation_completed) {
    logger.info(`Conversation ${conversation_id} already AI validated`);
    return { skipped: true, reason: 'already-validated' };
  }

  // Fetch SANITIZED messages (NEVER raw content)
  const messages = db.prepare(`
    SELECT id, content
    FROM messages
    WHERE id IN (${message_ids.map(() => '?').join(',')})
  `).all(...message_ids);

  // Perform AI validation on sanitized content
  const validationResult = await validateSanitizedContentWithAI(messages);

  // Update atomically
  db.transaction(() => {
    // Mark as AI validated
    metadata.ai_validation_completed = true;
    metadata.ai_validation_timestamp = new Date().toISOString();

    db.prepare(`
      UPDATE conversations
      SET metadata = ?,
          updated_at = ?
      WHERE id = ?
    `).run(
      JSON.stringify(metadata),
      new Date().toISOString(),
      conversation_id
    );

    // Log any additional findings
    for (const finding of validationResult.findings) {
      db.prepare(`
        INSERT INTO sanitization_log (
          id, conversation_id, message_id, category,
          original_snippet_hash, replacement, detector,
          confidence, created_at
        )
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
      `).run(
        ulid(),
        conversation_id,
        finding.message_id,
        finding.category,
        finding.snippet_hash,
        finding.replacement,
        'ai',
        finding.confidence,
        new Date().toISOString()
      );
    }
  })();

  return { success: true, findings: validationResult.findings.length };
}
```

#### 2. Idempotency Keys

```typescript
import { ulid } from 'ulid';

function enqueueJob(type: string, payload: any, idempotencyKey: string) {
  const now = new Date().toISOString();  // ISO-8601 format

  try {
    db.prepare(`
      INSERT INTO job_queue (
        id, type, payload, idempotency_key,
        scheduled_at, created_at, updated_at
      )
      VALUES (?, ?, ?, ?, ?, ?, ?)
    `).run(
      ulid(),
      type,
      JSON.stringify(payload),
      idempotencyKey,
      now,
      now,
      now
    );

    return { enqueued: true };
  } catch (error) {
    // UNIQUE constraint violation on idempotency_key
    if (error.code === 'SQLITE_CONSTRAINT_UNIQUE') {
      logger.info(`Job with key ${idempotencyKey} already enqueued`);
      return { enqueued: false, reason: 'duplicate' };
    }
    throw error;
  }
}
```

#### 3. Natural Keys

For learnings and uploads, use natural keys to prevent duplicates:

```typescript
import { ulid } from 'ulid';
import crypto from 'crypto';

async function extractLearningHandler(payload: ExtractLearningPayload) {
  const { conversation_id } = payload;

  // Extract learnings from Claude using SANITIZED messages
  const rawLearnings = await extractLearningsWithClaude(conversation_id);

  for (const learning of rawLearnings) {
    // Generate dedupe hash
    const dedupeHash = crypto
      .createHash('sha256')
      .update(`${learning.category}:${learning.content}`)
      .digest('hex');

    try {
      const now = new Date().toISOString();  // ISO-8601 format

      // Insert with UNIQUE constraint on dedupe_hash
      db.prepare(`
        INSERT INTO learnings (
          id, conversation_id, category, title, content,
          confidence, tags, dedupe_hash, source_message_ids,
          created_at
        )
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
      `).run(
        ulid(),
        conversation_id,
        learning.category,
        learning.title,
        learning.content,
        learning.confidence,
        JSON.stringify(learning.tags),
        dedupeHash,
        JSON.stringify(learning.source_message_ids),
        now
      );
    } catch (error) {
      if (error.code === 'SQLITE_CONSTRAINT_UNIQUE') {
        // Learning already exists, skip
        logger.debug(`Learning already exists: ${dedupeHash}`);
        continue;
      }
      throw error;
    }
  }

  return { success: true, learnings: rawLearnings.length };
}
```

#### 4. Upload Idempotency

```typescript
async function mineUploadHandler(payload: MineUploadPayload) {
  const { learning_id } = payload;

  // Check upload status
  const upload = db.prepare(`
    SELECT status, ipfs_cid, chain_tx_hash
    FROM uploads
    WHERE learning_id = ?
  `).get(learning_id);

  if (upload) {
    // Already uploaded
    if (upload.status === 'confirmed') {
      return { skipped: true, reason: 'already-confirmed' };
    }

    // Resume from last state
    if (upload.status === 'ipfs_uploaded' && upload.ipfs_cid) {
      // Skip IPFS, go straight to blockchain
      return await submitToBlockchain(learning_id, upload.ipfs_cid);
    }

    if (upload.status === 'tx_submitted' && upload.chain_tx_hash) {
      // Check transaction status
      return await checkTransactionStatus(learning_id, upload.chain_tx_hash);
    }
  }

  // Fresh upload
  const learning = db.prepare(`
    SELECT content FROM learnings WHERE id = ?
  `).get(learning_id);

  // Upload to IPFS
  const cid = await uploadToIPFS(learning.content);

  const now = new Date().toISOString();  // ISO-8601 format

  // Record IPFS upload
  db.prepare(`
    INSERT INTO uploads (id, learning_id, ipfs_cid, status, uploaded_at, created_at, updated_at)
    VALUES (?, ?, ?, 'ipfs_uploaded', ?, ?, ?)
    ON CONFLICT(learning_id) DO UPDATE SET
      ipfs_cid = excluded.ipfs_cid,
      status = excluded.status,
      uploaded_at = excluded.uploaded_at,
      updated_at = excluded.updated_at
  `).run(ulid(), learning_id, cid, now, now, now);

  // Submit to blockchain
  return await submitToBlockchain(learning_id, cid);
}
```

---

## Retry and Backoff

### Retry Strategy

**At-least-once delivery**: Failed jobs are retried with exponential backoff.

**Parameters**:
- **Max Retries**: 3 (configurable per job type)
- **Base Delay**: 1s
- **Max Delay**: 60s
- **Jitter**: ±0-1s (prevent thundering herd)

### Backoff Algorithm

```typescript
function calculateBackoff(attempts: number): number {
  const baseDelay = 1000; // 1s
  const maxDelay = 60000; // 60s

  // Exponential: 2^attempts * baseDelay
  const exponential = Math.min(baseDelay * Math.pow(2, attempts), maxDelay);

  // Add jitter: ±0-1s
  const jitter = Math.random() * 1000;

  return exponential + jitter;
}

// Example: attempts = 1 → ~2s, attempts = 2 → ~4s, attempts = 3 → ~8s
```

### Failure Handling

```typescript
async function handleJobFailure(job: Job, error: Error) {
  const attempts = job.attempts;
  const now = new Date().toISOString();  // ISO-8601 format

  if (attempts >= job.max_retries) {
    // Move to dead letter queue
    db.prepare(`
      UPDATE job_queue
      SET
        status = 'dead_letter',
        error = ?,
        completed_at = ?,
        updated_at = ?
      WHERE id = ?
    `).run(error.message, now, now, job.id);

    // Alert on DLQ additions
    await alertDeadLetterQueue(job, error);

    logger.error(`Job ${job.id} moved to dead_letter after ${attempts} attempts`, {
      job_id: job.id,
      job_type: job.type,
      error: error.message
    });

    return;
  }

  // Retry with backoff
  const backoffMs = calculateBackoff(attempts);
  const scheduledAt = new Date(Date.now() + backoffMs).toISOString();

  db.prepare(`
    UPDATE job_queue
    SET
      status = 'queued',
      error = ?,
      scheduled_at = ?,
      lease_owner = NULL,
      lease_until = NULL,
      started_at = NULL,
      updated_at = ?
    WHERE id = ?
  `).run(error.message, scheduledAt, now, job.id);

  logger.warn(`Job ${job.id} failed, retrying in ${backoffMs}ms (attempt ${attempts}/${job.max_retries})`, {
    job_id: job.id,
    job_type: job.type,
    error: error.message
  });
}
```

### Dead Letter Queue

**Purpose**: Collect jobs that failed permanently for manual intervention.

**Query Dead Letter Jobs**:
```typescript
function getDeadLetterJobs(limit: number = 100): Job[] {
  const jobs = db.prepare(`
    SELECT *
    FROM job_queue
    WHERE status = 'dead_letter'
    ORDER BY completed_at DESC
    LIMIT ?
  `).all(limit);

  return jobs.map(deserializeJob);
}
```

**Retry Dead Letter Job**:
```typescript
function retryDeadLetterJob(jobId: string) {
  const now = new Date().toISOString();  // ISO-8601 format

  db.prepare(`
    UPDATE job_queue
    SET
      status = 'queued',
      attempts = 0,
      error = NULL,
      scheduled_at = ?,
      completed_at = NULL,
      lease_owner = NULL,
      lease_until = NULL,
      started_at = NULL,
      updated_at = ?
    WHERE id = ? AND status = 'dead_letter'
  `).run(now, now, jobId);

  logger.info(`Retrying dead letter job ${jobId}`);
}
```

**Monitoring**:
```typescript
// Alert on DLQ growth
setInterval(() => {
  const count = db.prepare(`
    SELECT COUNT(*) as count
    FROM job_queue
    WHERE status = 'dead_letter'
  `).get().count;

  if (count > 10) {
    alertDeadLetterQueueSize(count);
  }
}, 300000); // Check every 5 minutes
```

---

## Graceful Shutdown

### Requirements

1. **No job loss** - Complete in-flight jobs before exit
2. **No orphaned leases** - Release leases on incomplete jobs
3. **Clean database state** - No stale locks
4. **Bounded shutdown time** - Max 30s wait for completion

### Implementation

```typescript
class JobWorker {
  private running = false;
  private currentJob: Job | null = null;
  private shutdownPromise: Promise<void> | null = null;

  async start() {
    this.running = true;
    logger.info(`Worker ${this.workerId} starting`);

    // Register signal handlers
    process.on('SIGTERM', () => this.shutdown('SIGTERM'));
    process.on('SIGINT', () => this.shutdown('SIGINT'));

    // Main loop
    while (this.running) {
      try {
        const job = await this.claimJob();

        if (job) {
          this.currentJob = job;
          await this.executeJob(job);
          this.currentJob = null;
        } else {
          // No jobs available, sleep
          await sleep(this.pollInterval);
        }
      } catch (error) {
        logger.error('Worker error', { error });
        await sleep(this.pollInterval);
      }
    }

    logger.info(`Worker ${this.workerId} stopped`);
  }

  async shutdown(signal: string) {
    if (this.shutdownPromise) {
      // Already shutting down
      return this.shutdownPromise;
    }

    logger.info(`Worker ${this.workerId} received ${signal}, shutting down gracefully`);

    this.shutdownPromise = this._doShutdown();
    return this.shutdownPromise;
  }

  private async _doShutdown() {
    // Stop accepting new jobs
    this.running = false;

    // Wait for current job to finish (with timeout)
    if (this.currentJob) {
      logger.info(`Waiting for current job ${this.currentJob.id} to complete`);

      await Promise.race([
        this.waitForJobCompletion(),
        sleep(30000) // 30s max wait
      ]);

      // If job still in progress after timeout, release lease
      if (this.currentJob) {
        logger.warn(`Job ${this.currentJob.id} did not complete in time, releasing lease`);

        db.prepare(`
          UPDATE job_queue
          SET
            status = 'queued',
            lease_owner = NULL,
            lease_until = NULL,
            started_at = NULL
          WHERE id = ?
        `).run(this.currentJob.id);
      }
    }

    // Close database connection
    db.close();

    logger.info('Worker shutdown complete');
    process.exit(0);
  }

  private async waitForJobCompletion(): Promise<void> {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        if (!this.currentJob) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 100);
    });
  }
}
```

### Shutdown Scenarios

#### Normal Shutdown (SIGTERM)
```
1. Stop accepting new jobs (running = false)
2. Wait for current job to complete (max 30s)
3. Release lease if job doesn't complete
4. Close database connection
5. Exit with code 0
```

#### Forced Shutdown (SIGKILL)
```
1. Process dies immediately
2. Lease expires after 60s
3. Watchdog releases lease
4. Job returns to 'queued' status
5. Another worker picks up job
```

#### Crash/Exception
```
1. Worker process dies
2. currentJob lease remains
3. Lease expires after 60s
4. Watchdog releases lease
5. Job returns to 'queued'
```

---

## Monitoring and Observability

### Key Metrics

1. **Queue Depth** - Number of jobs waiting (status = 'queued')
2. **Queue Age** - Age of oldest queued job
3. **Processing Rate** - Jobs completed per minute
4. **Success Rate** - Completed / (Completed + Failed)
5. **Dead Letter Count** - Jobs in permanent failure state
6. **Job Latency** - p50, p95, p99 processing time
7. **Lease Expiry Rate** - Jobs with expired leases (worker crashes)

### Metrics Collection

```typescript
interface QueueMetrics {
  queueDepth: number;
  queueAgeMs: number;
  inProgressCount: number;
  processingRate: number;    // jobs/min
  successRate: number;       // 0.0-1.0
  deadLetterCount: number;
  latencyP50Ms: number;
  latencyP95Ms: number;
  latencyP99Ms: number;
  leaseExpiryRate: number;   // expirations/min
}

function collectMetrics(windowMs: number = 3600000): QueueMetrics {
  const now = Date.now();
  const windowStart = new Date(now - windowMs).toISOString();

  // Queue depth
  const queueDepth = db.prepare(`
    SELECT COUNT(*) as count
    FROM job_queue
    WHERE status = 'queued'
  `).get().count;

  // Queue age (oldest queued job)
  const oldestJob = db.prepare(`
    SELECT created_at
    FROM job_queue
    WHERE status = 'queued'
    ORDER BY created_at ASC
    LIMIT 1
  `).get();

  const queueAgeMs = oldestJob
    ? now - new Date(oldestJob.created_at).getTime()
    : 0;

  // In-progress count
  const inProgressCount = db.prepare(`
    SELECT COUNT(*) as count
    FROM job_queue
    WHERE status = 'in_progress'
  `).get().count;

  // Dead letter count
  const deadLetterCount = db.prepare(`
    SELECT COUNT(*) as count
    FROM job_queue
    WHERE status = 'dead_letter'
  `).get().count;

  // Completed in window
  const completedCount = db.prepare(`
    SELECT COUNT(*) as count
    FROM job_queue
    WHERE status = 'completed'
      AND completed_at >= ?
  `).get(windowStart).count;

  // Failed in window
  const failedCount = db.prepare(`
    SELECT COUNT(*) as count
    FROM job_queue
    WHERE status IN ('failed', 'dead_letter')
      AND completed_at >= ?
  `).get(windowStart).count;

  // Processing rate (jobs/min)
  const processingRate = (completedCount / windowMs) * 60000;

  // Success rate
  const totalProcessed = completedCount + failedCount;
  const successRate = totalProcessed > 0
    ? completedCount / totalProcessed
    : 1.0;

  // Latency percentiles
  const latencies = db.prepare(`
    SELECT
      (julianday(completed_at) - julianday(started_at)) * 86400000 as latency_ms
    FROM job_queue
    WHERE status = 'completed'
      AND completed_at >= ?
    ORDER BY latency_ms ASC
  `).all(windowStart).map(r => r.latency_ms);

  const latencyP50Ms = percentile(latencies, 0.50);
  const latencyP95Ms = percentile(latencies, 0.95);
  const latencyP99Ms = percentile(latencies, 0.99);

  return {
    queueDepth,
    queueAgeMs,
    inProgressCount,
    processingRate,
    successRate,
    deadLetterCount,
    latencyP50Ms,
    latencyP95Ms,
    latencyP99Ms,
    leaseExpiryRate: 0 // TODO: Track lease expirations
  };
}

function percentile(values: number[], p: number): number {
  if (values.length === 0) return 0;
  const sorted = values.sort((a, b) => a - b);
  const index = Math.ceil(sorted.length * p) - 1;
  return sorted[Math.max(0, index)];
}
```

### Alerting Rules

```typescript
async function checkAlerts() {
  const metrics = collectMetrics(3600000); // 1 hour window

  // High queue depth
  if (metrics.queueDepth > 1000) {
    await alert({
      severity: 'WARNING',
      metric: 'queue_depth',
      value: metrics.queueDepth,
      threshold: 1000,
      message: 'Job queue depth is high, workers may be overwhelmed'
    });
  }

  // Old jobs in queue
  if (metrics.queueAgeMs > 3600000) { // 1 hour
    await alert({
      severity: 'WARNING',
      metric: 'queue_age',
      value: metrics.queueAgeMs,
      threshold: 3600000,
      message: 'Jobs are waiting too long in queue'
    });
  }

  // Low success rate
  if (metrics.successRate < 0.95) {
    await alert({
      severity: 'ERROR',
      metric: 'success_rate',
      value: metrics.successRate,
      threshold: 0.95,
      message: 'Job success rate is below 95%'
    });
  }

  // Dead letter queue growth
  if (metrics.deadLetterCount > 10) {
    await alert({
      severity: 'ERROR',
      metric: 'dead_letter_count',
      value: metrics.deadLetterCount,
      threshold: 10,
      message: 'Dead letter queue is growing, manual intervention needed'
    });
  }

  // High latency
  if (metrics.latencyP95Ms > 10000) { // 10s
    await alert({
      severity: 'WARNING',
      metric: 'latency_p95',
      value: metrics.latencyP95Ms,
      threshold: 10000,
      message: 'Job processing latency is high'
    });
  }
}

// Run every 5 minutes
setInterval(checkAlerts, 300000);
```

### Logging Standards

```typescript
// Structured logging for observability
interface JobLogContext {
  job_id: string;
  job_type: string;
  worker_id: string;
  attempts: number;
  duration_ms?: number;
  error?: string;
}

// Job claimed
logger.info('Job claimed', {
  job_id: job.id,
  job_type: job.type,
  worker_id: this.workerId,
  attempts: job.attempts,
  priority: job.priority
});

// Job succeeded
logger.info('Job completed', {
  job_id: job.id,
  job_type: job.type,
  worker_id: this.workerId,
  attempts: job.attempts,
  duration_ms: Date.now() - startTime
});

// Job failed
logger.error('Job failed', {
  job_id: job.id,
  job_type: job.type,
  worker_id: this.workerId,
  attempts: job.attempts,
  duration_ms: Date.now() - startTime,
  error: error.message
});

// Dead letter
logger.error('Job moved to dead_letter', {
  job_id: job.id,
  job_type: job.type,
  worker_id: this.workerId,
  attempts: job.attempts,
  error: error.message
});
```

---

## Operational Procedures

### Starting Workers

```bash
# Start single worker
node dist/workers/job-worker.js

# Start with PM2 (recommended)
pm2 start dist/workers/job-worker.js --name job-worker-1 -i 1

# Start multiple workers
pm2 start dist/workers/job-worker.js --name job-worker -i 4

# Monitor workers
pm2 monit
pm2 logs job-worker
```

### Stopping Workers

```bash
# Graceful shutdown (SIGTERM)
pm2 stop job-worker

# Force shutdown (SIGKILL)
pm2 kill job-worker

# Restart workers
pm2 restart job-worker
```

### Queue Maintenance

```typescript
// Clean up old completed jobs (archive or delete)
function cleanupCompletedJobs(retentionDays: number = 7) {
  const cutoff = new Date(Date.now() - retentionDays * 86400000).toISOString();  // ISO-8601

  const result = db.prepare(`
    DELETE FROM job_queue
    WHERE status = 'completed'
      AND completed_at < ?
  `).run(cutoff);

  logger.info(`Cleaned up ${result.changes} completed jobs older than ${retentionDays} days`);
}

// Run daily
setInterval(cleanupCompletedJobs, 86400000);
```

```typescript
// Release stale leases (crashed workers)
function releaseStaleLeases(timeoutMs: number = 300000) {
  const now = new Date().toISOString();  // ISO-8601 format
  const cutoff = new Date(Date.now() - timeoutMs).toISOString();  // ISO-8601

  const result = db.prepare(`
    UPDATE job_queue
    SET
      status = 'queued',
      lease_owner = NULL,
      lease_until = NULL,
      started_at = NULL,
      updated_at = ?
    WHERE status = 'in_progress'
      AND lease_until < ?
  `).run(now, cutoff);

  if (result.changes > 0) {
    logger.warn(`Released ${result.changes} stale leases`);
  }

  return result.changes;
}

// Run every 30s
setInterval(() => releaseStaleLeases(300000), 30000);
```

### Dead Letter Queue Management

```typescript
// Inspect dead letter jobs
function inspectDeadLetterQueue() {
  const jobs = db.prepare(`
    SELECT id, type, attempts, error, created_at, completed_at
    FROM job_queue
    WHERE status = 'dead_letter'
    ORDER BY completed_at DESC
    LIMIT 100
  `).all();

  console.table(jobs);
}

// Retry specific dead letter job
function retryDeadLetterJob(jobId: string) {
  const now = new Date().toISOString();  // ISO-8601 format

  db.prepare(`
    UPDATE job_queue
    SET
      status = 'queued',
      attempts = 0,
      error = NULL,
      completed_at = NULL,
      scheduled_at = ?,
      lease_owner = NULL,
      lease_until = NULL,
      started_at = NULL,
      updated_at = ?
    WHERE id = ? AND status = 'dead_letter'
  `).run(now, now, jobId);

  logger.info(`Retrying dead letter job ${jobId}`);
}

// Retry all dead letter jobs of specific type
function retryDeadLetterJobsByType(type: string) {
  const now = new Date().toISOString();  // ISO-8601 format

  const result = db.prepare(`
    UPDATE job_queue
    SET
      status = 'queued',
      attempts = 0,
      error = NULL,
      completed_at = NULL,
      scheduled_at = ?,
      lease_owner = NULL,
      lease_until = NULL,
      started_at = NULL,
      updated_at = ?
    WHERE type = ? AND status = 'dead_letter'
  `).run(now, now, type);

  logger.info(`Retrying ${result.changes} dead letter jobs of type ${type}`);
}
```

---

## Performance Budgets

From STANDARDS.md, these are the canonical performance budgets:

| Component | Budget | Measurement |
|-----------|--------|-------------|
| Hook execution | <100ms p95 | End-to-end (receive → sanitize → persist → enqueue) |
| Fast sanitization | <50ms p95 | Regex-based rules in hook |
| Database writes | <20ms p95 | WAL-mode insert |
| Job enqueue | <10ms p95 | INSERT with idempotency check |
| AI sanitization job | <2s p95 | Claude API call |
| Learning extraction job | <5s p95 | Claude API call |
| Upload job | 10-30s p95 | IPFS + blockchain network I/O |

---

## Testing Strategy

### Unit Tests

```typescript
describe('JobQueueRepository', () => {
  let db: Database;
  let repo: JobQueueRepository;

  beforeEach(() => {
    db = new Database(':memory:');
    runMigrations(db);
    repo = new JobQueueRepository(db);
  });

  afterEach(() => {
    db.close();
  });

  it('should enqueue job with idempotency key', () => {
    const payload = { conversation_id: 'test-123' };
    const job = repo.create({
      type: 'sanitize_ai_validation',
      payload,
      idempotency_key: 'test-key'
    });

    expect(job.status).toBe('queued');
    expect(job.payload).toEqual(payload);

    // Duplicate should fail
    expect(() => {
      repo.create({
        type: 'sanitize_ai_validation',
        payload,
        idempotency_key: 'test-key'
      });
    }).toThrow();
  });

  it('should claim job with lease', () => {
    // Enqueue job
    repo.create({
      type: 'extract_learning',
      payload: { conversation_id: 'test-123' }
    });

    // Claim job
    const job = repo.dequeue();
    expect(job).not.toBeNull();
    expect(job!.status).toBe('in_progress');
    expect(job!.lease_owner).toBeDefined();
    expect(job!.lease_until).toBeDefined();

    // No more jobs
    const noJob = repo.dequeue();
    expect(noJob).toBeNull();
  });

  it('should retry failed job with backoff', () => {
    const job = repo.create({
      type: 'mine_upload',
      payload: { learning_id: 'test-123' },
      max_retries: 3
    });

    // Claim and fail
    const claimed = repo.dequeue();
    repo.markFailed(claimed!.id, 'Network error');

    // Check requeued with backoff
    const requeued = repo.findById(claimed!.id);
    expect(requeued!.status).toBe('queued');
    expect(requeued!.attempts).toBe(1);
    expect(new Date(requeued!.scheduled_at).getTime()).toBeGreaterThan(Date.now());
  });

  it('should move to dead_letter after max retries', () => {
    const job = repo.create({
      type: 'extract_learning',
      payload: { conversation_id: 'test-123' },
      max_retries: 2
    });

    // Fail twice
    for (let i = 0; i < 2; i++) {
      const claimed = repo.dequeue();
      repo.markFailed(claimed!.id, `Error ${i + 1}`);
    }

    // Check dead_letter
    const deadJob = repo.findById(job.id);
    expect(deadJob!.status).toBe('dead_letter');
    expect(deadJob!.attempts).toBe(2);
  });
});
```

### Integration Tests

```typescript
describe('Worker Integration', () => {
  let db: Database;
  let worker: JobWorker;
  let handlerCalled = false;

  beforeEach(() => {
    db = new Database(':memory:');
    runMigrations(db);

    // Mock handler
    const mockHandler = async (payload: any) => {
      handlerCalled = true;
      return { success: true };
    };

    worker = new JobWorker({
      handlers: {
        test_job: mockHandler
      },
      pollInterval: 100
    });
  });

  afterEach(async () => {
    await worker.shutdown('TEST');
    db.close();
  });

  it('should process queued job', async () => {
    // Enqueue job
    const repo = new JobQueueRepository(db);
    repo.create({
      type: 'test_job',
      payload: { test: true }
    });

    // Start worker
    const workerPromise = worker.start();

    // Wait for handler to be called
    await waitFor(() => handlerCalled, 1000);

    // Stop worker
    await worker.shutdown('TEST');

    expect(handlerCalled).toBe(true);

    // Check job completed
    const jobs = repo.findByStatus('completed');
    expect(jobs).toHaveLength(1);
  });

  it('should handle job failure and retry', async () => {
    let attempts = 0;

    const failingHandler = async () => {
      attempts++;
      if (attempts < 2) {
        throw new Error('Temporary failure');
      }
      return { success: true };
    };

    worker = new JobWorker({
      handlers: {
        test_job: failingHandler
      },
      pollInterval: 100
    });

    // Enqueue job
    const repo = new JobQueueRepository(db);
    repo.create({
      type: 'test_job',
      payload: { test: true },
      max_retries: 3
    });

    // Start worker
    const workerPromise = worker.start();

    // Wait for retry and success
    await waitFor(() => attempts >= 2, 5000);

    // Stop worker
    await worker.shutdown('TEST');

    expect(attempts).toBe(2);

    // Check job completed
    const jobs = repo.findByStatus('completed');
    expect(jobs).toHaveLength(1);
  });
});
```

### End-to-End Tests

```typescript
describe('Async Processing E2E', () => {
  it('should process conversation through full pipeline', async () => {
    // 1. Hook enqueues sanitization job
    const conversationId = ulid();
    await hookHandler({
      event: 'UserPromptSubmit',
      conversation_id: conversationId,
      content: 'Test message with potential PII: john@example.com'
    });

    // Check job enqueued
    const repo = new JobQueueRepository(db);
    const jobs = repo.findByType('sanitize_ai_validation');
    expect(jobs).toHaveLength(1);

    // 2. Worker processes sanitization job
    const worker = new JobWorker();
    await worker.start();

    await waitFor(() => {
      const job = repo.findById(jobs[0].id);
      return job!.status === 'completed';
    }, 5000);

    // 3. Check learning extraction job enqueued
    const learningJobs = repo.findByType('extract_learning');
    expect(learningJobs).toHaveLength(1);

    // 4. Worker processes learning extraction
    await waitFor(() => {
      const job = repo.findById(learningJobs[0].id);
      return job!.status === 'completed';
    }, 10000);

    // 5. Check learnings created
    const learningRepo = new LearningRepository(db);
    const learnings = learningRepo.findByConversation(conversationId);
    expect(learnings.length).toBeGreaterThan(0);

    // 6. Check upload job enqueued
    const uploadJobs = repo.findByType('mine_upload');
    expect(uploadJobs.length).toBeGreaterThan(0);

    await worker.shutdown('TEST');
  });
});
```

---

## Security Considerations

### 1. Payload Sanitization

**Risk**: Malicious payloads in job queue

**Mitigation**:
- Validate payload schema before execution
- Use TypeScript types for payload validation
- Never eval() or execute arbitrary code from payloads

```typescript
function validatePayload<T>(payload: unknown, schema: z.ZodSchema<T>): T {
  try {
    return schema.parse(payload);
  } catch (error) {
    throw new Error(`Invalid payload: ${error.message}`);
  }
}

// Example usage
const SanitizePayloadSchema = z.object({
  conversation_id: z.string().regex(/^[0-9A-HJKMNP-TV-Z]{26}$/), // ULID format
  message_ids: z.array(z.string()),
  sanitization_version: z.string()
});

async function sanitizeAIValidationHandler(rawPayload: unknown) {
  const payload = validatePayload(rawPayload, SanitizePayloadSchema);
  // Safe to use payload
}
```

### 2. Worker Isolation

**Risk**: Worker compromise affects system

**Mitigation**:
- Workers run as separate processes
- Limited database permissions (no DDL)
- No access to raw content (only sanitized)
- No network access except approved endpoints (Claude API, IPFS)

### 3. Idempotency Key Security

**Risk**: Predictable keys allow queue manipulation

**Mitigation**:
- Include random component in keys where appropriate
- Use cryptographic hashes for content-based keys
- Validate key format before use

```typescript
function generateIdempotencyKey(conversation_id: string, nonce?: string): string {
  const input = nonce
    ? `${conversation_id}:${nonce}`
    : conversation_id;

  return `sanitize-${crypto.createHash('sha256').update(input).digest('hex').slice(0, 16)}`;
}
```

### 4. Dead Letter Queue Privacy

**Risk**: Failed jobs expose sensitive data

**Mitigation**:
- Never log full payloads in errors
- Sanitize error messages before storage
- Restrict access to dead letter queue
- Purge dead letter jobs after resolution

---

## Related Documents

### Standards
- [Global Project Standards](../STANDARDS.md) - Canonical schema, status enums, IDs

### Architecture
- [Global Context Network Architecture](./architecture-global-context-network-2025-01-16.md)
- [Hooks & Event Capture](./architecture-hooks-event-capture-2025-01-16.md)
- [Sanitization Pipeline](./architecture-sanitization-pipeline-2025-01-16.md)

### Decisions
- [ADR-006: Async Processing Model](../decisions/decision-async-processing-model-2025-01-16.md)
- [ADR-005: Use SQLite](../decisions/decision-use-sqlite-2025-01-16.md)
- [ADR-004: Sanitize Before Storage](../decisions/decision-sanitize-before-storage-2025-01-16.md)

### Reference
- [Database Schema Reference](../reference/reference-database-schema-2025-01-16.md)
- [Job Types Reference](../reference/reference-job-types-2025-01-16.md)

### Plans
- [Phase 4: Async Processing](../plans/plan-phase-4-async-processing-2025-01-16.md)

---

## Changelog

### 2025-01-16 - Initial Version
- Complete async processing architecture
- SQLite-based job queue with canonical schema
- Worker model with lease-based execution
- Idempotency strategies
- Retry and backoff logic
- Dead letter queue handling
- Graceful shutdown procedures
- Monitoring and observability
- Operational procedures
- Security considerations
- 100% alignment with STANDARDS.md
````

## File: architecture/architecture-global-context-network-2025-01-16.md
````markdown
# Global Context Network - System Architecture

> Complete system architecture for the Global Context Network MVP

---
title: Global Context Network System Architecture
category: architecture
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [architecture, system-design, blockchain, privacy, subagents]
---

## Overview

The Global Context Network is a decentralized system for capturing, sanitizing, storing, and sharing AI agent learnings globally. It enables agents to learn from each other's experiences while maintaining strict privacy guarantees through PII sanitization before storage.

### Core Innovation

**"Mining through Learning"**: Instead of computational mining, users contribute valuable learnings to the network and receive token rewards based on quality and validation.

## System Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         Claude Code (User Agent)                         │
└────────────┬─────────────────────────────────────┬──────────────────────┘
             │                                      │
             ▼ UserPromptSubmit                    ▼ Stop
    ┌────────────────┐                    ┌───────────────┐
    │  Hook Handler  │                    │  Hook Handler │
    └────────┬───────┘                    └───────┬───────┘
             │                                      │
             └──────────────┬──────────────────────┘
                            ▼
                 ┌──────────────────────┐
                 │  Event Collector     │
                 │  (Captures events)   │
                 └──────────┬───────────┘
                            │
                            ▼
                 ┌──────────────────────┐
                 │   Event Queue        │
                 │   (Persistent)       │
                 └──────────┬───────────┘
                            │
                            ▼
                 ┌──────────────────────┐
                 │  Sanitization Queue  │
                 │  (Async Worker)      │
                 └──────────┬───────────┘
                            │
                            ▼
              ┌─────────────────────────────┐
              │   Sanitization Pipeline     │
              │  ┌────────────────────────┐ │
              │  │ Rule-Based Detector    │ │
              │  │ (Regex, Fast)          │ │
              │  └──────────┬─────────────┘ │
              │             ▼                │
              │  ┌────────────────────────┐ │
              │  │ AI-Powered Sanitizer   │ │
              │  │ (Context-Aware)        │ │
              │  └──────────┬─────────────┘ │
              │             ▼                │
              │  ┌────────────────────────┐ │
              │  │ Hybrid Validator       │ │
              │  │ (Combine Results)      │ │
              │  └──────────┬─────────────┘ │
              └─────────────┼───────────────┘
                            │
                            ▼ SANITIZED DATA ONLY
                 ┌──────────────────────┐
                 │   SQLite Database    │
                 │  ┌────────────────┐  │
                 │  │ Conversations  │  │
                 │  │ Messages       │  │
                 │  │ Learnings      │  │
                 │  │ Job Queue      │  │
                 │  │ Uploads        │  │
                 │  └────────────────┘  │
                 └──────────┬───────────┘
                            │
                            ▼
                 ┌──────────────────────┐
                 │ Learning Extractor   │
                 │ (Async Worker)       │
                 └──────────┬───────────┘
                            │
                            ▼
                 ┌──────────────────────┐
                 │  Quality Filter      │
                 │  (Score & Validate)  │
                 └──────────┬───────────┘
                            │
                            ├──────────────────┐
                            │                  │
                            ▼                  ▼
                 ┌──────────────────┐  ┌──────────────────┐
                 │   MCP Server     │  │  Mining Queue    │
                 │  (Query Access)  │  │  (Upload)        │
                 └──────────────────┘  └────────┬─────────┘
                            │                    │
                            │                    ▼
                            │         ┌──────────────────┐
                            │         │  IPFS Upload     │
                            │         └────────┬─────────┘
                            │                  │
                            │                  ▼
                            │         ┌──────────────────┐
                            │         │ Blockchain Tx    │
                            │         │ (Token Rewards)  │
                            │         └──────────────────┘
                            │
                            ▼
                 ┌──────────────────────┐
                 │   Agent Clients      │
                 │ (Query via MCP)      │
                 └──────────────────────┘
```

## Core Components

### 1. Event Capture Layer

**Purpose**: Capture Claude Code conversations without blocking the user

**Components**:
- **UserPromptSubmit Hook**: Captures user input
- **Stop Hook**: Captures agent responses
- **Event Collector**: Aggregates events into conversations
- **Event Queue**: Persists events (SQLite-based)

**Key Requirements**:
- Hook execution < 100ms
- Never block user interaction
- Fail silently with logging
- Persist events across restarts

### 2. Sanitization Pipeline

**Purpose**: Remove ALL PII before database storage

**Components**:
- **Rule-Based Detector**: Fast regex-based PII detection
- **AI Sanitizer**: Context-aware detection using LLM
- **Hybrid Validator**: Combines both approaches
- **Audit Logger**: Tracks what was redacted

**PII Categories**:
1. API Keys & Secrets
2. File Paths (absolute with usernames)
3. Email Addresses
4. IP Addresses
5. Names (person names, not variables)
6. Phone Numbers
7. URLs with tokens

**Critical Guarantee**: NEVER store raw data. Sanitization happens BEFORE database insert.

### 3. Storage Layer

**Purpose**: Persist sanitized conversations and learnings

**Database**: SQLite with migrations

**Tables**:
- `conversations`: Sanitized conversation metadata
- `messages`: Individual sanitized messages
- `learnings`: Extracted insights and patterns
- `job_queue`: Async job tracking
- `uploads`: Network upload status
- `sanitization_log`: Audit trail

**Design Principles**:
- ACID compliance
- Indexed for performance (queries < 100ms)
- Versioned migrations
- Transaction-based updates

### 4. Async Processing Layer

**Purpose**: Process jobs without blocking

**Components**:
- **Job Queue**: Persistent, priority-based queue
- **Workers**: Independent job processors
- **Retry Logic**: Exponential backoff
- **Error Handling**: Quarantine failed jobs

**Job Types**:
1. `sanitize_conversation`: Run sanitization pipeline
2. `extract_learning`: Generate learnings
3. `mine_upload`: Upload to network

### 5. Learning Extraction Layer

**Purpose**: Extract valuable, reusable learnings

**Components**:
- **Conversation Analyzer**: Determines if conversation has value
- **Category Extractors**: Specialized by learning type
- **Quality Scorer**: Assigns confidence scores
- **Deduplication**: Prevents duplicate learnings

**Learning Categories**:
- `pattern`: Code patterns and architectures
- `best_practice`: Recommended approaches
- `anti_pattern`: Things to avoid
- `bug_fix`: Problem-solving strategies
- `optimization`: Performance improvements
- `tool_usage`: How to use tools/libraries
- `workflow`: Development workflows
- `decision`: Architecture decisions

**Quality Requirements**:
- Confidence score ≥ 0.6
- Content length ≥ 100 characters
- Well-categorized with tags
- Not trivial or generic

### 6. Query Interface (MCP Server)

**Purpose**: Enable agents to query learnings

**Protocol**: Model Context Protocol (MCP)

**Tools**:
- `search_learnings`: Query by text, category, tags
- `get_learning_by_id`: Retrieve specific learning
- `get_learning_context`: Full conversation for learning

**Resources**:
- `context://learnings/recent`: Latest learnings
- `context://learnings/top-rated`: Highest confidence
- `context://stats`: Network statistics

**Performance**: All queries < 200ms

### 7. Network Layer

**Purpose**: Share learnings globally with rewards

**Components**:
- **IPFS Client**: Decentralized storage
- **Blockchain Integration**: Transaction handling
- **Token System**: Reward calculation
- **Validator Network**: Quality validation (future)

**Upload Process**:
1. Learning queued for upload
2. Content uploaded to IPFS → CID generated
3. Blockchain transaction with CID
4. Token reward calculated
5. Status tracked in uploads table

## Data Flow

### Happy Path: Conversation → Global Network

```
1. User interacts with Claude Code
   ↓
2. Hooks capture UserPromptSubmit + Stop events
   ↓
3. Events queued (< 100ms, non-blocking)
   ↓
4. Async worker picks up sanitization job
   ↓
5. Sanitization pipeline removes ALL PII
   ↓
6. Sanitized data stored in SQLite
   ↓
7. Learning extraction job queued
   ↓
8. Async worker extracts learnings
   ↓
9. Quality filter scores and filters learnings
   ↓
10. High-quality learnings queued for upload
   ↓
11. Mining worker uploads to IPFS
   ↓
12. Blockchain transaction records upload
   ↓
13. Token reward distributed
   ↓
14. Other agents query via MCP server
```

## Privacy Guarantees

### Zero-Trust PII Handling

**Rule 1**: Never store unsanitized data
**Rule 2**: Sanitize before database insertion
**Rule 3**: Audit all redactions
**Rule 4**: User control over uploads

### Sanitization Validation

**Rule-Based Layer** (Fast, Deterministic):
- Regex patterns for known PII formats
- < 1% false positive rate
- Processing time < 10ms

**AI Layer** (Accurate, Context-Aware):
- LLM-based context analysis
- Distinguishes names from variables
- Handles company-specific terminology
- < 5% false negative rate

**Hybrid Validation**:
- Rules catch obvious cases quickly
- AI validates and enhances
- Combined result sanitized
- Audit log tracks all detections

## Performance Requirements

| Component | Requirement | Rationale |
|-----------|-------------|-----------|
| Hook Execution | < 100ms | Never block user |
| Event Queueing | < 50ms | Fast persistence |
| Sanitization | < 2s per conversation | Acceptable async delay |
| Database Queries | < 100ms | Responsive queries |
| MCP Queries | < 200ms | Agent experience |
| Learning Extraction | < 5s per conversation | Background processing |

## Scalability Considerations

### Current (MVP)
- Single SQLite database
- Local processing
- File-based queue

### Future Scaling
- PostgreSQL for multi-user
- Distributed job queue (Redis)
- Horizontal worker scaling
- CDN for IPFS content
- Sharded blockchain integration

## Security Model

### Threat Model

**Threats Addressed**:
1. PII Leakage → Sanitization before storage
2. Unauthorized Access → Local-first architecture
3. Data Corruption → ACID transactions
4. Injection Attacks → Parameterized queries
5. Secret Exposure → Hook-level filtering

**Future Threats**:
1. Network Byzantine actors → Validator consensus
2. Spam/Junk learnings → Quality scoring + validation
3. Sybil attacks → Identity verification
4. Reward manipulation → Multi-validator consensus

### Access Control

**MVP**: Local-only access (single user)

**Future**:
- Multi-user authentication
- Role-based access control
- API key management for MCP
- Encrypted storage option

## Technology Stack

| Layer | Technology | Rationale |
|-------|-----------|-----------|
| Runtime | Node.js + TypeScript | Type safety, async-first |
| Database | SQLite | Simple, embedded, ACID |
| Testing | Vitest | Fast, modern, TypeScript-first |
| Sanitization | Regex + Claude API | Hybrid approach |
| MCP Server | @modelcontextprotocol/sdk | Standard protocol |
| Blockchain | TBD (Ethereum/Celestia) | EVM compatibility |
| Storage | IPFS | Decentralized, content-addressed |
| Queue | SQLite-based | Simple, persistent |

## Error Handling

### Graceful Degradation

**Hooks Fail**: Log error, don't block user
**Sanitization Fails**: Quarantine conversation, alert
**Learning Extraction Fails**: Mark for manual review
**Upload Fails**: Retry with exponential backoff
**MCP Query Fails**: Return empty with error message

### Recovery Strategies

1. **Job Retries**: Max 3 attempts with backoff
2. **Dead Letter Queue**: Failed jobs for analysis
3. **Manual Review**: Quarantine for complex cases
4. **Rollback**: Database migrations reversible
5. **Audit Trail**: Full logging for debugging

## Testing Strategy

### Test Pyramid

- **70% Unit Tests**: Isolated component testing
- **20% Integration Tests**: Component interactions
- **10% E2E Tests**: Full system workflows

### Critical Test Coverage

1. **Sanitization**: Zero PII leaks in 1000+ test cases
2. **Hooks**: Non-blocking, error handling
3. **Queue**: No job loss, proper ordering
4. **Database**: ACID compliance, concurrency
5. **MCP**: Protocol compliance, performance

### Claude-Powered Testing Harness

Uses Claude Agent SDK to:
- Generate comprehensive test suites
- Validate test quality
- Verify implementations
- Enforce quality gates

## Deployment Architecture

### MVP (Local Development)
```
User Machine:
  - Claude Code with hooks
  - SQLite database
  - Background workers
  - MCP server (local)
```

### Production (Future)
```
User Machines:
  - Claude Code with hooks
  - Local SQLite cache
  - MCP client

Cloud Infrastructure:
  - PostgreSQL cluster
  - Worker pool (auto-scaling)
  - MCP server (HA)
  - IPFS node/gateway
  - Blockchain node
```

## Integration Points

### Claude Code Hooks
- Configuration via `hooks.json`
- Scripts in `.claude/hooks/`
- Environment variables for paths

### MCP Protocol
- Standard MCP server implementation
- Claude Code auto-discovery
- Tool and resource definitions

### Blockchain
- Smart contract for rewards
- Event listening for confirmations
- Wallet integration for payouts

### IPFS
- Content upload via API
- CID generation and tracking
- Gateway for content retrieval

## Monitoring & Observability

### Key Metrics

**Performance**:
- Hook execution time
- Sanitization duration
- Query response time
- Job processing rate

**Quality**:
- PII detection rate
- Learning confidence scores
- Test coverage percentage
- Quality gate pass rate

**Business**:
- Conversations captured
- Learnings extracted
- Network uploads
- Token rewards distributed

### Logging Strategy

1. **Structured Logging**: JSON format
2. **Log Levels**: DEBUG, INFO, WARN, ERROR
3. **Correlation IDs**: Track conversation flow
4. **Audit Trail**: All PII redactions
5. **Performance Metrics**: Timing for all operations

## Related Documents

### Architecture
- [Subagent System](./architecture-subagent-system-2025-01-16.md)
- [Testing Harness](./architecture-testing-harness-2025-01-16.md)
- [Sanitization Pipeline](./architecture-sanitization-pipeline-2025-01-16.md)
- [Database Schema](./architecture-database-schema-2025-01-16.md)

### Decisions
- [ADR: Use Claude Hooks](../decisions/decision-use-claude-hooks-2025-01-16.md)
- [ADR: Sanitize Before Storage](../decisions/decision-sanitize-before-storage-2025-01-16.md)
- [ADR: Subagent-Driven Development](../decisions/decision-subagent-driven-development-2025-01-16.md)

### Plans
- [Implementation Roadmap](../plans/plan-implementation-roadmap-2025-01-16.md)
- [Original User Vision](../plans/plan-original-user-vision-2025-01-16.md)

### Reference
- [Database Schema Reference](../reference/reference-database-schema-2025-01-16.md)
- [Testing Strategy](../reference/reference-testing-strategy-2025-01-16.md)
````

## File: architecture/architecture-hooks-event-capture-2025-01-16.md
````markdown
# Hooks and Event Capture Architecture

> Low-latency event capture system using Claude Code hooks without blocking user interactions

---
title: Hooks and Event Capture Architecture
category: architecture
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [hooks, event-capture, performance, async, non-blocking]
---

## Overview

The Hooks and Event Capture system captures every Claude Code interaction (user prompts and agent responses) without impacting user experience. It achieves sub-100ms performance through fire-and-forget async design, persistent queuing, and graceful error handling.

**Core Principle**: Never block the user. Capture everything, fail silently with logging.

## Goals

- Capture 100% of user-agent interactions
- Hook execution < 100ms (p95)
- Non-blocking design (fire-and-forget)
- Crash-safe event persistence
- Zero user-visible errors
- Idempotent event processing

## Non-Goals

- Real-time processing (use async queue)
- Perfect ordering across crashes (eventual consistency OK)
- Capturing internal LLM chain-of-thought (not accessible)
- Network-based event streaming (local-first)

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     Claude Code                              │
│                 (User Interaction Layer)                     │
└───────────┬─────────────────────────────┬───────────────────┘
            │                              │
            ▼ UserPromptSubmit             ▼ Stop
   ┌────────────────┐            ┌────────────────┐
   │  Hook Handler  │            │  Hook Handler  │
   │  (< 50ms)      │            │  (< 50ms)      │
   └────────┬───────┘            └───────┬────────┘
            │                             │
            ├─────────────────────────────┘
            │
            ▼ serialize + enqueue (fire-and-forget)
   ┌────────────────────────────────────────────┐
   │         Event Collector                     │
   │  ┌──────────────────────────────────────┐  │
   │  │  - Assign conversation_id            │  │
   │  │  - Assign message_id                 │  │
   │  │  - Add sequence number               │  │
   │  │  - Add timestamps                    │  │
   │  │  - Add idempotency_key               │  │
   │  └──────────────────────────────────────┘  │
   └────────────┬───────────────────────────────┘
                │
                ▼ persist (WAL mode)
   ┌────────────────────────────────────────────┐
   │      SQLite Event Queue                     │
   │  ┌──────────────────────────────────────┐  │
   │  │  events table                        │  │
   │  │  - id, conversation_id, message_id   │  │
   │  │  - type, role, content, sequence     │  │
   │  │  - idempotency_key, timestamps       │  │
   │  │  - status (pending/processing/done)  │  │
   │  └──────────────────────────────────────┘  │
   └────────────┬───────────────────────────────┘
                │
                ▼ async worker picks up
   ┌────────────────────────────────────────────┐
   │    Sanitization Job Queue                   │
   │    (downstream processing)                  │
   └────────────────────────────────────────────┘
```

## Event Schema

### Event Structure

```typescript
interface CapturedEvent {
  // Identity
  id: string;                    // UUID v4
  conversation_id: string;       // Stable conversation identifier
  message_id: string;            // Unique message identifier
  idempotency_key: string;       // For deduplication

  // Content
  type: 'user_prompt' | 'agent_response';
  role: 'user' | 'assistant';
  content: string;               // Raw content (NOT sanitized yet)
  tool_calls?: ToolCall[];       // Tool invocations
  attachments?: Attachment[];    // File attachments

  // Metadata
  sequence: number;              // Order within conversation
  session_id: string;            // Claude Code session ID
  project_id?: string;           // Project identifier
  created_at: number;            // Unix timestamp (ms)
  hook_name: string;             // 'UserPromptSubmit' | 'Stop'

  // Versioning
  client_version: string;        // Claude Code version
  policy_version: string;        // Sanitization policy version

  // Processing
  status: 'pending' | 'processing' | 'completed' | 'failed';
  processed_at?: number;
  error?: string;
}

interface ToolCall {
  tool: string;
  input: any;
  output?: any;
}

interface Attachment {
  name: string;
  path: string;
  mime_type: string;
  size: number;
}
```

## Hook Implementation

### UserPromptSubmit Hook

**File**: `.claude/hooks/user-prompt-submit.ts`

```typescript
#!/usr/bin/env node
import { captureEvent } from './lib/event-collector';

/**
 * UserPromptSubmit hook
 * Executes BEFORE Claude processes the user's prompt
 * Performance budget: < 50ms
 */
async function main() {
  const startTime = performance.now();

  try {
    // Read hook payload from stdin
    const payload = await readStdin();
    const hookData = JSON.parse(payload);

    // Fire-and-forget event capture
    captureEvent({
      type: 'user_prompt',
      role: 'user',
      content: hookData.prompt,
      tool_calls: hookData.toolCalls,
      attachments: hookData.attachments,
      session_id: hookData.sessionId,
      project_id: hookData.projectId
    }).catch(error => {
      // Log error but don't throw (never block user)
      console.error('[Hook Error]', error);
    });

    // Always succeed quickly
    const duration = performance.now() - startTime;
    if (duration > 50) {
      console.warn(`[Hook Warning] Execution took ${duration}ms (budget: 50ms)`);
    }

  } catch (error) {
    // Silent failure - log only
    console.error('[Hook Critical Error]', error);
  }

  // Exit successfully (never block)
  process.exit(0);
}

main();
```

### Stop Hook

**File**: `.claude/hooks/stop.ts`

```typescript
#!/usr/bin/env node
import { captureEvent } from './lib/event-collector';

/**
 * Stop hook
 * Executes AFTER Claude completes its response
 * Performance budget: < 50ms
 */
async function main() {
  const startTime = performance.now();

  try {
    const payload = await readStdin();
    const hookData = JSON.parse(payload);

    // Fire-and-forget event capture
    captureEvent({
      type: 'agent_response',
      role: 'assistant',
      content: hookData.response,
      tool_calls: hookData.toolCalls,
      session_id: hookData.sessionId,
      project_id: hookData.projectId
    }).catch(error => {
      console.error('[Hook Error]', error);
    });

    const duration = performance.now() - startTime;
    if (duration > 50) {
      console.warn(`[Hook Warning] Execution took ${duration}ms (budget: 50ms)`);
    }

  } catch (error) {
    console.error('[Hook Critical Error]', error);
  }

  process.exit(0);
}

main();
```

### Hook Configuration

**File**: `.claude/hooks.json`

```json
{
  "hooks": {
    "UserPromptSubmit": {
      "script": ".claude/hooks/user-prompt-submit.ts",
      "enabled": true,
      "timeout": 100
    },
    "Stop": {
      "script": ".claude/hooks/stop.ts",
      "enabled": true,
      "timeout": 100
    }
  },
  "config": {
    "eventQueuePath": "${PROJECT_ROOT}/.data/events.db",
    "maxBufferSize": 1000,
    "fallbackToSampling": true,
    "samplingRate": 0.1
  }
}
```

## Event Collector

### Non-Blocking Design

```typescript
// lib/event-collector.ts
import { EventEmitter } from 'events';
import { v4 as uuid } from 'uuid';

/**
 * Fire-and-forget event capture
 * Returns immediately, processes async
 */
export async function captureEvent(event: Partial<CapturedEvent>): Promise<void> {
  // Immediate return - don't await
  setImmediate(() => {
    processEvent(event).catch(error => {
      logger.error('Event processing failed', { error, event });
    });
  });

  // Return immediately (< 1ms)
  return;
}

/**
 * Async event processing
 * Runs in background, never blocks caller
 */
async function processEvent(event: Partial<CapturedEvent>): Promise<void> {
  try {
    // Enrich event with metadata
    const enrichedEvent: CapturedEvent = {
      id: uuid(),
      conversation_id: getOrCreateConversationId(event),
      message_id: uuid(),
      idempotency_key: generateIdempotencyKey(event),
      sequence: getNextSequence(event.conversation_id!),
      session_id: event.session_id || getSessionId(),
      created_at: Date.now(),
      hook_name: event.type === 'user_prompt' ? 'UserPromptSubmit' : 'Stop',
      client_version: getClientVersion(),
      policy_version: getPolicyVersion(),
      status: 'pending',
      ...event
    } as CapturedEvent;

    // Persist to queue (< 10ms with WAL)
    await eventQueue.enqueue(enrichedEvent);

    logger.info('Event captured', {
      conversation_id: enrichedEvent.conversation_id,
      message_id: enrichedEvent.message_id,
      type: enrichedEvent.type
    });

  } catch (error) {
    logger.error('Event enrichment failed', { error, event });
    throw error;
  }
}
```

### Conversation Management

```typescript
// lib/conversation-tracker.ts
const conversationCache = new Map<string, string>();

/**
 * Get or create stable conversation ID
 * Uses session_id + heuristics to group messages
 */
function getOrCreateConversationId(event: Partial<CapturedEvent>): string {
  const sessionId = event.session_id || getSessionId();

  // Check cache
  if (conversationCache.has(sessionId)) {
    return conversationCache.get(sessionId)!;
  }

  // Create new conversation ID
  const conversationId = uuid();
  conversationCache.set(sessionId, conversationId);

  return conversationId;
}

/**
 * Get next sequence number for conversation
 * Ensures ordering within conversation
 */
function getNextSequence(conversationId: string): number {
  // Query database for max sequence
  const result = db.prepare(`
    SELECT COALESCE(MAX(sequence), 0) as max_seq
    FROM events
    WHERE conversation_id = ?
  `).get(conversationId);

  return (result?.max_seq || 0) + 1;
}
```

## Event Queue Persistence

### SQLite Configuration

```typescript
// lib/event-queue.ts
import Database from 'better-sqlite3';

class EventQueue {
  private db: Database.Database;

  constructor(dbPath: string) {
    this.db = new Database(dbPath);

    // Performance optimizations
    this.db.pragma('journal_mode = WAL');        // Write-ahead logging
    this.db.pragma('synchronous = NORMAL');      // Balanced safety/speed
    this.db.pragma('foreign_keys = ON');
    this.db.pragma('busy_timeout = 5000');

    this.initializeSchema();
  }

  private initializeSchema(): void {
    this.db.exec(`
      CREATE TABLE IF NOT EXISTS events (
        id TEXT PRIMARY KEY,
        conversation_id TEXT NOT NULL,
        message_id TEXT NOT NULL,
        idempotency_key TEXT NOT NULL UNIQUE,

        type TEXT NOT NULL,
        role TEXT NOT NULL,
        content TEXT NOT NULL,
        tool_calls TEXT,
        attachments TEXT,

        sequence INTEGER NOT NULL,
        session_id TEXT NOT NULL,
        project_id TEXT,
        created_at INTEGER NOT NULL,
        hook_name TEXT NOT NULL,

        client_version TEXT NOT NULL,
        policy_version TEXT NOT NULL,

        status TEXT NOT NULL DEFAULT 'pending',
        processed_at INTEGER,
        error TEXT,

        CHECK(type IN ('user_prompt', 'agent_response')),
        CHECK(role IN ('user', 'assistant')),
        CHECK(status IN ('pending', 'processing', 'completed', 'failed'))
      );

      CREATE INDEX IF NOT EXISTS idx_events_conversation
        ON events(conversation_id, sequence);

      CREATE INDEX IF NOT EXISTS idx_events_status
        ON events(status, created_at);

      CREATE INDEX IF NOT EXISTS idx_events_session
        ON events(session_id, created_at);
    `);
  }

  /**
   * Enqueue event (< 10ms with WAL)
   * Idempotent via idempotency_key
   */
  async enqueue(event: CapturedEvent): Promise<void> {
    const insert = this.db.prepare(`
      INSERT OR IGNORE INTO events (
        id, conversation_id, message_id, idempotency_key,
        type, role, content, tool_calls, attachments,
        sequence, session_id, project_id, created_at, hook_name,
        client_version, policy_version, status
      ) VALUES (
        ?, ?, ?, ?,
        ?, ?, ?, ?, ?,
        ?, ?, ?, ?, ?,
        ?, ?, ?
      )
    `);

    insert.run(
      event.id,
      event.conversation_id,
      event.message_id,
      event.idempotency_key,
      event.type,
      event.role,
      event.content,
      JSON.stringify(event.tool_calls || []),
      JSON.stringify(event.attachments || []),
      event.sequence,
      event.session_id,
      event.project_id,
      event.created_at,
      event.hook_name,
      event.client_version,
      event.policy_version,
      event.status
    );
  }

  /**
   * Dequeue pending events for processing
   */
  async dequeue(limit: number = 10): Promise<CapturedEvent[]> {
    const events = this.db.prepare(`
      SELECT * FROM events
      WHERE status = 'pending'
      ORDER BY created_at ASC
      LIMIT ?
    `).all(limit);

    return events.map(row => ({
      ...row,
      tool_calls: JSON.parse(row.tool_calls || '[]'),
      attachments: JSON.parse(row.attachments || '[]')
    }));
  }
}
```

### Idempotency

```typescript
/**
 * Generate idempotency key for deduplication
 * Ensures retry-safe operations
 */
function generateIdempotencyKey(event: Partial<CapturedEvent>): string {
  const components = [
    event.session_id,
    event.type,
    event.content?.slice(0, 100), // First 100 chars
    event.created_at
  ];

  return createHash('sha256')
    .update(components.join('|'))
    .digest('hex');
}
```

## Performance Optimizations

### Backpressure Handling

```typescript
// lib/backpressure.ts
class BackpressureManager {
  private bufferSize: number = 0;
  private maxBufferSize: number = 1000;
  private samplingRate: number = 1.0;

  async handleEvent(event: Partial<CapturedEvent>): Promise<boolean> {
    // Check buffer size
    this.bufferSize = await this.getQueueSize();

    if (this.bufferSize > this.maxBufferSize) {
      // Apply sampling
      if (Math.random() > this.samplingRate) {
        logger.warn('Event dropped due to backpressure', {
          bufferSize: this.bufferSize,
          samplingRate: this.samplingRate
        });
        return false;
      }
    }

    // Process event
    await captureEvent(event);
    return true;
  }

  private async getQueueSize(): Promise<number> {
    const result = db.prepare(`
      SELECT COUNT(*) as count
      FROM events
      WHERE status = 'pending'
    `).get();

    return result?.count || 0;
  }
}
```

### Streaming Handling

```typescript
// lib/streaming-handler.ts
class StreamingHandler {
  private partialBuffers = new Map<string, string>();

  handlePartial(sessionId: string, chunk: string): void {
    // Buffer partial chunks
    const existing = this.partialBuffers.get(sessionId) || '';
    this.partialBuffers.set(sessionId, existing + chunk);
  }

  async handleComplete(sessionId: string, final?: string): Promise<void> {
    // Get buffered content
    const buffered = this.partialBuffers.get(sessionId) || '';
    const content = final || buffered;

    // Capture complete event
    await captureEvent({
      session_id: sessionId,
      type: 'agent_response',
      role: 'assistant',
      content
    });

    // Clear buffer
    this.partialBuffers.delete(sessionId);
  }

  // Cleanup stale buffers
  cleanup(): void {
    const now = Date.now();
    for (const [sessionId, _] of this.partialBuffers) {
      const lastUpdate = this.getLastUpdate(sessionId);
      if (now - lastUpdate > 60000) { // 1 minute timeout
        this.partialBuffers.delete(sessionId);
      }
    }
  }
}
```

## Error Handling

### Graceful Degradation

```typescript
// lib/error-handler.ts
class HookErrorHandler {
  async safeExecute<T>(
    operation: () => Promise<T>,
    fallback: T
  ): Promise<T> {
    try {
      return await operation();
    } catch (error) {
      logger.error('Hook operation failed', { error });

      // Emit metric
      metrics.increment('hook.error', {
        operation: operation.name
      });

      // Return fallback (never throw to caller)
      return fallback;
    }
  }

  handleCriticalError(error: Error): void {
    // Log to file (don't rely on network)
    appendFileSync(
      '.data/hook-errors.log',
      JSON.stringify({ timestamp: Date.now(), error: error.message }) + '\n'
    );

    // Attempt to emit metric
    metrics.increment('hook.critical_error').catch(() => {});
  }
}
```

### Recovery Strategies

```typescript
// lib/recovery.ts
class RecoveryManager {
  /**
   * Recover from crashed queue
   * Reset stuck events to pending
   */
  async recoverQueue(): Promise<void> {
    const updated = db.prepare(`
      UPDATE events
      SET status = 'pending', error = 'Recovered from crash'
      WHERE status = 'processing'
        AND created_at < ?
    `).run(Date.now() - 300000); // 5 minutes ago

    logger.info('Queue recovery completed', {
      recovered: updated.changes
    });
  }

  /**
   * Cleanup old completed events
   */
  async cleanup(retentionDays: number = 7): Promise<void> {
    const cutoff = Date.now() - (retentionDays * 24 * 60 * 60 * 1000);

    const deleted = db.prepare(`
      DELETE FROM events
      WHERE status = 'completed'
        AND processed_at < ?
    `).run(cutoff);

    logger.info('Event cleanup completed', {
      deleted: deleted.changes
    });
  }
}
```

## Security Considerations

### Hook Sandboxing

```typescript
// Deny network egress in hooks
process.env.NODE_ENV = 'hook';
process.env.NO_PROXY = '*';

// Restrict file system access
const allowedPaths = [
  path.join(PROJECT_ROOT, '.data'),
  path.join(PROJECT_ROOT, '.claude')
];

function validatePath(filePath: string): boolean {
  const resolved = path.resolve(filePath);
  return allowedPaths.some(allowed => resolved.startsWith(allowed));
}
```

### Log Sanitization

```typescript
// Never log sensitive data in hooks
function sanitizeLogData(data: any): any {
  const sanitized = { ...data };

  // Remove potential PII fields
  delete sanitized.content;
  delete sanitized.tool_calls;
  delete sanitized.attachments;

  return sanitized;
}

logger.info('Event captured', sanitizeLogData(event));
```

## Monitoring

### Key Metrics

```typescript
// Metrics to track
metrics.timing('hook.execution_time', duration);
metrics.increment('hook.success');
metrics.increment('hook.error');
metrics.gauge('event_queue.size', queueSize);
metrics.timing('event_queue.enqueue_time', enqueueDuration);
```

### Health Checks

```typescript
// Health check endpoint
async function healthCheck(): Promise<HealthStatus> {
  return {
    queue: {
      size: await getQueueSize(),
      oldestEvent: await getOldestEventAge()
    },
    hooks: {
      enabled: areHooksEnabled(),
      lastExecution: getLastExecutionTime()
    }
  };
}
```

## Related Documents

### Architecture
- [Global Context Network](./architecture-global-context-network-2025-01-16.md)
- [Sanitization Pipeline](./architecture-sanitization-pipeline-2025-01-16.md)
- [Database Schema](./architecture-database-schema-2025-01-16.md)

### Reference
- [Event Schema Reference](../reference/reference-event-schema-2025-01-16.md)
- [Hook Configuration](../reference/reference-hook-configuration-2025-01-16.md)
````

## File: architecture/architecture-learning-extraction-2025-01-16.md
````markdown
# Learning Extraction Architecture

> Automated system for extracting valuable, actionable learnings from sanitized conversations using AI-powered analysis and quality validation

---
title: Learning Extraction Architecture
category: architecture
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [learning-extraction, ai, quality-scoring, deduplication, evidence]
---

## Overview

The Learning Extraction system analyzes sanitized conversations to identify and extract valuable, reusable learnings. It transforms raw conversation data into structured knowledge that can be shared across the Global Context Network, enabling AI agents to learn from each other's experiences.

**Core Value Proposition**: Mine actionable knowledge from conversations, not just capture transcripts.

### Alignment with Standards

This architecture is 100% compliant with [STANDARDS.md](../STANDARDS.md):
- Uses `learnings` table with ULID IDs
- Processes via `job_queue` with canonical status enums
- Operates on SANITIZED data only (never raw)
- Async processing with quality gates
- Evidence-based (links to source messages)

## Goals

- Extract high-value learnings from 20-30% of conversations
- Quality threshold: confidence score ≥ 0.6
- Deduplication: Prevent similar learnings from being stored
- Evidence linkage: Every learning traceable to source messages
- Performance: < 5s per conversation (p95)
- Categorization: Automatic classification into 8 learning types

## Non-Goals

- Capturing every conversation (most lack valuable learnings)
- Real-time extraction (async processing is acceptable)
- Perfect recall (precision > recall for quality)
- General summarization (focus on actionable insights)

## Architecture

```
┌──────────────────────────────────────────────────────────────┐
│                    Sanitized Messages                         │
│                   (SQLite: messages table)                    │
└────────────────┬─────────────────────────────────────────────┘
                 │
                 ▼ Trigger: conversation marked complete
      ┌──────────────────────────────────┐
      │      Job Queue (job_queue)        │
      │  Type: extract_learning           │
      │  Status: queued                   │
      └────────────┬─────────────────────┘
                   │
                   ▼ Worker picks up job
      ┌──────────────────────────────────┐
      │  Conversation Value Analyzer      │
      │  ┌────────────────────────────┐  │
      │  │ - Check length (≥3 messages)│  │
      │  │ - Check topics (coding?)    │  │
      │  │ - Check tools used          │  │
      │  │ - Estimate value score      │  │
      │  └─────────┬──────────────────┘  │
      └────────────┼─────────────────────┘
                   │
                   ├─── Low Value (< 0.3) ──> Skip (mark complete)
                   │
                   ▼ High Value (≥ 0.3)
      ┌──────────────────────────────────┐
      │   Category Detection              │
      │  ┌────────────────────────────┐  │
      │  │ Classify conversation into: │  │
      │  │ - pattern                   │  │
      │  │ - best_practice             │  │
      │  │ - anti_pattern              │  │
      │  │ - bug_fix                   │  │
      │  │ - optimization              │  │
      │  │ - tool_usage                │  │
      │  │ - workflow                  │  │
      │  │ - decision                  │  │
      │  └─────────┬──────────────────┘  │
      └────────────┼─────────────────────┘
                   │
                   ▼ For each detected category
      ┌──────────────────────────────────┐
      │  Category-Specific Extractors     │
      │  ┌────────────────────────────┐  │
      │  │ Pattern Extractor          │  │
      │  │ Best Practice Extractor    │  │
      │  │ Anti-Pattern Extractor     │  │
      │  │ Bug Fix Extractor          │  │
      │  │ Optimization Extractor     │  │
      │  │ Tool Usage Extractor       │  │
      │  │ Workflow Extractor         │  │
      │  │ Decision Extractor         │  │
      │  └─────────┬──────────────────┘  │
      └────────────┼─────────────────────┘
                   │
                   ▼ Raw learnings extracted
      ┌──────────────────────────────────┐
      │   Quality Scoring System          │
      │  ┌────────────────────────────┐  │
      │  │ - Actionability (0-1)      │  │
      │  │ - Generalizability (0-1)   │  │
      │  │ - Clarity (0-1)            │  │
      │  │ - Evidence strength (0-1)  │  │
      │  │ → Confidence = weighted avg│  │
      │  └─────────┬──────────────────┘  │
      └────────────┼─────────────────────┘
                   │
                   ├─── Low Confidence (< 0.6) ──> Discard
                   │
                   ▼ High Confidence (≥ 0.6)
      ┌──────────────────────────────────┐
      │   Deduplication Check             │
      │  ┌────────────────────────────┐  │
      │  │ 1. Embedding similarity     │  │
      │  │    (cosine > 0.85 = dup)    │  │
      │  │ 2. SimHash comparison       │  │
      │  │    (Hamming < 3 = dup)      │  │
      │  │ 3. Key phrase overlap       │  │
      │  │    (>80% = dup)             │  │
      │  └─────────┬──────────────────┘  │
      └────────────┼─────────────────────┘
                   │
                   ├─── Duplicate Found ──> Merge or Skip
                   │
                   ▼ Unique Learning
      ┌──────────────────────────────────┐
      │   Evidence Linkage                │
      │  ┌────────────────────────────┐  │
      │  │ - Link to source messages   │  │
      │  │ - Extract key quotes        │  │
      │  │ - Reference tool calls      │  │
      │  │ - Store context metadata    │  │
      │  └─────────┬──────────────────┘  │
      └────────────┼─────────────────────┘
                   │
                   ▼ Store in database
      ┌──────────────────────────────────┐
      │   Learnings Table (SQLite)        │
      │  ┌────────────────────────────┐  │
      │  │ - id (ULID)                 │  │
      │  │ - conversation_id           │  │
      │  │ - category                  │  │
      │  │ - title, content            │  │
      │  │ - confidence, tags          │  │
      │  │ - evidence (message_ids)    │  │
      │  │ - created_at (ISO-8601)     │  │
      │  └────────────────────────────┘  │
      └──────────────────────────────────┘
                   │
                   ▼ Available for query
      ┌──────────────────────────────────┐
      │   MCP Server (Query Interface)    │
      │   + Future: Upload to Network     │
      └──────────────────────────────────┘
```

## Component Specifications

### 1. Conversation Value Analyzer

**Purpose**: Determine if a conversation is worth extracting learnings from

**Input**: Conversation metadata from `conversations` and `messages` tables

**Output**: Value score (0-1) and decision (process / skip)

**Heuristics**:
```typescript
interface ValueAnalysis {
  score: number;           // 0-1 composite score
  reasons: string[];       // Why this score
  shouldProcess: boolean;  // score >= 0.3
}

async function analyzeConversationValue(
  conversationId: string
): Promise<ValueAnalysis> {
  const conversation = await getConversation(conversationId);
  const messages = await getMessages(conversationId);

  let score = 0;
  const reasons: string[] = [];

  // Heuristic 1: Message count (min 3 for context)
  if (messages.length < 3) {
    return { score: 0, reasons: ['Too few messages'], shouldProcess: false };
  }

  // Heuristic 2: Contains code blocks (strong signal)
  const codeBlockCount = messages.filter(m =>
    m.content.includes('```')
  ).length;
  if (codeBlockCount >= 2) {
    score += 0.3;
    reasons.push('Contains code examples');
  }

  // Heuristic 3: Tool usage (file edits, searches)
  const toolCallCount = messages.filter(m =>
    m.tool_calls && m.tool_calls.length > 0
  ).length;
  if (toolCallCount >= 3) {
    score += 0.25;
    reasons.push('Substantial tool usage');
  }

  // Heuristic 4: Conversation length (complexity proxy)
  const totalTokens = estimateTokens(messages);
  if (totalTokens >= 2000) {
    score += 0.2;
    reasons.push('Substantial conversation');
  }

  // Heuristic 5: Technical keywords
  const technicalKeywords = [
    'error', 'bug', 'fix', 'implement', 'optimize',
    'pattern', 'architecture', 'design', 'test'
  ];
  const keywordMatches = messages.filter(m =>
    technicalKeywords.some(kw => m.content.toLowerCase().includes(kw))
  ).length;
  if (keywordMatches >= 2) {
    score += 0.25;
    reasons.push('Technical problem-solving detected');
  }

  return {
    score: Math.min(score, 1.0),
    reasons,
    shouldProcess: score >= 0.3
  };
}
```

**Performance**: < 100ms (simple heuristics, no AI call)

---

### 2. Category Detection

**Purpose**: Classify conversation into one or more learning categories

**Input**: Sanitized conversation messages

**Output**: List of detected categories with confidence

**Categories** (from STANDARDS.md):
1. `pattern`: Code patterns and architectural approaches
2. `best_practice`: Recommended ways to do things
3. `anti_pattern`: Things to avoid
4. `bug_fix`: Problem-solving strategies
5. `optimization`: Performance improvements
6. `tool_usage`: How to use specific tools/libraries
7. `workflow`: Development workflows and processes
8. `decision`: Architecture decisions and trade-offs

**Implementation**:
```typescript
interface CategoryDetection {
  category: LearningCategory;
  confidence: number;     // 0-1
  signals: string[];      // Why detected
}

async function detectCategories(
  conversationId: string
): Promise<CategoryDetection[]> {
  const messages = await getMessages(conversationId);

  // Preselect relevant spans (keep within token budget ~2k tokens)
  const relevantMessages = preselectRelevantSpans(messages);
  const conversationText = relevantMessages
    .map(m => `[${m.role}]: ${m.content}`)
    .join('\n\n');

  // AI-powered classification with strict JSON output
  const prompt = `
Analyze this technical conversation and identify learning categories present.

CONVERSATION:
${conversationText}

CATEGORIES (select up to 2 most relevant):
- pattern: Reusable code patterns or architectural approaches
- best_practice: Recommended ways to accomplish tasks
- anti_pattern: Things to avoid or common mistakes
- bug_fix: Debugging strategies or error resolutions
- optimization: Performance improvements or efficiency gains
- tool_usage: How to use specific tools, libraries, or frameworks
- workflow: Development processes or workflows
- decision: Architecture decisions with trade-offs

OUTPUT (JSON only, no explanations):
{
  "categories": [
    {
      "category": "pattern",
      "confidence": 0.85,
      "signals": ["Uses factory pattern", "Implements dependency injection"]
    }
  ]
}

RULES:
- Return ONLY valid JSON, no markdown, no explanations
- Maximum 2 categories (highest confidence only)
- Only include categories with confidence >= 0.5
- No PII, content is sanitized
- No chain-of-thought, be concise
`;

  const response = await callClaudeAPI({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1000,
    temperature: 0.2,  // Low temp for consistency
    system: 'You are a strict JSON-only extractor. Output only valid minified JSON that conforms to the schema. Do not include explanations, code fences, or additional text. Refuse to include any PII. Content is sanitized.',
    messages: [{ role: 'user', content: prompt }]
  });

  // Robust JSON parsing
  const parsed = parseRobustJSON(response.content[0].text);
  const categories = parsed.categories
    .filter(c => c.confidence >= 0.5)
    .slice(0, 2); // Cap at 2 categories

  return categories;
}

// Preselect relevant conversation spans to stay within token budget
function preselectRelevantSpans(messages: Message[]): Message[] {
  const MAX_TOKENS = 2000;
  const selected: Message[] = [];

  // Priority 1: Messages with code blocks
  const withCode = messages.filter(m => m.content.includes('```'));
  selected.push(...withCode);

  // Priority 2: Messages with tool calls
  const withTools = messages.filter(m => m.tool_calls && m.tool_calls.length > 0);
  selected.push(...withTools);

  // Priority 3: Messages with errors/problems
  const withErrors = messages.filter(m =>
    /error|exception|bug|fix|problem/i.test(m.content)
  );
  selected.push(...withErrors);

  // Dedupe and sort by original order
  const unique = Array.from(new Set(selected));
  unique.sort((a, b) => messages.indexOf(a) - messages.indexOf(b));

  // Truncate to token budget
  let totalTokens = 0;
  const final: Message[] = [];
  for (const msg of unique) {
    const msgTokens = estimateTokens([msg]);
    if (totalTokens + msgTokens > MAX_TOKENS) break;
    final.push(msg);
    totalTokens += msgTokens;
  }

  return final.length > 0 ? final : messages.slice(0, 5); // Fallback: first 5
}

// Robust JSON parser - extracts largest valid JSON object
function parseRobustJSON(text: string): any {
  // Try direct parse first
  try {
    return JSON.parse(text);
  } catch {
    // Find JSON object boundaries
    const start = text.indexOf('{');
    const end = text.lastIndexOf('}');
    if (start === -1 || end === -1) {
      throw new Error('No JSON object found in response');
    }
    return JSON.parse(text.slice(start, end + 1));
  }
}
```

**Performance**: < 2s (single AI call with streaming)

**Caching**: Cache category detection per conversation_id

**Token Budget**: Preselect relevant spans to stay within ~2k input tokens

---

### 3. Category-Specific Extractors

**Purpose**: Extract structured learning content for each category

**Implementation Pattern**:
```typescript
interface LearningExtraction {
  category: LearningCategory;  // Category this learning belongs to
  title: string;               // Concise title (≤100 chars)
  content: string;             // Full learning description (≥100 chars)
  tags: string[];              // Relevant tags (3-10)
  evidence: string[];          // Message IDs that support this learning
  confidence_factors: {
    actionability: number;      // 0-1
    generalizability: number;   // 0-1
    clarity: number;            // 0-1
    evidence_strength: number;  // 0-1
  };
}

// Example: Pattern Extractor
async function extractPattern(
  conversationId: string
): Promise<LearningExtraction> {
  const messages = await getMessages(conversationId);
  const conversationText = messages.map(m =>
    `[${m.role}]: ${m.content}`
  ).join('\n\n');

  const prompt = `
Extract a reusable code pattern from this conversation.

CONVERSATION:
${conversationText}

EXTRACTION REQUIREMENTS:
1. Title: Concise name for the pattern (e.g., "Factory Pattern for Plugin System")
2. Content:
   - WHAT the pattern is (1-2 sentences)
   - WHY it's useful (benefits)
   - WHEN to use it (use cases)
   - HOW to implement it (key steps or code structure)
   - Minimum 100 characters
3. Tags: 3-10 relevant tags (e.g., ["factory-pattern", "dependency-injection", "typescript"])
4. Evidence: Which parts of the conversation demonstrate this pattern? (quote message indices)

QUALITY CRITERIA:
- Actionability: Can someone apply this immediately? (0-1)
- Generalizability: Useful beyond this specific case? (0-1)
- Clarity: Is it clearly explained? (0-1)
- Evidence Strength: Well-supported by conversation? (0-1)

OUTPUT (JSON):
{
  "title": "...",
  "content": "...",
  "tags": ["...", "..."],
  "evidence_message_indices": [0, 3, 5],
  "confidence_factors": {
    "actionability": 0.9,
    "generalizability": 0.85,
    "clarity": 0.8,
    "evidence_strength": 0.9
  }
}
`;

  const response = await callClaudeAPI({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 2000,
    temperature: 0.2,
    messages: [{ role: 'user', content: prompt }]
  });

  const parsed = JSON.parse(response.content[0].text);

  // Map message indices to message IDs
  const evidenceMessageIds = parsed.evidence_message_indices.map(
    idx => messages[idx]?.id
  ).filter(Boolean);

  return {
    category: 'pattern',
    title: parsed.title,
    content: parsed.content,
    tags: parsed.tags,
    evidence: evidenceMessageIds,
    confidence_factors: parsed.confidence_factors
  };
}
```

**Extractor Variations**:

Each category has a specialized extractor with tailored prompts:

- **Best Practice Extractor**: Focus on "why this approach is recommended"
- **Anti-Pattern Extractor**: Focus on "what problem this causes" and "better alternatives"
- **Bug Fix Extractor**: Focus on "symptoms → root cause → solution"
- **Optimization Extractor**: Focus on "before/after performance" and "trade-offs"
- **Tool Usage Extractor**: Focus on "how to use X to accomplish Y"
- **Workflow Extractor**: Focus on "step-by-step process"
- **Decision Extractor**: Focus on "options considered → chosen option → rationale"

**Performance**: < 3s per extractor (p95)

---

### 4. Quality Scoring System

**Purpose**: Calculate confidence score to filter low-quality learnings

**Formula**:
```typescript
function calculateConfidence(
  factors: ConfidenceFactors,
  learning: LearningExtraction
): number {
  // Automatic guards and penalties (fail fast)

  // Length penalty: reject if too short
  if (learning.content.length < 100) {
    return 0; // Will be rejected by threshold
  }

  // Genericity penalty: reject trivial patterns
  const genericPatterns = [
    /^write tests?$/i,
    /^use logging$/i,
    /^follow best practices$/i,
    /^add comments$/i,
    /^handle errors$/i
  ];
  const isTrivial = genericPatterns.some(p =>
    p.test(learning.title) || p.test(learning.content)
  );
  if (isTrivial) {
    return 0; // Reject generic advice
  }

  // Evidence cross-check: penalize weak evidence
  let evidenceScore = factors.evidence_strength;
  if (!learning.evidence || learning.evidence.length === 0) {
    evidenceScore = 0.4; // Cap at 0.4 for missing evidence
  }

  // Weighted average (weights sum to 1.0)
  const weights = {
    actionability: 0.35,      // Most important: can you use it?
    generalizability: 0.30,   // Second: broadly applicable?
    clarity: 0.20,            // Third: well-explained?
    evidence_strength: 0.15   // Fourth: well-supported?
  };

  const confidence =
    factors.actionability * weights.actionability +
    factors.generalizability * weights.generalizability +
    factors.clarity * weights.clarity +
    evidenceScore * weights.evidence_strength;

  return confidence; // Store full float precision
}

// Quality thresholds (category-aware)
const MIN_CONFIDENCE_DEFAULT = 0.6;
const MIN_CONFIDENCE_GENERIC_CATEGORIES = 0.65; // Higher bar for best_practice, anti_pattern

function meetsQualityThreshold(
  confidence: number,
  category: LearningCategory
): boolean {
  const threshold = ['best_practice', 'anti_pattern'].includes(category)
    ? MIN_CONFIDENCE_GENERIC_CATEGORIES
    : MIN_CONFIDENCE_DEFAULT;

  return confidence >= threshold;
}
```

**Factor Definitions**:

1. **Actionability** (0-1):
   - 1.0: Step-by-step instructions or clear code example
   - 0.7: General guidance with concrete suggestions
   - 0.4: Vague recommendations without specifics
   - 0.0: Purely theoretical with no practical application

2. **Generalizability** (0-1):
   - 1.0: Applies to many projects/languages/contexts
   - 0.7: Applies to specific domain but broadly useful
   - 0.4: Very project-specific
   - 0.0: Only relevant to one specific case

3. **Clarity** (0-1):
   - 1.0: Crystal clear, no ambiguity
   - 0.7: Mostly clear with minor gaps
   - 0.4: Confusing or requires significant interpretation
   - 0.0: Incomprehensible

4. **Evidence Strength** (0-1):
   - 1.0: Multiple concrete examples from conversation
   - 0.7: One good example with supporting context
   - 0.4: Weak connection to conversation
   - 0.0: No evidence in conversation

---

### 5. Deduplication Strategy

**Purpose**: Prevent storing near-duplicate learnings

**Three-Layer Approach** (ordered by speed and reliability):

#### Layer 1: Canonical Hash (Primary - Required by Schema)
```typescript
function canonicalizeText(text: string): string {
  return text
    .toLowerCase()
    .trim()
    // Collapse whitespace
    .replace(/\s+/g, ' ')
    // Remove volatile numerals (timestamps, issue IDs like #123)
    .replace(/\b\d{4}-\d{2}-\d{2}\b/g, 'DATE')
    .replace(/#\d+/g, 'ISSUE')
    // Strip URL query params and tokens
    .replace(/\?[^\s]+/g, '')
    .replace(/[&?]token=[^\s&]+/g, '');
}

function computeDedupeHash(learning: LearningExtraction): string {
  const canonical = canonicalizeText(
    `${learning.category}:${learning.title} ${learning.content}`
  );
  return sha256(canonical);
}

async function checkCanonicalDuplicate(
  learning: LearningExtraction
): Promise<boolean> {
  const dedupeHash = computeDedupeHash(learning);

  try {
    // This insert will fail if UNIQUE constraint on dedupe_hash is violated
    await db.prepare(`
      INSERT INTO learnings (dedupe_hash, ...) VALUES (?, ...)
    `).run(dedupeHash, /* ... */);
    return false; // Not a duplicate
  } catch (error) {
    if (error.code === 'SQLITE_CONSTRAINT' && error.message.includes('dedupe_hash')) {
      // Duplicate detected - optionally merge evidence
      await mergeEvidenceIfNeeded(dedupeHash, learning);
      return true;
    }
    throw error; // Other error
  }
}

async function mergeEvidenceIfNeeded(
  dedupeHash: string,
  newLearning: LearningExtraction
): Promise<void> {
  // Optional: Update existing learning with supplemental evidence
  await db.transaction(() => {
    const existing = db.prepare(`
      SELECT id, source_message_ids, metadata, confidence
      FROM learnings WHERE dedupe_hash = ?
    `).get(dedupeHash);

    // Union source_message_ids
    const existingIds = JSON.parse(existing.source_message_ids);
    const newIds = newLearning.evidence;
    const mergedIds = Array.from(new Set([...existingIds, ...newIds]));

    // Update confidence (conservative: take max)
    const newConf = calculateConfidence(newLearning.confidence_factors, newLearning);
    const updatedConf = Math.max(existing.confidence, newConf);

    // Append to metadata.extraction_runs
    const metadata = JSON.parse(existing.metadata);
    metadata.extraction_runs = metadata.extraction_runs || [];
    metadata.extraction_runs.push({
      timestamp: new Date().toISOString(),
      new_sources: newIds.filter(id => !existingIds.includes(id)),
      confidence: newConf
    });

    db.prepare(`
      UPDATE learnings
      SET source_message_ids = ?,
          confidence = ?,
          metadata = ?
      WHERE id = ?
    `).run(
      JSON.stringify(mergedIds),
      updatedConf,
      JSON.stringify(metadata),
      existing.id
    );
  });
}
```

#### Layer 2: SimHash (Fast Approximate - Secondary)
```typescript
function checkSimHashSimilarity(
  newLearning: LearningExtraction,
  existingLearnings: Learning[]
): DuplicateMatch | null {
  const newHash = simhash(newLearning.content);

  for (const existing of existingLearnings) {
    if (existing.category !== newLearning.category) continue;

    // Read simhash from metadata (precomputed and stored)
    const metadata = JSON.parse(existing.metadata);
    const existingHash = metadata.signatures?.simhash;
    if (!existingHash) continue; // Skip if not computed

    const distance = hammingDistance(BigInt(newHash), BigInt(existingHash));

    // Hamming distance < 3 indicates very similar
    if (distance < 3) {
      return {
        type: 'simhash',
        matchId: existing.id,
        distance,
        action: 'skip' // Near-duplicate, skip insertion
      };
    }
  }

  return null;
}

// SimHash implementation (locality-sensitive hashing)
function simhash(text: string): bigint {
  const tokens = tokenize(text);
  const features = new Map<string, number>();

  // TF-IDF weighting
  for (const token of tokens) {
    features.set(token, (features.get(token) || 0) + 1);
  }

  // Hash each feature
  const V = new Array(64).fill(0);
  for (const [feature, weight] of features) {
    const hash = hashCode(feature);
    for (let i = 0; i < 64; i++) {
      if ((hash >> i) & 1) {
        V[i] += weight;
      } else {
        V[i] -= weight;
      }
    }
  }

  // Generate fingerprint
  let fingerprint = 0n;
  for (let i = 0; i < 64; i++) {
    if (V[i] > 0) {
      fingerprint |= (1n << BigInt(i));
    }
  }

  return fingerprint;
}
```

#### Layer 3: Embedding Similarity (Optional - Local Model Only)
```typescript
// Only use if local sentence-transformers model is available
// Do NOT use remote embeddings due to latency (exceeds 500ms budget)
async function checkEmbeddingSimilarity(
  newLearning: LearningExtraction,
  existingLearnings: Learning[]
): Promise<DuplicateMatch | null> {
  // Generate embedding for new learning (local model)
  const newEmbedding = await generateLocalEmbedding(
    newLearning.title + '\n' + newLearning.content
  );

  // Compare with existing learnings
  for (const existing of existingLearnings) {
    if (existing.category !== newLearning.category) continue;

    // Read embedding from metadata or compute
    const metadata = JSON.parse(existing.metadata);
    const existingEmbedding = metadata.signatures?.embedding ||
      await generateLocalEmbedding(existing.title + '\n' + existing.content);

    const similarity = cosineSimilarity(newEmbedding, existingEmbedding);

    if (similarity > 0.85) {
      return {
        type: 'embedding',
        matchId: existing.id,
        similarity,
        action: 'skip'
      };
    }
  }

  return null;
}

// Use local sentence-transformers model (e.g., all-MiniLM-L6-v2)
// For MVP: Use TF-IDF as fallback
async function generateLocalEmbedding(text: string): Promise<number[]> {
  // Option A: Local sentence-transformers via Python subprocess
  // Option B: TF-IDF (fast, deterministic)
  return tfidfVector(text);
}
```

**Deduplication Decision Logic** (ordered by speed):
```typescript
async function deduplicateCheck(
  newLearning: LearningExtraction
): Promise<DeduplicationResult> {
  // Layer 1: Canonical hash check (required, concurrency-safe)
  // This happens during INSERT via UNIQUE constraint
  // See checkCanonicalDuplicate() in storage section

  // Layer 2: SimHash for near-duplicates (fast, 100ms)
  const existingLearnings = await db.prepare(`
    SELECT id, metadata, category FROM learnings
    WHERE category = ?
    ORDER BY created_at DESC
    LIMIT 100
  `).all(newLearning.category);

  const simhashMatch = checkSimHashSimilarity(newLearning, existingLearnings);
  if (simhashMatch) {
    return {
      isDuplicate: true,
      reason: 'SimHash near-duplicate',
      matchId: simhashMatch.matchId
    };
  }

  // Layer 3: Embedding similarity (optional, local only)
  // Skip if no local model available or over time budget
  if (hasLocalEmbeddingModel()) {
    const embeddingMatch = await checkEmbeddingSimilarity(newLearning, existingLearnings);
    if (embeddingMatch?.action === 'skip') {
      return {
        isDuplicate: true,
        reason: 'High embedding similarity',
        matchId: embeddingMatch.matchId
      };
    }
  }

  return { isDuplicate: false };
}
```

---

### 6. Evidence Linkage

**Purpose**: Make every learning traceable to source messages

**Implementation**:
```typescript
interface Evidence {
  message_ids: string[];     // ULIDs of source messages
  quotes: Quote[];           // Key quotes from conversation
  context_summary: string;   // Brief context description
}

interface Quote {
  message_id: string;
  text: string;              // Extracted quote (≤200 chars)
  role: 'user' | 'assistant';
}

async function buildEvidence(
  extractedLearning: LearningExtraction,
  conversationId: string
): Promise<Evidence> {
  const messages = await getMessages(conversationId);

  // Get messages referenced in extraction
  const evidenceMessages = messages.filter(m =>
    extractedLearning.evidence.includes(m.id)
  );

  // Enforce bounds: 2-4 quotes, ≤200 chars each
  const MAX_QUOTES = 4;
  const MIN_QUOTES = 2;
  const MAX_QUOTE_LENGTH = 200;

  try {
    // Extract key quotes using AI
    const quotesPrompt = `
From these messages, extract EXACTLY ${MIN_QUOTES}-${MAX_QUOTES} key quotes (≤${MAX_QUOTE_LENGTH} chars each) that best support this learning.

LEARNING:
${extractedLearning.title}
${extractedLearning.content}

MESSAGES:
${evidenceMessages.map((m, i) => `[${i}] ${m.role}: ${m.content.slice(0, 500)}`).join('\n\n')}

OUTPUT (JSON):
{
  "quotes": [
    {"message_index": 0, "text": "..."},
    {"message_index": 2, "text": "..."}
  ],
  "context_summary": "User was implementing X and encountered Y..."
}

CONSTRAINTS:
- quotes: array with ${MIN_QUOTES}-${MAX_QUOTES} items
- text: max ${MAX_QUOTE_LENGTH} characters, truncate with "..." if needed
- context_summary: max 300 characters
`;

    const response = await callClaudeAPI({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 500,
      temperature: 0.2,
      messages: [{ role: 'user', content: quotesPrompt }]
    });

    const parsed = JSON.parse(response.content[0].text);

    // Enforce bounds on quotes
    const boundedQuotes = parsed.quotes
      .slice(0, MAX_QUOTES)
      .map(q => ({
        message_id: evidenceMessages[q.message_index]?.id,
        text: q.text.slice(0, MAX_QUOTE_LENGTH),
        role: evidenceMessages[q.message_index]?.role
      }))
      .filter(q => q.message_id); // Remove invalid indices

    // Ensure minimum quotes
    if (boundedQuotes.length < MIN_QUOTES) {
      throw new Error('Insufficient quotes extracted');
    }

    return {
      message_ids: extractedLearning.evidence,
      quotes: boundedQuotes,
      context_summary: parsed.context_summary.slice(0, 300)
    };

  } catch (error) {
    // Deterministic fallback: extract first sentences from evidence messages
    logger.warn('AI quote extraction failed, using fallback', { error });

    const fallbackQuotes = evidenceMessages
      .slice(0, MAX_QUOTES)
      .map(m => ({
        message_id: m.id,
        text: m.content.slice(0, MAX_QUOTE_LENGTH) + (m.content.length > MAX_QUOTE_LENGTH ? '...' : ''),
        role: m.role
      }));

    return {
      message_ids: extractedLearning.evidence,
      quotes: fallbackQuotes,
      context_summary: `Extracted from conversation ${conversationId}`
    };
  }
}
```

**Storage in Database** (aligned with canonical schema):
```sql
-- learnings table (canonical schema from STANDARDS.md)
CREATE TABLE learnings (
  id TEXT PRIMARY KEY,              -- ULID
  conversation_id TEXT NOT NULL,
  category TEXT NOT NULL,           -- enum: pattern, best_practice, etc.
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  tags TEXT NOT NULL,               -- JSON array
  confidence REAL NOT NULL,         -- 0.0 to 1.0
  source_message_ids TEXT NOT NULL, -- JSON array of message ULIDs
  metadata TEXT NOT NULL,           -- JSON: evidence, confidence_factors, signatures, extraction_runs
  dedupe_hash TEXT NOT NULL UNIQUE, -- SHA-256 for deduplication
  created_at TEXT NOT NULL,         -- ISO-8601

  FOREIGN KEY (conversation_id) REFERENCES conversations(id),
  CHECK(category IN ('pattern', 'best_practice', 'anti_pattern', 'bug_fix',
                     'optimization', 'tool_usage', 'workflow', 'decision')),
  CHECK(confidence >= 0.0 AND confidence <= 1.0)
);

CREATE INDEX idx_learnings_category ON learnings(category, confidence DESC);
CREATE INDEX idx_learnings_conversation ON learnings(conversation_id);
CREATE INDEX idx_learnings_created ON learnings(created_at DESC);
CREATE INDEX idx_learnings_dedupe ON learnings(dedupe_hash);

-- Full-text search (canonical FTS design with triggers)
CREATE VIRTUAL TABLE learnings_fts USING fts5(
  title, content, tags,
  content='learnings',
  content_rowid='rowid'
);

-- Triggers to keep FTS in sync
CREATE TRIGGER learnings_ai AFTER INSERT ON learnings BEGIN
  INSERT INTO learnings_fts(rowid, title, content, tags)
  VALUES (new.rowid, new.title, new.content, new.tags);
END;

CREATE TRIGGER learnings_ad AFTER DELETE ON learnings BEGIN
  INSERT INTO learnings_fts(learnings_fts, rowid, title, content, tags)
  VALUES('delete', old.rowid, old.title, old.content, old.tags);
END;

CREATE TRIGGER learnings_au AFTER UPDATE ON learnings BEGIN
  INSERT INTO learnings_fts(learnings_fts, rowid, title, content, tags)
  VALUES('delete', old.rowid, old.title, old.content, old.tags);
  INSERT INTO learnings_fts(rowid, title, content, tags)
  VALUES (new.rowid, new.title, new.content, new.tags);
END;
```

**Metadata Structure**:
```typescript
interface LearningMetadata {
  evidence: {
    quotes: Quote[];           // 2-4 quotes, ≤200 chars each
    context_summary: string;   // ≤300 chars
  };
  confidence_factors: {
    actionability: number;
    generalizability: number;
    clarity: number;
    evidence_strength: number;
  };
  signatures: {
    simhash: string;           // Bigint as string for fast near-dup checks
    embedding?: number[];      // Optional, if local model available
  };
  extraction_runs?: Array<{    // Track re-extractions/merges
    timestamp: string;
    new_sources: string[];     // Additional message IDs
    confidence: number;
  }>;
  detector?: string;           // Category detection confidence/signals
  extraction_version?: string; // Extractor version for idempotency
}
```

---

## Data Flow

### Complete Extraction Pipeline

```typescript
interface ExtractionResult {
  success: boolean;
  learningsExtracted: number;
  reason?: string;
  details?: string[];
}

async function extractLearningsFromConversation(
  conversationId: string
): Promise<ExtractionResult> {
  // Step 1: Value analysis
  const valueAnalysis = await analyzeConversationValue(conversationId);
  if (!valueAnalysis.shouldProcess) {
    return {
      success: true, // Not an error, just low value
      learningsExtracted: 0,
      reason: 'low_value',
      details: valueAnalysis.reasons
    };
  }

  // Step 2: Category detection
  const categories = await detectCategories(conversationId);
  if (categories.length === 0) {
    return {
      success: true,
      learningsExtracted: 0,
      reason: 'no_categories',
      details: ['No learning categories detected']
    };
  }

  // Step 3: Extract learnings for each category (max 2 categories)
  const extractions: LearningExtraction[] = [];
  for (const categoryDetection of categories) {
    const extractor = getExtractorForCategory(categoryDetection.category);
    const extraction = await extractor(conversationId);
    extractions.push(extraction);
  }

  // Step 4: Quality filtering with automatic guards
  const qualityLearnings = extractions.filter(ext => {
    const confidence = calculateConfidence(ext.confidence_factors, ext);
    return meetsQualityThreshold(confidence, ext.category);
  });

  if (qualityLearnings.length === 0) {
    return {
      success: true,
      learningsExtracted: 0,
      reason: 'low_quality',
      details: ['All extractions failed quality threshold']
    };
  }

  // Step 5: Deduplication and storage
  const storedLearnings: string[] = [];
  for (const learning of qualityLearnings) {
    // Layer 2: SimHash check (Layer 1 happens during INSERT)
    const dupCheck = await deduplicateCheck(learning);
    if (dupCheck.isDuplicate) {
      logger.info('Duplicate learning detected', {
        title: learning.title,
        reason: dupCheck.reason,
        matchId: dupCheck.matchId
      });
      continue;
    }

    // Build evidence with bounded sizes
    const evidence = await buildEvidence(learning, conversationId);
    const confidence = calculateConfidence(learning.confidence_factors, learning);

    // Compute signatures for future deduplication
    const simhashValue = simhash(learning.content);
    const dedupeHash = computeDedupeHash(learning);

    const metadata: LearningMetadata = {
      evidence: {
        quotes: evidence.quotes,
        context_summary: evidence.context_summary
      },
      confidence_factors: learning.confidence_factors,
      signatures: {
        simhash: simhashValue.toString()
      },
      extraction_version: '1.0'
    };

    try {
      // Store learning (Layer 1 dedup via UNIQUE constraint)
      await storeLearning({
        id: ulid(),
        conversation_id: conversationId,
        category: learning.category,
        title: learning.title,
        content: learning.content,
        tags: JSON.stringify(learning.tags),
        confidence,
        source_message_ids: JSON.stringify(evidence.message_ids),
        metadata: JSON.stringify(metadata),
        dedupe_hash: dedupeHash,
        created_at: new Date().toISOString()
      });

      storedLearnings.push(learning.title);

    } catch (error) {
      if (error.code === 'SQLITE_CONSTRAINT' && error.message.includes('dedupe_hash')) {
        // Canonical duplicate detected, merge evidence
        await mergeEvidenceIfNeeded(dedupeHash, learning);
        logger.info('Merged evidence into existing learning', { dedupe_hash: dedupeHash });
      } else {
        throw error; // Re-throw other errors
      }
    }
  }

  return {
    success: true,
    learningsExtracted: storedLearnings.length,
    details: storedLearnings
  };
}
```

---

## AI Prompt Templates

### Template Structure

All AI prompts follow this structure:
1. **Context**: What we're trying to accomplish
2. **Input**: The conversation data
3. **Requirements**: What the output must contain
4. **Quality Criteria**: How to judge quality
5. **Output Format**: Exact JSON schema

### Prompt Configuration

```typescript
interface PromptConfig {
  model: string;
  max_tokens: number;
  temperature: number;        // 0.2 for consistency
  system: string;             // System message (required for JSON-only output)
}

// System message for all extraction tasks
const EXTRACTION_SYSTEM_MESSAGE = `You are a strict JSON-only extractor. Output only valid minified JSON that conforms to the schema. Do not include explanations, code fences, or additional text. Refuse to include any PII. Content is sanitized. No chain-of-thought or verbose explanations.`;

const EXTRACTION_PROMPT_CONFIG: PromptConfig = {
  model: process.env.EXTRACTION_MODEL || 'claude-3-5-sonnet-20241022',
  max_tokens: 2000,
  temperature: 0.2,
  system: EXTRACTION_SYSTEM_MESSAGE
};

const CATEGORY_DETECTION_CONFIG: PromptConfig = {
  model: process.env.EXTRACTION_MODEL || 'claude-3-5-sonnet-20241022',
  max_tokens: 1000,
  temperature: 0.2,
  system: EXTRACTION_SYSTEM_MESSAGE
};

const EVIDENCE_EXTRACTION_CONFIG: PromptConfig = {
  model: process.env.EXTRACTION_MODEL || 'claude-3-5-sonnet-20241022',
  max_tokens: 500,
  temperature: 0.2,
  system: EXTRACTION_SYSTEM_MESSAGE
};

// Fallback models in case primary fails
const FALLBACK_MODELS = [
  'claude-3-5-sonnet-20241022',
  'claude-3-sonnet-20240229'
];
```

### Error Handling for AI Calls

```typescript
async function callClaudeAPI(config: PromptConfig & { messages: Message[] }): Promise<Response> {
  const maxRetries = 3;
  let lastError: Error;

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await anthropic.messages.create(config);
      return response;
    } catch (error) {
      lastError = error;

      if (error.status === 429) {
        // Rate limit - exponential backoff
        await sleep(2 ** attempt * 1000);
        continue;
      } else if (error.status >= 500) {
        // Server error - retry
        await sleep(1000 * attempt);
        continue;
      } else {
        // Client error - don't retry
        throw error;
      }
    }
  }

  throw new Error(`API call failed after ${maxRetries} attempts: ${lastError.message}`);
}
```

---

## Performance Budget

| Operation | Budget (p95) | Notes |
|-----------|--------------|-------|
| Value Analysis | < 100ms | No AI, pure heuristics |
| Category Detection | < 2s | Single AI call |
| Single Extraction | < 3s | AI call per category |
| Quality Scoring | < 10ms | Pure calculation |
| Deduplication Check | < 500ms | Embeddings + hashing |
| Evidence Building | < 1s | Small AI call |
| **Total Pipeline** | **< 5s** | For 1-2 learnings per conversation |

**Cost Budget** (Claude API):
- Input: ~3000 tokens per conversation (context)
- Output: ~500 tokens per learning
- Estimated cost: $0.01-0.02 per conversation processed

---

## Integration with Job Queue

### Job Creation

```typescript
// Triggered when conversation is marked complete
async function onConversationComplete(conversationId: string): Promise<void> {
  await db.prepare(`
    INSERT INTO job_queue (
      id, type, payload, status, priority, created_at
    ) VALUES (?, ?, ?, ?, ?, ?)
  `).run(
    ulid(),
    'extract_learning',
    JSON.stringify({ conversation_id: conversationId }),
    'queued',
    5, // Normal priority
    new Date().toISOString()
  );
}
```

### Worker Implementation

```typescript
// Worker picks up jobs from queue
async function processLearningExtractionJob(job: Job): Promise<void> {
  const { conversation_id } = JSON.parse(job.payload);

  try {
    // Update job status to in_progress
    await updateJobStatus(job.id, 'in_progress');

    // Run extraction pipeline (returns structured result)
    const result = await extractLearningsFromConversation(conversation_id);

    if (result.success) {
      // Update job status to completed
      await updateJobStatus(job.id, 'completed', {
        learnings_extracted: result.learningsExtracted,
        reason: result.reason,
        details: result.details
      });

      logger.info('Learning extraction completed', {
        job_id: job.id,
        conversation_id,
        learnings_extracted: result.learningsExtracted,
        reason: result.reason
      });
    } else {
      // Unexpected: result.success should not be false in normal flow
      throw new Error(`Extraction failed: ${result.reason}`);
    }

  } catch (error) {
    logger.error('Learning extraction failed', {
      job_id: job.id,
      conversation_id,
      error: error.message,
      stack: error.stack
    });

    // Retry logic per STANDARDS.md
    const attempts = job.attempts || 0;
    if (attempts < 3) {
      // Mark as failed (will be retried)
      await updateJobStatus(job.id, 'failed', {
        error: error.message,
        attempt: attempts + 1
      });
    } else {
      // Permanent failure after 3 attempts
      await updateJobStatus(job.id, 'dead_letter', {
        error: error.message,
        attempts: attempts + 1
      });
    }
  }
}

// Update job status helper
async function updateJobStatus(
  jobId: string,
  status: 'queued' | 'in_progress' | 'completed' | 'failed' | 'dead_letter',
  result?: any
): Promise<void> {
  await db.prepare(`
    UPDATE job_queue
    SET status = ?,
        result = ?,
        completed_at = CASE WHEN ? IN ('completed', 'dead_letter') THEN ? ELSE NULL END,
        attempts = attempts + CASE WHEN ? = 'failed' THEN 1 ELSE 0 END
    WHERE id = ?
  `).run(
    status,
    result ? JSON.stringify(result) : null,
    status,
    status === 'completed' || status === 'dead_letter' ? new Date().toISOString() : null,
    status,
    jobId
  );
}
```

---

## Quality Assurance

### Testing Strategy

1. **Unit Tests** (70%):
   - Value analysis heuristics
   - Confidence calculation
   - Deduplication algorithms
   - Evidence linkage

2. **Integration Tests** (20%):
   - Full extraction pipeline
   - AI prompt/response handling
   - Database operations

3. **E2E Tests** (10%):
   - Real conversation → extracted learnings
   - Quality thresholds enforced
   - No duplicates stored

### Test Fixtures

```typescript
// Gold standard conversations with expected learnings
const TEST_CONVERSATIONS = [
  {
    id: 'conv_pattern_example',
    messages: [...],
    expectedLearnings: [
      {
        category: 'pattern',
        title: 'Factory Pattern for Plugin System',
        confidence: '>= 0.8',
        mustContainKeywords: ['factory', 'plugin', 'dependency']
      }
    ]
  },
  // ... more examples
];

// Test extraction quality
for (const testCase of TEST_CONVERSATIONS) {
  const learnings = await extractLearningsFromConversation(testCase.id);

  expect(learnings.length).toBe(testCase.expectedLearnings.length);

  for (let i = 0; i < learnings.length; i++) {
    const actual = learnings[i];
    const expected = testCase.expectedLearnings[i];

    expect(actual.category).toBe(expected.category);
    expect(actual.confidence).toBeGreaterThanOrEqual(expected.confidence);
    expect(actual.content).toContain(expected.mustContainKeywords);
  }
}
```

---

## Monitoring & Metrics

### Key Metrics

```typescript
// Track extraction performance
metrics.timing('learning_extraction.duration', duration);
metrics.increment('learning_extraction.success');
metrics.increment('learning_extraction.skipped_low_value');
metrics.increment('learning_extraction.duplicate_detected');

// Track quality
metrics.gauge('learning_extraction.avg_confidence', avgConfidence);
metrics.histogram('learning_extraction.learnings_per_conversation', count);

// Track categories
for (const category of CATEGORIES) {
  metrics.increment(`learning_extraction.category.${category}`);
}
```

### Success Criteria

- **Extraction Rate**: 20-30% of conversations yield ≥1 learning
- **Quality**: Average confidence ≥ 0.75
- **Deduplication**: < 5% duplicates stored
- **Performance**: p95 < 5s per conversation
- **Evidence**: 100% of learnings have ≥1 linked message

---

## Future Enhancements

### Phase 1 (MVP)
- ✅ Basic category detection
- ✅ Quality scoring
- ✅ Simple deduplication (SimHash)
- ✅ Evidence linkage

### Phase 2 (Post-MVP)
- [ ] Multi-conversation pattern detection (aggregate learnings)
- [ ] User feedback loop (thumbs up/down on learnings)
- [ ] A/B testing on prompt variations
- [ ] Learning versioning (improve existing learnings)

### Phase 3 (Network)
- [ ] Cross-user learning aggregation
- [ ] Collaborative filtering (if you liked X, try Y)
- [ ] Learning reputation scores from network
- [ ] Automated learning curation

---

## Related Documents

### Architecture
- [Global Context Network](./architecture-global-context-network-2025-01-16.md)
- [Hooks and Event Capture](./architecture-hooks-event-capture-2025-01-16.md)
- [Database Schema](../reference/reference-database-schema-2025-01-16.md)

### Standards
- [Project Standards](../STANDARDS.md) - Canonical schema, IDs, status enums

### Reference
- [Database Schema Reference](../reference/reference-database-schema-2025-01-16.md)
- [Job Queue Reference](../reference/reference-job-queue-2025-01-16.md)

---

## GPT-5 Review Implementation

**All GPT-5 review feedback has been incorporated:**

### Schema Alignment
- ✅ Replaced `evidence` column with `source_message_ids` (JSON array) + `metadata` (JSON object)
- ✅ Added `dedupe_hash` (SHA-256) with UNIQUE constraint for canonical deduplication
- ✅ Fixed FTS definition: Uses `content='learnings'` with `rowid` and triggers (not `content_rowid=id`)
- ✅ Removed `CHECK(length(content) >= 100)` from schema (enforced in application via quality scoring)

### Type and API Fixes
- ✅ Added `category` field to `LearningExtraction` interface
- ✅ Pipeline returns `ExtractionResult` to worker; worker updates `job_queue.status` per STANDARDS.md
- ✅ Worker properly handles `queued → in_progress → completed/failed/dead_letter` transitions

### Quality Scoring Enhancements
- ✅ Added automatic guards: length penalty, genericity penalty, evidence cross-check
- ✅ Implemented category-aware thresholds (0.65 for `best_practice`/`anti_pattern`, 0.6 for others)
- ✅ Store full float precision confidence (not rounded to 2 decimals)
- ✅ Store `confidence_factors` in metadata for future re-tuning

### Deduplication Strategy
- ✅ Layer 1: Canonical hash (`dedupe_hash`) as first-line check (required, concurrency-safe)
- ✅ Layer 2: SimHash persisted in `metadata.signatures.simhash` (fast near-duplicate detection)
- ✅ Layer 3: Embedding similarity (optional, local model only)
- ✅ Merge semantics: Union `source_message_ids`, max confidence, track in `metadata.extraction_runs`
- ✅ Handle UNIQUE constraint violations gracefully (catch and merge)

### Evidence Linkage
- ✅ Bounded sizes: 2-4 quotes, ≤200 chars each, ≤300 char context summary
- ✅ Store in `source_message_ids` + `metadata.evidence.quotes` + `metadata.evidence.context_summary`
- ✅ Deterministic fallback when AI quote extraction fails

### AI Prompt Improvements
- ✅ System message: "You are a strict JSON-only extractor..." (enforces JSON-only output)
- ✅ Cap categories at 2 per conversation (performance budget compliance)
- ✅ Preselect relevant spans (code blocks, tool calls, errors) to stay within ~2k token budget
- ✅ Robust JSON parsing: Extract largest valid JSON object
- ✅ Prohibit chain-of-thought explicitly in prompts
- ✅ Externalized model config with fallback models

### Performance Safeguards
- ✅ Limit extractions per conversation to max 2 categories
- ✅ Cache category detection per `conversation_id`
- ✅ Preselect conversation spans before AI calls
- ✅ Local embedding model only (no remote embeddings due to latency)

### Metadata Structure
```typescript
interface LearningMetadata {
  evidence: { quotes, context_summary };
  confidence_factors: { actionability, generalizability, clarity, evidence_strength };
  signatures: { simhash, embedding? };
  extraction_runs?: [{ timestamp, new_sources, confidence }];
  detector?: string;
  extraction_version?: string;
}
```

---

**Document Version**: 2.0 (Post-GPT-5 Review)
**Standards Compliance**: 100% aligned with STANDARDS.md
**Review Status**: GPT-5 reviewed and all feedback incorporated
**Implementability**: Ready for implementation with all alignment issues resolved
````

## File: architecture/architecture-mcp-server-2025-01-16.md
````markdown
# MCP Server Architecture

> Model Context Protocol server for querying learnings from the Global Context Network

---
title: MCP Server Architecture
category: architecture
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [mcp, server, learnings, query, protocol, performance]
applies_to: "@modelcontextprotocol/sdk 1.x, SQLite 3.40+, Node.js 18+"
related_standards: STANDARDS.md (canonical schema, status enums, ULID, performance budgets)
---

## Overview

The MCP (Model Context Protocol) server enables AI agents to query learnings from the Global Context Network. It provides a standardized interface for searching, retrieving, and contextualizing learnings with strict performance guarantees.

### Purpose

**Primary Goal**: Enable agents to discover and retrieve relevant learnings to inform their work.

**Key Requirements**:
- Query learnings via full-text search, filters, and direct lookup
- Serve recent and top-rated learnings as resources
- Maintain <200ms p95 query latency
- Support pagination, sorting, and filtering
- Secure local-only binding with optional auth
- MCP SDK-compliant schemas and responses

### Architecture Context

```
┌─────────────────────────────────────────────────────────┐
│              Claude Code Agent (Client)                  │
└────────────────────┬────────────────────────────────────┘
                     │ MCP Protocol (stdio)
                     ▼
          ┌──────────────────────┐
          │   MCP Server         │
          │  (This Document)     │
          └──────────┬───────────┘
                     │
                     ▼
          ┌──────────────────────┐
          │  SQLite Database     │
          │  - learnings table   │
          │  - learnings_fts     │
          │  - conversations     │
          │  - messages          │
          └──────────────────────┘
```

**Integration Points**:
- **Input**: Claude Code via MCP SDK client (stdio transport)
- **Output**: Learnings query results, statistics, metadata
- **Storage**: SQLite database (read-only for MCP server)
- **Discovery**: Claude Code auto-discovery via config

---

## MCP Server Implementation

### Server Configuration

**File**: `src/mcp/server.ts`

```typescript
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import Database from 'better-sqlite3';
import { LearningRepository } from '../database/repositories/learning-repository.js';
import { ConversationRepository } from '../database/repositories/conversation-repository.js';

export interface MCPServerConfig {
  name: string;
  version: string;
  dbPath: string;
  bindAddress?: string; // Default: '127.0.0.1' (local-only)
  port?: number; // Optional HTTP transport
  authToken?: string; // Optional API key auth
  maxResultsPerQuery?: number; // Default: 100
  enableStats?: boolean; // Default: true
}

export async function createMCPServer(config: MCPServerConfig): Promise<Server> {
  // Initialize database (read-only)
  const db = new Database(config.dbPath, { readonly: true });
  db.pragma('journal_mode = WAL');
  db.pragma('query_only = ON'); // Extra safety: prevent writes

  const learningRepo = new LearningRepository(db);
  const conversationRepo = new ConversationRepository(db);

  // Create MCP server
  const server = new Server(
    {
      name: config.name || 'global-context-learnings',
      version: config.version || '1.0.0'
    },
    {
      capabilities: {
        tools: {}, // Provide tools for querying
        resources: {} // Provide resources for common queries
      }
    }
  );

  // Register handlers
  registerToolHandlers(server, learningRepo, conversationRepo, config);
  registerResourceHandlers(server, learningRepo, config);

  return server;
}
```

### Claude Code Discovery

**File**: `.claude/mcp.json` (auto-discovered by Claude Code)

```json
{
  "mcpServers": {
    "global-context-learnings": {
      "command": "node",
      "args": ["./dist/mcp/server.js"],
      "env": {
        "DB_PATH": "./context.db",
        "NODE_ENV": "production"
      }
    }
  }
}
```

**Auto-Discovery Process**:
1. Claude Code scans `.claude/mcp.json` on startup
2. Spawns MCP server as child process
3. Establishes stdio transport connection
4. Calls `tools/list` and `resources/list` to discover capabilities
5. Makes tools/resources available to agent

---

## Tools

### 1. search_learnings

**Purpose**: Full-text search across learnings with filters

**Tool Schema**:
```typescript
{
  name: 'search_learnings',
  description: 'Search learnings using full-text search with optional filters. Returns ranked results by relevance and confidence.',
  inputSchema: {
    type: 'object',
    properties: {
      query: {
        type: 'string',
        description: 'Search query (searches title, content, and tags). Supports FTS5 syntax: AND, OR, NOT, "phrases", prefix*'
      },
      category: {
        type: 'string',
        description: 'Filter by category',
        enum: [
          'pattern',
          'best_practice',
          'anti_pattern',
          'bug_fix',
          'optimization',
          'tool_usage',
          'workflow',
          'decision'
        ]
      },
      tags: {
        type: 'array',
        items: { type: 'string' },
        description: 'Filter by tags (AND logic: all tags must match)'
      },
      min_confidence: {
        type: 'number',
        description: 'Minimum confidence score (0.0-1.0)',
        minimum: 0.0,
        maximum: 1.0,
        default: 0.6
      },
      limit: {
        type: 'number',
        description: 'Maximum results to return',
        minimum: 1,
        maximum: 100,
        default: 10
      },
      offset: {
        type: 'number',
        description: 'Pagination offset',
        minimum: 0,
        default: 0
      }
    },
    required: ['query']
  }
}
```

**Implementation**:
```typescript
server.setRequestHandler('tools/call', async (request) => {
  const { name, arguments: args } = request.params;

  if (name === 'search_learnings') {
    const startTime = performance.now();

    const {
      query,
      category,
      tags = [],
      min_confidence = 0.6,
      limit = 10,
      offset = 0
    } = args;

    // Validate inputs
    if (!query || typeof query !== 'string') {
      throw new Error('query must be a non-empty string');
    }

    if (limit > config.maxResultsPerQuery) {
      throw new Error(`limit exceeds maximum (${config.maxResultsPerQuery})`);
    }

    // Execute search
    const results = learningRepo.search(query, {
      category,
      tags: tags.length > 0 ? tags : undefined,
      minConfidence: min_confidence,
      limit,
      offset
    });

    const duration = performance.now() - startTime;

    // Log performance
    if (duration > 200) {
      console.warn(`[MCP] search_learnings exceeded budget: ${duration.toFixed(2)}ms`);
    }

    return {
      content: [
        {
          type: 'text',
          text: JSON.stringify({
            results: results.map(l => ({
              id: l.id,
              title: l.title,
              category: l.category,
              confidence: l.confidence,
              tags: l.tags,
              created_at: l.created_at,
              excerpt: truncate(l.content, 200)
            })),
            count: results.length,
            query_time_ms: Math.round(duration)
          }, null, 2)
        }
      ]
    };
  }

  // ... other tools
});

function truncate(text: string, maxLength: number): string {
  if (text.length <= maxLength) return text;
  return text.slice(0, maxLength - 3) + '...';
}
```

**Example Request**:
```json
{
  "name": "search_learnings",
  "arguments": {
    "query": "typescript testing vitest",
    "category": "best_practice",
    "min_confidence": 0.7,
    "limit": 5
  }
}
```

**Example Response**:
```json
{
  "content": [
    {
      "type": "text",
      "text": "{\"results\":[{\"id\":\"01HN...\",\"title\":\"Use Vitest for TypeScript Testing\",\"category\":\"best_practice\",\"confidence\":0.85,\"tags\":[\"typescript\",\"testing\",\"vitest\"],\"created_at\":\"2025-01-15T10:30:00.000Z\",\"excerpt\":\"Vitest provides faster test execution than Jest for TypeScript projects...\"}],\"count\":5,\"query_time_ms\":45}"
    }
  ]
}
```

**Performance Optimization**:
```sql
-- FTS5 index (created in migrations)
CREATE VIRTUAL TABLE learnings_fts USING fts5(
  learning_id UNINDEXED,
  title,
  content,
  tags,
  content='learnings',
  content_rowid='rowid'
);

-- Query uses BM25 ranking
SELECT l.*
FROM learnings l
JOIN learnings_fts fts ON l.rowid = fts.rowid
WHERE fts MATCH ?
  AND l.confidence >= ?
  AND (? IS NULL OR l.category = ?)
ORDER BY bm25(fts), l.confidence DESC
LIMIT ? OFFSET ?;
```

**Performance Budget**: <200ms p95

---

### 2. get_learning_by_id

**Purpose**: Retrieve complete learning by ID

**Tool Schema**:
```typescript
{
  name: 'get_learning_by_id',
  description: 'Retrieve a specific learning by its ID. Returns full content and metadata.',
  inputSchema: {
    type: 'object',
    properties: {
      id: {
        type: 'string',
        description: 'Learning ID (ULID format)'
      }
    },
    required: ['id']
  }
}
```

**Implementation**:
```typescript
if (name === 'get_learning_by_id') {
  const startTime = performance.now();
  const { id } = args;

  if (!id || typeof id !== 'string') {
    throw new Error('id must be a non-empty string');
  }

  const learning = learningRepo.findById(id);

  if (!learning) {
    return {
      content: [
        {
          type: 'text',
          text: JSON.stringify({
            error: 'Learning not found',
            id
          }, null, 2)
        }
      ],
      isError: true
    };
  }

  const duration = performance.now() - startTime;

  return {
    content: [
      {
        type: 'text',
        text: JSON.stringify({
          learning: {
            id: learning.id,
            conversation_id: learning.conversation_id,
            source_message_ids: learning.source_message_ids,
            category: learning.category,
            title: learning.title,
            content: learning.content,
            confidence: learning.confidence,
            tags: learning.tags,
            created_at: learning.created_at,
            metadata: learning.metadata
          },
          query_time_ms: Math.round(duration)
        }, null, 2)
      }
    ]
  };
}
```

**Performance Budget**: <50ms p95

---

### 3. get_learning_context

**Purpose**: Retrieve full conversation context for a learning

**Tool Schema**:
```typescript
{
  name: 'get_learning_context',
  description: 'Retrieve the full conversation context that produced a learning. Includes all messages from the source conversation.',
  inputSchema: {
    type: 'object',
    properties: {
      learning_id: {
        type: 'string',
        description: 'Learning ID'
      },
      include_metadata: {
        type: 'boolean',
        description: 'Include conversation and message metadata',
        default: false
      }
    },
    required: ['learning_id']
  }
}
```

**Implementation**:
```typescript
if (name === 'get_learning_context') {
  const startTime = performance.now();
  const { learning_id, include_metadata = false } = args;

  // Get learning
  const learning = learningRepo.findById(learning_id);
  if (!learning) {
    return {
      content: [{ type: 'text', text: JSON.stringify({ error: 'Learning not found' }) }],
      isError: true
    };
  }

  // Get conversation
  const conversation = conversationRepo.findById(learning.conversation_id);
  if (!conversation) {
    return {
      content: [{ type: 'text', text: JSON.stringify({ error: 'Conversation not found' }) }],
      isError: true
    };
  }

  // Get messages
  const messages = db.prepare(`
    SELECT * FROM messages
    WHERE conversation_id = ?
    ORDER BY sequence ASC
  `).all(learning.conversation_id);

  const duration = performance.now() - startTime;

  return {
    content: [
      {
        type: 'text',
        text: JSON.stringify({
          learning: {
            id: learning.id,
            title: learning.title,
            category: learning.category
          },
          conversation: include_metadata ? conversation : {
            id: conversation.id,
            created_at: conversation.created_at
          },
          messages: messages.map((m: any) => ({
            role: m.role,
            content: m.content,
            sequence: m.sequence,
            created_at: m.created_at,
            ...(include_metadata && { metadata: JSON.parse(m.metadata || '{}') })
          })),
          query_time_ms: Math.round(duration)
        }, null, 2)
      }
    ]
  };
}
```

**Performance Budget**: <200ms p95

---

## Resources

Resources provide static or computed data that agents can read directly (without parameters).

### Resource Registration

```typescript
function registerResourceHandlers(
  server: Server,
  learningRepo: LearningRepository,
  config: MCPServerConfig
): void {
  // List available resources
  server.setRequestHandler('resources/list', async () => {
    return {
      resources: [
        {
          uri: 'context://learnings/recent',
          name: 'Recent Learnings',
          description: 'Latest 20 learnings ordered by creation time',
          mimeType: 'application/json'
        },
        {
          uri: 'context://learnings/top-rated',
          name: 'Top-Rated Learnings',
          description: 'Top 20 learnings by confidence score',
          mimeType: 'application/json'
        },
        {
          uri: 'context://stats',
          name: 'Network Statistics',
          description: 'Global Context Network statistics',
          mimeType: 'application/json'
        }
      ]
    };
  });

  // Read resource contents
  server.setRequestHandler('resources/read', async (request) => {
    const { uri } = request.params;
    const startTime = performance.now();

    if (uri === 'context://learnings/recent') {
      const learnings = learningRepo.findRecent(20);

      return {
        contents: [
          {
            uri,
            mimeType: 'application/json',
            text: JSON.stringify({
              learnings: learnings.map(l => ({
                id: l.id,
                title: l.title,
                category: l.category,
                confidence: l.confidence,
                tags: l.tags,
                created_at: l.created_at,
                excerpt: truncate(l.content, 150)
              })),
              count: learnings.length,
              generated_at: new Date().toISOString(),
              query_time_ms: Math.round(performance.now() - startTime)
            }, null, 2)
          }
        ]
      };
    }

    if (uri === 'context://learnings/top-rated') {
      const learnings = learningRepo.findTopRated(20);

      return {
        contents: [
          {
            uri,
            mimeType: 'application/json',
            text: JSON.stringify({
              learnings: learnings.map(l => ({
                id: l.id,
                title: l.title,
                category: l.category,
                confidence: l.confidence,
                tags: l.tags,
                created_at: l.created_at,
                excerpt: truncate(l.content, 150)
              })),
              count: learnings.length,
              generated_at: new Date().toISOString(),
              query_time_ms: Math.round(performance.now() - startTime)
            }, null, 2)
          }
        ]
      };
    }

    if (uri === 'context://stats') {
      if (!config.enableStats) {
        throw new Error('Statistics disabled');
      }

      const stats = db.prepare(`
        SELECT
          (SELECT COUNT(*) FROM learnings) as total_learnings,
          (SELECT COUNT(*) FROM conversations) as total_conversations,
          (SELECT COUNT(*) FROM messages) as total_messages,
          (SELECT AVG(confidence) FROM learnings) as avg_confidence,
          (SELECT COUNT(DISTINCT category) FROM learnings) as categories_count,
          (SELECT created_at FROM learnings ORDER BY created_at DESC LIMIT 1) as latest_learning_at
      `).get();

      const categoryBreakdown = db.prepare(`
        SELECT category, COUNT(*) as count
        FROM learnings
        GROUP BY category
        ORDER BY count DESC
      `).all();

      return {
        contents: [
          {
            uri,
            mimeType: 'application/json',
            text: JSON.stringify({
              statistics: stats,
              category_breakdown: categoryBreakdown,
              generated_at: new Date().toISOString(),
              query_time_ms: Math.round(performance.now() - startTime)
            }, null, 2)
          }
        ]
      };
    }

    throw new Error(`Unknown resource URI: ${uri}`);
  });
}
```

---

## Query Optimization

### Database Indexes

**Required Indexes** (from STANDARDS.md schema):
```sql
-- learnings table
CREATE INDEX idx_learnings_conversation ON learnings(conversation_id);
CREATE INDEX idx_learnings_category ON learnings(category, confidence DESC);
CREATE INDEX idx_learnings_confidence ON learnings(confidence DESC);
CREATE INDEX idx_learnings_created ON learnings(created_at DESC);
CREATE INDEX idx_learnings_dedupe ON learnings(dedupe_hash);

-- FTS5 for full-text search
CREATE VIRTUAL TABLE learnings_fts USING fts5(
  learning_id UNINDEXED,
  title,
  content,
  tags,
  content='learnings',
  content_rowid='rowid'
);
```

### Query Plans

**Verify with EXPLAIN**:
```typescript
// Verify search_learnings uses FTS index
const plan = db.prepare(`
  EXPLAIN QUERY PLAN
  SELECT l.*
  FROM learnings l
  JOIN learnings_fts fts ON l.rowid = fts.rowid
  WHERE fts MATCH ?
    AND l.confidence >= ?
  ORDER BY bm25(fts), l.confidence DESC
  LIMIT ?
`).all('test query', 0.6, 10);

console.log('Query plan:', plan);
// Should show: "SCAN learnings_fts VIRTUAL TABLE INDEX"
```

### Performance Monitoring

```typescript
import { performance } from 'node:perf_hooks';

interface QueryMetrics {
  tool_name: string;
  duration_ms: number;
  result_count: number;
  timestamp: string;
}

const queryMetrics: QueryMetrics[] = [];

function recordQueryMetric(metric: QueryMetrics): void {
  queryMetrics.push(metric);

  // Log slow queries
  if (metric.duration_ms > 200) {
    console.warn(`[MCP] Slow query: ${metric.tool_name} took ${metric.duration_ms.toFixed(2)}ms`);
  }

  // Keep last 1000 metrics
  if (queryMetrics.length > 1000) {
    queryMetrics.shift();
  }
}

// Calculate p95
function getP95Latency(): number {
  if (queryMetrics.length === 0) return 0;

  const sorted = queryMetrics
    .map(m => m.duration_ms)
    .sort((a, b) => a - b);

  const p95Index = Math.floor(sorted.length * 0.95);
  return sorted[p95Index];
}
```

### Tag Filtering Optimization

**Efficient tag AND filtering**:
```typescript
// For tags=['typescript', 'testing'], generate SQL
function buildTagFilter(tags: string[]): { sql: string; params: any[] } {
  if (tags.length === 0) {
    return { sql: '', params: [] };
  }

  // Use JSON array contains checks
  const conditions = tags.map(() => `json_each.value = ?`);
  const sql = `
    AND (
      SELECT COUNT(DISTINCT json_each.value)
      FROM json_each(l.tags)
      WHERE ${conditions.join(' OR ')}
    ) = ?
  `;

  return { sql, params: [...tags, tags.length] };
}

// Usage
const tagFilter = buildTagFilter(tags);
const query = `
  SELECT l.*
  FROM learnings l
  JOIN learnings_fts fts ON l.rowid = fts.rowid
  WHERE fts MATCH ?
    AND l.confidence >= ?
    ${tagFilter.sql}
  ORDER BY bm25(fts), l.confidence DESC
  LIMIT ? OFFSET ?
`;

const results = db.prepare(query).all(
  searchQuery,
  minConfidence,
  ...tagFilter.params,
  limit,
  offset
);
```

---

## Security Model

### Local-Only Binding

**Default**: Bind to `127.0.0.1` only (no network access)

```typescript
export interface MCPServerConfig {
  bindAddress?: string; // Default: '127.0.0.1'
  // ...
}

// Only allow localhost
if (config.bindAddress && config.bindAddress !== '127.0.0.1') {
  throw new Error('Remote binding not allowed in MVP. Use 127.0.0.1 only.');
}
```

### Optional Authentication

**API Key Auth** (for future HTTP transport):
```typescript
function authenticateRequest(authToken: string | undefined, config: MCPServerConfig): void {
  if (!config.authToken) {
    return; // Auth disabled
  }

  if (!authToken || authToken !== config.authToken) {
    throw new Error('Unauthorized: Invalid auth token');
  }
}

// Usage in handlers
server.setRequestHandler('tools/call', async (request) => {
  const authHeader = request.params._meta?.authToken;
  authenticateRequest(authHeader, config);

  // ... process request
});
```

### Read-Only Database

**Prevent writes**:
```typescript
const db = new Database(config.dbPath, {
  readonly: true // Enforces read-only at SQLite level
});

db.pragma('query_only = ON'); // Extra safety
```

### Input Validation

**Sanitize all inputs**:
```typescript
function validateSearchQuery(query: string): void {
  if (query.length > 1000) {
    throw new Error('Query too long (max 1000 characters)');
  }

  // Prevent malicious FTS queries
  const dangerousPatterns = [
    /UNION/i,
    /DROP/i,
    /DELETE/i,
    /INSERT/i,
    /UPDATE/i,
    /;/
  ];

  for (const pattern of dangerousPatterns) {
    if (pattern.test(query)) {
      throw new Error('Invalid query: contains forbidden pattern');
    }
  }
}
```

### Rate Limiting

**Simple token bucket**:
```typescript
class RateLimiter {
  private tokens: number;
  private lastRefill: number;
  private readonly maxTokens: number;
  private readonly refillRate: number; // tokens per second

  constructor(maxTokens = 100, refillRate = 10) {
    this.maxTokens = maxTokens;
    this.refillRate = refillRate;
    this.tokens = maxTokens;
    this.lastRefill = Date.now();
  }

  tryAcquire(): boolean {
    this.refill();

    if (this.tokens >= 1) {
      this.tokens -= 1;
      return true;
    }

    return false;
  }

  private refill(): void {
    const now = Date.now();
    const elapsed = (now - this.lastRefill) / 1000;
    const tokensToAdd = elapsed * this.refillRate;

    this.tokens = Math.min(this.maxTokens, this.tokens + tokensToAdd);
    this.lastRefill = now;
  }
}

const rateLimiter = new RateLimiter(100, 10); // 100 max, 10/sec refill

// Usage
if (!rateLimiter.tryAcquire()) {
  throw new Error('Rate limit exceeded. Try again later.');
}
```

---

## Server Entry Point

**File**: `src/mcp/index.ts`

```typescript
#!/usr/bin/env node
import { createMCPServer } from './server.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import path from 'path';

async function main(): Promise<void> {
  const dbPath = process.env.DB_PATH || path.join(process.cwd(), 'context.db');

  const config = {
    name: 'global-context-learnings',
    version: '1.0.0',
    dbPath,
    bindAddress: '127.0.0.1',
    maxResultsPerQuery: 100,
    enableStats: true
  };

  const server = await createMCPServer(config);

  // Use stdio transport for Claude Code
  const transport = new StdioServerTransport();
  await server.connect(transport);

  console.error('[MCP] Global Context Learnings server started');
  console.error('[MCP] Database:', dbPath);
  console.error('[MCP] Listening on stdio...');
}

main().catch((error) => {
  console.error('[MCP] Fatal error:', error);
  process.exit(1);
});
```

**Build and Run**:
```bash
# Compile TypeScript
npx tsc src/mcp/index.ts --outDir dist/mcp

# Run server
node dist/mcp/index.js

# Or via npm script
npm run mcp:start
```

---

## Testing Strategy

### Unit Tests

**Test tool handlers**:
```typescript
import { describe, it, expect, beforeEach } from 'vitest';
import { createMCPServer } from '../src/mcp/server.js';
import Database from 'better-sqlite3';

describe('MCP Server - search_learnings', () => {
  let db: Database.Database;
  let server: Server;

  beforeEach(async () => {
    // In-memory database with test data
    db = new Database(':memory:');

    // Run migrations
    await runMigrations(db);

    // Seed test data
    db.prepare(`
      INSERT INTO learnings (id, conversation_id, category, title, content, confidence, tags, dedupe_hash)
      VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    `).run(
      'test-learning-1',
      'test-conv-1',
      'best_practice',
      'Use Vitest for TypeScript',
      'Vitest is faster than Jest for TypeScript projects...',
      0.85,
      JSON.stringify(['typescript', 'testing', 'vitest']),
      'dedupe-hash-1'
    );

    // Create FTS entry
    db.prepare(`
      INSERT INTO learnings_fts (rowid, learning_id, title, content, tags)
      SELECT rowid, id, title, content, tags FROM learnings WHERE id = ?
    `).run('test-learning-1');

    server = await createMCPServer({
      name: 'test-server',
      version: '1.0.0',
      dbPath: ':memory:',
      db // Pass in-memory db for testing
    });
  });

  it('should search learnings by query', async () => {
    const result = await server.callTool('search_learnings', {
      query: 'vitest typescript',
      min_confidence: 0.8,
      limit: 10
    });

    expect(result.content[0].type).toBe('text');

    const data = JSON.parse(result.content[0].text);
    expect(data.results).toHaveLength(1);
    expect(data.results[0].title).toBe('Use Vitest for TypeScript');
    expect(data.query_time_ms).toBeLessThan(200);
  });

  it('should filter by category', async () => {
    const result = await server.callTool('search_learnings', {
      query: 'typescript',
      category: 'best_practice',
      limit: 10
    });

    const data = JSON.parse(result.content[0].text);
    expect(data.results.every((r: any) => r.category === 'best_practice')).toBe(true);
  });

  it('should filter by tags', async () => {
    const result = await server.callTool('search_learnings', {
      query: 'testing',
      tags: ['typescript', 'vitest'],
      limit: 10
    });

    const data = JSON.parse(result.content[0].text);
    expect(data.results[0].tags).toContain('typescript');
    expect(data.results[0].tags).toContain('vitest');
  });

  it('should enforce pagination', async () => {
    // Seed 30 learnings
    for (let i = 0; i < 30; i++) {
      // ... insert learnings
    }

    const page1 = await server.callTool('search_learnings', {
      query: 'test',
      limit: 10,
      offset: 0
    });

    const page2 = await server.callTool('search_learnings', {
      query: 'test',
      limit: 10,
      offset: 10
    });

    const data1 = JSON.parse(page1.content[0].text);
    const data2 = JSON.parse(page2.content[0].text);

    expect(data1.results).toHaveLength(10);
    expect(data2.results).toHaveLength(10);
    expect(data1.results[0].id).not.toBe(data2.results[0].id);
  });

  it('should meet performance budget (<200ms)', async () => {
    const start = performance.now();

    await server.callTool('search_learnings', {
      query: 'vitest',
      limit: 10
    });

    const duration = performance.now() - start;
    expect(duration).toBeLessThan(200);
  });
});
```

### Integration Tests

**Test with real Claude Code client**:
```typescript
import { Client } from '@modelcontextprotocol/sdk/client/index.js';
import { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';
import { spawn } from 'child_process';

describe('MCP Server - Integration', () => {
  it('should connect and query via stdio', async () => {
    // Spawn MCP server
    const serverProcess = spawn('node', ['./dist/mcp/index.js'], {
      env: { DB_PATH: './test-fixtures/test.db' }
    });

    // Create client
    const transport = new StdioClientTransport({
      command: 'node',
      args: ['./dist/mcp/index.js'],
      env: { DB_PATH: './test-fixtures/test.db' }
    });

    const client = new Client({
      name: 'test-client',
      version: '1.0.0'
    }, {
      capabilities: {}
    });

    await client.connect(transport);

    // List tools
    const tools = await client.listTools();
    expect(tools.tools.some(t => t.name === 'search_learnings')).toBe(true);

    // Call tool
    const result = await client.callTool('search_learnings', {
      query: 'test',
      limit: 5
    });

    expect(result.content[0].type).toBe('text');

    // Cleanup
    await client.close();
    serverProcess.kill();
  });
});
```

### Performance Tests

**Load testing**:
```typescript
import { performance } from 'node:perf_hooks';

describe('MCP Server - Performance', () => {
  it('should handle 100 concurrent queries', async () => {
    const queries = Array(100).fill(null).map((_, i) =>
      server.callTool('search_learnings', {
        query: `test query ${i}`,
        limit: 10
      })
    );

    const start = performance.now();
    const results = await Promise.all(queries);
    const duration = performance.now() - start;

    expect(results).toHaveLength(100);
    expect(duration).toBeLessThan(5000); // 100 queries in <5s
  });

  it('should maintain p95 latency <200ms under load', async () => {
    const latencies: number[] = [];

    for (let i = 0; i < 1000; i++) {
      const start = performance.now();

      await server.callTool('search_learnings', {
        query: 'typescript testing',
        limit: 10
      });

      latencies.push(performance.now() - start);
    }

    latencies.sort((a, b) => a - b);
    const p95 = latencies[Math.floor(latencies.length * 0.95)];

    expect(p95).toBeLessThan(200);
  });
});
```

---

## Deployment

### Package.json Scripts

```json
{
  "scripts": {
    "mcp:build": "tsc src/mcp/index.ts --outDir dist/mcp",
    "mcp:start": "node dist/mcp/index.js",
    "mcp:dev": "tsx watch src/mcp/index.ts",
    "mcp:test": "vitest run src/mcp/**/*.test.ts"
  }
}
```

### Claude Code Configuration

**File**: `.claude/mcp.json`

```json
{
  "mcpServers": {
    "global-context-learnings": {
      "command": "node",
      "args": ["./dist/mcp/index.js"],
      "env": {
        "DB_PATH": "./context.db",
        "NODE_ENV": "production"
      }
    }
  }
}
```

### Monitoring

**Health check endpoint** (future HTTP transport):
```typescript
app.get('/health', (req, res) => {
  const isHealthy = db.prepare('SELECT 1').get();

  res.json({
    status: isHealthy ? 'healthy' : 'unhealthy',
    uptime_seconds: process.uptime(),
    p95_latency_ms: getP95Latency(),
    total_queries: queryMetrics.length
  });
});
```

---

## Performance Budget Summary

| Operation | Budget | Measurement |
|-----------|--------|-------------|
| search_learnings | <200ms p95 | FTS5 query + filters + serialization |
| get_learning_by_id | <50ms p95 | Primary key lookup |
| get_learning_context | <200ms p95 | Join + messages fetch |
| Resource reads | <100ms p95 | Indexed queries |
| Server startup | <1s | Database connection + index load |

---

## Related Documents

### Standards
- [STANDARDS.md](../STANDARDS.md) - Canonical schema, status enums, ULID, performance budgets

### Architecture
- [Global Context Network](./architecture-global-context-network-2025-01-16.md) - System overview
- [Database Schema](../reference/reference-database-schema-2025-01-16.md) - learnings table, FTS5 indexes

### Reference
- [Claude Agent SDK API](../reference/reference-claude-agent-sdk-api-2025-01-16.md) - MCP integration patterns

### Guides
- [Phase 6 Tasks](../plans/plan-phase-6-tasks-2025-01-16.md) - MCP server implementation tasks

---

## Appendix: MCP SDK Compliance

### Tools Schema Compliance

**Required Fields**:
- `name`: Tool identifier (kebab-case)
- `description`: Human-readable description
- `inputSchema`: JSON Schema (draft-07)

**Example**:
```typescript
{
  name: 'search_learnings',
  description: 'Search learnings using full-text search',
  inputSchema: {
    type: 'object',
    properties: { /* ... */ },
    required: ['query']
  }
}
```

### Resources Schema Compliance

**Required Fields**:
- `uri`: Unique resource identifier (custom scheme)
- `name`: Human-readable name
- `description`: Resource description
- `mimeType`: Content type (e.g., 'application/json')

**Example**:
```typescript
{
  uri: 'context://learnings/recent',
  name: 'Recent Learnings',
  description: 'Latest 20 learnings',
  mimeType: 'application/json'
}
```

### Response Format

**Tool Call Response**:
```typescript
{
  content: [
    {
      type: 'text',
      text: '{"results": [...]}' // JSON string
    }
  ],
  isError?: boolean // Optional error flag
}
```

**Resource Read Response**:
```typescript
{
  contents: [
    {
      uri: 'context://learnings/recent',
      mimeType: 'application/json',
      text: '{"learnings": [...]}'
    }
  ]
}
```

---

**Document Status**: Active - Implementation-ready with complete schemas, performance plans, and security model

**Next Steps**:
1. Get external review (GPT-5) to validate MCP SDK compliance
2. Implement server.ts with tool and resource handlers
3. Add comprehensive tests (unit, integration, performance)
4. Validate <200ms p95 latency with real database
5. Document Claude Code discovery integration
````

## File: architecture/architecture-subagent-system-2025-01-16.md
````markdown
# Subagent System Architecture

> Architecture for subagent-driven development using Claude Agent SDK

---
title: Subagent System Architecture
category: architecture
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [subagents, claude-agent-sdk, development-workflow, parallelization]
---

## Overview

The Global Context Network uses a **subagent-driven development model** where ALL implementation and testing is delegated to specialized Claude agents. This ensures focused expertise, parallel execution, and built-in quality validation.

**Core Principle**: Never implement directly - always delegate to specialized subagents.

## Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│                    Main Orchestrator Agent                        │
│                  (Claude Code Main Session)                       │
└────────────┬───────────────────────────────┬─────────────────────┘
             │                                │
             ▼ Delegation                    ▼ Delegation
┌────────────────────────┐      ┌────────────────────────┐
│ Implementation Subagent│      │    Test Subagent       │
│  - Reads requirements  │      │  - Generates tests     │
│  - Writes code         │      │  - Validates quality   │
│  - Runs locally        │      │  - Checks coverage     │
└────────────┬───────────┘      └────────────┬───────────┘
             │                                │
             │                                │
             ▼                                ▼
┌────────────────────────┐      ┌────────────────────────┐
│  Quality Gate Subagent │◄─────┤ Integration Subagent   │
│  - Lint + Type Check   │      │  - Component testing   │
│  - Security scan       │      │  - E2E validation      │
│  - Performance review  │      │  - Regression check    │
└────────────────────────┘      └────────────────────────┘
```

## Subagent Types

### 1. Implementation Subagents

**Purpose**: Build features and components

**Specialized By Phase**:

| Phase | Subagent | Responsibility | Tools |
|-------|----------|----------------|-------|
| 0 | `foundation-setup-agent` | TypeScript + Vitest setup | Write, Bash, Read |
| 0 | `database-schema-agent` | Schema + migrations | Write, Bash |
| 0 | `test-infrastructure-agent` | Test utilities | Write, Read |
| 1 | `hook-developer-agent` | Hook scripts | Write, Bash, Read |
| 1 | `event-collector-agent` | Event aggregation | Write, Read |
| 1 | `queue-system-agent` | Persistent queue | Write, Bash |
| 2 | `rule-sanitizer-agent` | Regex PII detection | Write, Read |
| 2 | `ai-sanitizer-agent` | LLM sanitization | Write, Read |
| 2 | `sanitization-pipeline-agent` | Pipeline orchestration | Write, Read |
| 3 | `repository-agent` | Repository pattern | Write, Read |
| 3 | `query-optimization-agent` | Indexes + queries | Write, Bash |
| 4 | `job-queue-agent` | Queue implementation | Write, Read |
| 4 | `worker-agent` | Worker processes | Write, Bash |
| 5 | `learning-extractor-agent` | Extraction pipeline | Write, Read |
| 5 | `quality-filter-agent` | Scoring + filtering | Write, Read |
| 6 | `mcp-protocol-agent` | MCP implementation | Write, Read |
| 6 | `query-optimization-agent` | Search + filtering | Write, Bash |
| 7 | `ipfs-integration-agent` | IPFS uploads | Write, Bash |
| 7 | `blockchain-agent` | Blockchain transactions | Write, Bash |

**Configuration Example**:
```typescript
{
  "hook-developer-agent": {
    description: "Implements Claude Code hooks for event capture",
    prompt: `You are a Claude Code hooks expert. Implement:
    - UserPromptSubmit hook (< 100ms)
    - Stop hook (< 100ms)
    - Event serialization
    - Error handling (never block user)
    Follow TDD: write tests first.`,
    tools: ["Write", "Read", "Bash", "mcp__test-runner__run_unit_tests"],
    model: "sonnet"
  }
}
```

### 2. Test Generation Subagents

**Purpose**: Create comprehensive test suites

**Types**:

**Unit Test Generator**:
```typescript
{
  "unit-test-generator": {
    description: "Generates unit tests with 100% coverage target",
    prompt: `Generate unit tests following TDD:
    - Test each function in isolation
    - Cover edge cases and boundaries
    - Test error conditions
    - Use proper arrange-act-assert
    - Mock external dependencies
    Target: > 85% coverage`,
    tools: ["Write", "Read", "mcp__test-runner__validate_test_quality"],
    model: "sonnet"
  }
}
```

**Integration Test Generator**:
```typescript
{
  "integration-test-generator": {
    description: "Creates integration tests for component interactions",
    prompt: `Generate integration tests:
    - Test component interactions
    - Use real dependencies where safe
    - Test database transactions
    - Verify async behavior
    - Check error propagation`,
    tools: ["Write", "Read", "Bash", "mcp__test-runner__run_integration_tests"],
    model: "sonnet"
  }
}
```

**E2E Test Generator**:
```typescript
{
  "e2e-test-generator": {
    description: "Creates end-to-end workflow tests",
    prompt: `Generate E2E tests:
    - Test complete user workflows
    - Hook → Database → MCP flow
    - Sanitization → Learning → Upload flow
    - Verify system behavior
    - Test failure scenarios`,
    tools: ["Write", "Read", "Bash", "mcp__test-runner__run_e2e_tests"],
    model: "sonnet"
  }
}
```

### 3. Test Validation Subagents

**Purpose**: Validate test quality and implementation

**Test Quality Validator**:
```typescript
{
  "test-quality-validator": {
    description: "Reviews test code quality and completeness",
    prompt: `Validate tests for:
    - Proper structure (describe, it, expect)
    - Clear test names
    - Complete edge case coverage
    - Proper assertions (not just truthy)
    - No flaky tests
    - Maintainability
    Score each test 0-1. Require > 0.8 to pass.`,
    tools: ["Read", "Grep", "mcp__test-runner__validate_test_quality"],
    model: "sonnet"
  }
}
```

**Coverage Validator**:
```typescript
{
  "coverage-validator": {
    description: "Ensures adequate test coverage",
    prompt: `Analyze coverage:
    - Lines, statements, functions, branches
    - Identify uncovered code paths
    - Flag critical missing tests
    - Require > 85% coverage
    - No untested error handlers`,
    tools: ["Read", "Bash", "mcp__test-runner__get_coverage_report"],
    model: "haiku"
  }
}
```

**Implementation Validator**:
```typescript
{
  "implementation-validator": {
    description: "Validates implementation against tests",
    prompt: `Verify implementation:
    - All tests pass
    - Handles all edge cases from tests
    - Proper error handling
    - No security vulnerabilities
    - Follows coding standards
    - Performance acceptable`,
    tools: ["Read", "Grep", "Bash", "mcp__test-runner__run_unit_tests"],
    model: "sonnet"
  }
}
```

### 4. Quality Gate Subagents

**Purpose**: Enforce standards before merge

**Code Quality Validator**:
```typescript
{
  "code-quality-validator": {
    description: "Reviews code quality and standards",
    prompt: `Review code for:
    - TypeScript strict mode compliance
    - ESLint rule adherence
    - Prettier formatting
    - Proper type annotations
    - Clear naming conventions
    - DRY principles
    - SOLID principles
    All checks must pass.`,
    tools: ["Read", "Bash", "Grep"],
    model: "sonnet"
  }
}
```

**Security Validator**:
```typescript
{
  "security-validator": {
    description: "Scans for security vulnerabilities",
    prompt: `Security audit:
    - SQL injection vulnerabilities
    - Command injection risks
    - Path traversal attempts
    - Hardcoded secrets
    - Insecure dependencies
    - XSS vectors
    Block if any critical issues found.`,
    tools: ["Read", "Bash", "Grep"],
    model: "sonnet"
  }
}
```

**Performance Validator**:
```typescript
{
  "performance-validator": {
    description: "Analyzes performance characteristics",
    prompt: `Analyze performance:
    - Algorithm complexity
    - Database query efficiency
    - Memory usage patterns
    - Blocking operations
    - Resource leaks
    Flag performance regressions.`,
    tools: ["Read", "Bash", "Grep"],
    model: "sonnet"
  }
}
```

## Subagent Workflow

### TDD Cycle with Subagents

```
1. 🔴 RED Phase (Test Generator Subagent)
   ├─► Generate failing test
   ├─► Validate test quality (Test Quality Validator)
   └─► Ensure test properly fails

2. 🟢 GREEN Phase (Implementation Subagent)
   ├─► Write minimal code to pass test
   ├─► Run tests continuously
   └─► Validate implementation (Implementation Validator)

3. 🔵 REFACTOR Phase (Refactor Subagent)
   ├─► Improve code quality
   ├─► Maintain passing tests
   └─► Re-validate quality (Code Quality Validator)

4. ✅ QUALITY GATE (Quality Gate Subagents)
   ├─► Run all validators in parallel
   ├─► Coverage > 85%
   ├─► Security scan passes
   ├─► Performance acceptable
   └─► All gates MUST pass
```

### Parallel Execution

Subagents can run in parallel when independent:

```typescript
// Phase 0 example: Parallel execution
const response = query({
  prompt: "Implement Phase 0 Foundation",
  options: {
    agents: {
      "foundation-setup": { ... },
      "database-schema": { ... },
      "test-infrastructure": { ... }
    }
  }
});

// All three agents work simultaneously
// Main agent coordinates and integrates results
```

### Sequential Dependencies

Some subagents must wait for others:

```
foundation-setup ─► database-schema ─► test-infrastructure
                    (needs project)    (needs database)
```

## Subagent Communication

### Via Main Agent

Subagents communicate through the main orchestrator:

```typescript
// Main agent delegates
const testGenResult = await runSubagent("unit-test-generator", {...});

// Main agent passes results to next subagent
const implResult = await runSubagent("implementation-agent", {
  tests: testGenResult.tests
});

// Main agent validates
const validationResult = await runSubagent("implementation-validator", {
  implementation: implResult.code,
  tests: testGenResult.tests
});
```

### Via Shared Context

Subagents can access shared resources:
- File system (read/write)
- Database (queries)
- Test results (via MCP tools)
- Coverage reports (via MCP tools)

## Benefits of Subagent Architecture

### 1. Specialization
Each subagent is expert in its domain:
- Hook developer knows hook best practices
- Test generator knows testing patterns
- Security validator knows OWASP top 10

### 2. Parallelization
Independent tasks run concurrently:
- Multiple components implemented simultaneously
- Tests generated while implementation proceeds
- Validators run in parallel for quality gates

### 3. Quality Assurance
Built-in validation at every step:
- Test quality validated before implementation
- Implementation validated against tests
- Quality gates enforce standards

### 4. Maintainability
Clear separation of concerns:
- Each subagent has focused responsibility
- Easy to update individual agents
- Consistent patterns across codebase

### 5. Auditability
Complete trace of decisions:
- Each subagent produces artifacts
- Validation results documented
- Quality scores tracked

## Configuration Management

### Subagent Definitions

Stored in configuration files:

```typescript
// subagents/implementation-agents.ts
export const implementationAgents = {
  "hook-developer-agent": { ... },
  "event-collector-agent": { ... },
  // ... more agents
};

// subagents/test-agents.ts
export const testAgents = {
  "unit-test-generator": { ... },
  "integration-test-generator": { ... },
  // ... more agents
};

// subagents/validation-agents.ts
export const validationAgents = {
  "test-quality-validator": { ... },
  "coverage-validator": { ... },
  // ... more agents
};
```

### Agent Orchestration

```typescript
import { query } from "@anthropic-ai/claude-agent-sdk";
import { implementationAgents, testAgents, validationAgents } from './subagents';

export async function implementPhase(phaseNumber: number) {
  const response = query({
    prompt: `Implement Phase ${phaseNumber} following TDD`,
    options: {
      model: "claude-sonnet-4-5",
      agents: {
        ...implementationAgents,
        ...testAgents,
        ...validationAgents
      },
      mcpServers: {
        "test-runner": testRunnerServer
      }
    }
  });

  for await (const message of response) {
    // Process subagent communications
    if (message.type === 'system' && message.subtype === 'subagent_start') {
      console.log(`Starting: ${message.agent_name}`);
    }
    if (message.type === 'system' && message.subtype === 'subagent_end') {
      console.log(`Completed: ${message.agent_name}`);
    }
  }
}
```

## Error Handling

### Subagent Failures

**Retry Logic**:
```typescript
async function runWithRetry(subagentName: string, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await runSubagent(subagentName);
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await sleep(2 ** i * 1000); // Exponential backoff
    }
  }
}
```

**Fallback Strategies**:
- If test generator fails → Manual test creation prompt
- If implementation fails → Simplify requirements
- If validator fails → Manual review required

### Quality Gate Failures

**Block and Report**:
```typescript
if (validationResult.coverageScore < 0.85) {
  throw new Error(`Coverage ${validationResult.coverageScore} < 0.85 required`);
}

if (validationResult.securityIssues.length > 0) {
  throw new Error(`Security issues: ${validationResult.securityIssues.join(', ')}`);
}
```

## Performance Optimization

### Selective Subagent Invocation

Don't invoke all subagents for every task:

```typescript
// Simple task: minimal validation
if (task.complexity === 'simple') {
  agents = ['implementation-agent', 'unit-test-generator'];
}

// Complex task: full validation
if (task.complexity === 'complex') {
  agents = [
    'implementation-agent',
    'unit-test-generator',
    'integration-test-generator',
    'test-quality-validator',
    'code-quality-validator',
    'security-validator'
  ];
}
```

### Caching Results

Cache subagent results to avoid re-runs:

```typescript
const cache = new Map<string, SubagentResult>();

async function getCachedSubagentResult(name: string, input: any) {
  const key = `${name}:${JSON.stringify(input)}`;
  if (cache.has(key)) {
    return cache.get(key);
  }
  const result = await runSubagent(name, input);
  cache.set(key, result);
  return result;
}
```

## Monitoring & Metrics

### Subagent Performance

Track subagent execution:
- Invocation count
- Success rate
- Average duration
- Error types

### Quality Metrics

Track validation results:
- Test coverage trends
- Quality gate pass rate
- Security issue count
- Performance regression count

## Related Documents

### Architecture
- [Global Context Network](./architecture-global-context-network-2025-01-16.md)
- [Testing Harness](./architecture-testing-harness-2025-01-16.md)

### Decisions
- [ADR: Subagent-Driven Development](../decisions/decision-subagent-driven-development-2025-01-16.md)

### Guides
- [Using Subagents](../guides/guide-using-subagents-2025-01-16.md)
- [TDD Workflow](../guides/guide-tdd-workflow-2025-01-16.md)

### Reference
- [Subagent Types](../reference/reference-subagent-types-2025-01-16.md)
- [Claude Agent SDK API](../reference/reference-claude-agent-sdk-api-2025-01-16.md)
````

## File: architecture/architecture-testing-harness-2025-01-16.md
````markdown
# Testing Harness Architecture

> Claude-powered testing infrastructure with TDD enforcement and quality gates

---
title: Testing Harness Architecture
category: architecture
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [testing, claude-agent-sdk, tdd, quality-gates, subagents]
---

## Overview

The Testing Harness is a Claude-powered infrastructure that ensures code quality through Test-Driven Development (TDD), automated test generation, continuous validation, and quality gate enforcement. It leverages the Claude Agent SDK to create specialized subagents for different testing concerns.

**Core Principle**: Every feature is test-first, every test is validated, every implementation is verified.

## Goals

- Enforce TDD workflow (Red-Green-Refactor)
- Generate comprehensive test suites automatically
- Validate test quality and coverage
- Ensure zero regressions through continuous testing
- Provide fast feedback loops (< 30s for unit tests)
- Support deterministic testing for LLM-powered components

## Non-Goals

- Manual test writing (delegated to subagents)
- UI-based test runners (CLI-only for MVP)
- Cross-browser testing (not needed for Node.js backend)
- Performance testing (covered separately)

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                   Claude Agent SDK                           │
│                  (Main Orchestrator)                         │
└──────────┬──────────────────────────────────────────────────┘
           │
           ├──────────────────────────────────────┐
           │                                       │
           ▼ Delegation                           ▼ Delegation
┌──────────────────────────┐          ┌──────────────────────────┐
│  Test Generator Subagent │          │ Test Validator Subagent  │
│  ┌────────────────────┐  │          │  ┌────────────────────┐  │
│  │ Unit Test Gen      │  │          │  │ Quality Validator  │  │
│  │ Integration Gen    │  │          │  │ Coverage Validator │  │
│  │ E2E Test Gen       │  │          │  │ Impl Validator     │  │
│  └────────────────────┘  │          │  └────────────────────┘  │
└──────────┬───────────────┘          └───────────┬──────────────┘
           │                                       │
           │                                       │
           ▼                                       ▼
┌──────────────────────────┐          ┌──────────────────────────┐
│   Implementation Agent   │          │  Quality Gate Subagent   │
│  - Writes minimal code   │          │  ┌────────────────────┐  │
│  - Passes tests          │◄─────────┤  │ Lint Validator     │  │
│  - Refactors             │          │  │ Type Validator     │  │
└──────────────────────────┘          │  │ Security Validator │  │
                                       │  │ Performance Check  │  │
                                       │  └────────────────────┘  │
                                       └──────────────────────────┘
           │                                       │
           └───────────────┬───────────────────────┘
                           ▼
                  ┌─────────────────┐
                  │  MCP Test Tool  │
                  │  ┌───────────┐  │
                  │  │ Run Tests │  │
                  │  │ Coverage  │  │
                  │  │ Validate  │  │
                  │  └───────────┘  │
                  └─────────┬───────┘
                            │
                            ▼
                  ┌─────────────────┐
                  │   Vitest        │
                  │   (Test Runner) │
                  └─────────────────┘
```

## Components

### 1. Test Generator Subagents

**Purpose**: Automatically generate comprehensive test suites

#### Unit Test Generator

```typescript
{
  "unit-test-generator": {
    description: "Generates unit tests following TDD principles",
    prompt: `Generate unit tests with these requirements:
    - Test each function in complete isolation
    - Cover all edge cases and boundary conditions
    - Test error conditions thoroughly
    - Use arrange-act-assert pattern
    - Mock all external dependencies
    - Target coverage: > 85% lines, > 80% branches

    Use deterministic fixtures for LLM-powered components.
    Include property-based tests for complex logic.`,
    tools: ["Write", "Read", "mcp__test-runner__run_unit_tests"],
    model: "sonnet-4-5"
  }
}
```

**Key Features**:
- Deterministic testing for LLM steps (fixture-based outputs)
- Property-based testing using fast-check
- Adversarial test cases for PII detection
- Cost controls (local mocks for CI, cloud models for validation)

#### Integration Test Generator

```typescript
{
  "integration-test-generator": {
    description: "Creates integration tests for component interactions",
    prompt: `Generate integration tests:
    - Test real component interactions
    - Use actual database (SQLite in-memory for tests)
    - Verify async behavior and timing
    - Test transaction boundaries
    - Check error propagation paths
    - Validate job queue semantics`,
    tools: ["Write", "Read", "Bash", "mcp__test-runner__run_integration_tests"],
    model: "sonnet-4-5"
  }
}
```

**Key Features**:
- Real database interactions (in-memory SQLite)
- Async behavior verification
- Transaction testing
- Queue semantics validation

#### E2E Test Generator

```typescript
{
  "e2e-test-generator": {
    description: "Creates end-to-end workflow tests",
    prompt: `Generate E2E tests for complete workflows:
    - Hook → Event Queue → Sanitization → Database
    - Learning Extraction → Quality Filter → MCP Query
    - Mining Upload → IPFS → Blockchain
    - Test failure scenarios and recovery
    - Verify data integrity end-to-end`,
    tools: ["Write", "Read", "Bash", "mcp__test-runner__run_e2e_tests"],
    model: "sonnet-4-5"
  }
}
```

**Key Features**:
- Complete system workflows
- Failure scenario testing
- Data integrity verification
- Recovery path validation

### 2. Test Validation Subagents

**Purpose**: Ensure test quality and completeness

#### Test Quality Validator

```typescript
{
  "test-quality-validator": {
    description: "Reviews test code quality",
    prompt: `Validate tests against quality criteria:
    - Proper structure (describe, it, expect with clear names)
    - Complete edge case coverage
    - Strong assertions (not just truthy checks)
    - No flaky tests (deterministic execution)
    - Maintainable code (clear intent, minimal duplication)
    - Performance tests have clear thresholds

    Score each test 0-1. Require score > 0.8 to pass.
    Flag any non-deterministic behavior.`,
    tools: ["Read", "Grep", "mcp__test-runner__validate_test_quality"],
    model: "sonnet-4-5"
  }
}
```

**Quality Criteria**:
- Clear test names describing behavior
- Proper arrange-act-assert structure
- Strong, specific assertions
- Deterministic execution
- No shared mutable state
- Timing-independent

#### Coverage Validator

```typescript
{
  "coverage-validator": {
    description: "Ensures adequate test coverage",
    prompt: `Analyze test coverage:
    - Lines, statements, functions, branches
    - Critical paths MUST be covered
    - Sanitization code > 95% coverage
    - Queue error paths > 90% coverage
    - Hook handlers > 90% coverage
    - Overall target: > 85%

    Flag uncovered critical code.
    Fail if any critical path lacks coverage.`,
    tools: ["Read", "Bash", "mcp__test-runner__get_coverage_report"],
    model: "haiku"
  }
}
```

**Coverage Gates**:
- Sanitization: > 95% lines/branches
- Hooks: > 90% coverage
- Queue: > 90% error paths
- Overall: > 85% coverage
- No untested critical paths

#### Implementation Validator

```typescript
{
  "implementation-validator": {
    description: "Validates implementation against tests",
    prompt: `Verify implementation quality:
    - All tests pass
    - Handles all edge cases from tests
    - Proper error handling (no silent failures)
    - No security vulnerabilities (SQL injection, path traversal)
    - Follows TypeScript strict mode
    - Performance acceptable (within budgets)
    - No LLM over-trust (validate all outputs)`,
    tools: ["Read", "Grep", "Bash", "mcp__test-runner__run_unit_tests"],
    model: "sonnet-4-5"
  }
}
```

**Validation Checks**:
- All tests passing
- Error handling comprehensive
- Security vulnerabilities absent
- Performance within budgets
- Type safety enforced

### 3. Quality Gate Subagents

**Purpose**: Enforce standards before code integration

#### Code Quality Validator

```typescript
{
  "code-quality-validator": {
    description: "Reviews code quality standards",
    prompt: `Review code for quality:
    - TypeScript strict mode compliance
    - ESLint rules adhered to
    - Prettier formatting applied
    - Proper type annotations (no 'any')
    - Clear naming conventions
    - DRY principles followed
    - SOLID principles applied

    All checks MUST pass.`,
    tools: ["Read", "Bash", "Grep"],
    model: "sonnet-4-5"
  }
}
```

**Quality Checks**:
- Lint: ESLint passing
- Types: Strict TypeScript
- Format: Prettier applied
- Style: Naming conventions
- Architecture: SOLID principles

#### Security Validator

```typescript
{
  "security-validator": {
    description: "Scans for security vulnerabilities",
    prompt: `Security audit:
    - SQL injection (use parameterized queries only)
    - Command injection (sanitize shell inputs)
    - Path traversal (validate file paths)
    - Hardcoded secrets (reject commits with secrets)
    - Prompt injection resistance (LLM as stateless classifier)
    - Deserialization risks (validate JSON schemas)
    - Supply chain (check npm dependencies with Socket)

    Block if ANY critical issues found.`,
    tools: ["Read", "Bash", "Grep", "mcp__socket__depscore"],
    model: "sonnet-4-5"
  }
}
```

**Security Checks**:
- SQL injection prevention
- Command injection blocking
- Path traversal validation
- Secrets scanning
- Dependency security (Socket)
- Prompt injection resistance

#### Performance Validator

```typescript
{
  "performance-validator": {
    description: "Analyzes performance characteristics",
    prompt: `Analyze performance:
    - Algorithm complexity (avoid O(n²) in hot paths)
    - Database query efficiency (use EXPLAIN)
    - Memory usage patterns (no leaks)
    - Blocking operations (async where needed)
    - Resource leaks (proper cleanup)

    Performance budgets:
    - Hook execution: < 100ms
    - Event queueing: < 50ms
    - DB queries: < 100ms
    - MCP queries: < 200ms

    Fail if budgets exceeded.`,
    tools: ["Read", "Bash", "Grep"],
    model: "sonnet-4-5"
  }
}
```

**Performance Budgets**:
- Hook execution: < 100ms
- Event queueing: < 50ms
- Database queries: < 100ms
- MCP queries: < 200ms
- Sanitization: < 2s per conversation

### 4. MCP Test Runner Server

**Purpose**: Provide testing tools to subagents

#### Tool Schemas

**run_unit_tests**:
```typescript
{
  name: "run_unit_tests",
  description: "Execute unit tests with coverage",
  inputSchema: {
    type: "object",
    properties: {
      pattern: { type: "string", description: "Test file pattern (e.g., '*.test.ts')" },
      timeout: { type: "number", default: 30000, description: "Timeout in ms" },
      coverage: { type: "boolean", default: true, description: "Collect coverage" }
    }
  },
  outputSchema: {
    type: "object",
    properties: {
      passed: { type: "number" },
      failed: { type: "number" },
      skipped: { type: "number" },
      coverage: {
        type: "object",
        properties: {
          lines: { type: "number" },
          statements: { type: "number" },
          functions: { type: "number" },
          branches: { type: "number" }
        }
      },
      duration: { type: "number" },
      errors: { type: "array", items: { type: "string" } }
    }
  }
}
```

**validate_test_quality**:
```typescript
{
  name: "validate_test_quality",
  description: "Validate test code quality",
  inputSchema: {
    type: "object",
    properties: {
      testFile: { type: "string", description: "Path to test file" }
    },
    required: ["testFile"]
  },
  outputSchema: {
    type: "object",
    properties: {
      score: { type: "number", minimum: 0, maximum: 1 },
      issues: { type: "array", items: { type: "string" } },
      recommendations: { type: "array", items: { type: "string" } }
    }
  }
}
```

**get_coverage_report**:
```typescript
{
  name: "get_coverage_report",
  description: "Get detailed coverage report",
  inputSchema: {
    type: "object",
    properties: {
      format: { type: "string", enum: ["json", "html", "lcov"], default: "json" }
    }
  },
  outputSchema: {
    type: "object",
    properties: {
      summary: {
        type: "object",
        properties: {
          lines: { type: "object" },
          statements: { type: "object" },
          functions: { type: "object" },
          branches: { type: "object" }
        }
      },
      files: { type: "array" },
      uncovered: { type: "array", description: "Critical uncovered paths" }
    }
  }
}
```

## TDD Workflow

### Red-Green-Refactor Cycle

```
1. 🔴 RED Phase
   ├─► Test Generator creates failing test
   ├─► Test Quality Validator reviews test
   ├─► Run test → confirm it fails correctly
   └─► Score test quality (must be > 0.8)

2. 🟢 GREEN Phase
   ├─► Implementation Agent writes minimal code
   ├─► Run tests continuously
   ├─► Implementation Validator checks quality
   └─► All tests passing

3. 🔵 REFACTOR Phase
   ├─► Improve code quality
   ├─► Code Quality Validator reviews
   ├─► Run tests (must still pass)
   └─► Performance Validator checks budgets

4. ✅ QUALITY GATE
   ├─► Lint passing (ESLint)
   ├─► Types passing (TypeScript strict)
   ├─► Coverage > 85%
   ├─► Security scan clean
   ├─► Performance budgets met
   └─► ALL gates MUST pass
```

### Deterministic Testing for LLM Components

**Challenge**: LLM outputs are non-deterministic

**Solution**: Fixture-based testing

```typescript
// tests/fixtures/sanitization-outputs.ts
export const sanitizationFixtures = {
  "test-case-1": {
    input: "My email is john@example.com",
    expected: "My email is <EMAIL_1>",
    piiDetected: [
      { type: "email", value: "john@example.com", replacement: "<EMAIL_1>" }
    ]
  },
  "test-case-2": {
    input: "API key: sk-1234567890abcdef",
    expected: "API key: <API_KEY_1>",
    piiDetected: [
      { type: "api_key", value: "sk-1234567890abcdef", replacement: "<API_KEY_1>" }
    ]
  }
};

// tests/sanitization.test.ts
describe("Sanitization", () => {
  it("should sanitize using fixtures", () => {
    const fixture = sanitizationFixtures["test-case-1"];
    const result = sanitize(fixture.input, { useMock: true });
    expect(result.sanitized).toBe(fixture.expected);
    expect(result.piiDetected).toEqual(fixture.piiDetected);
  });
});
```

**Cost Controls**:
- CI uses local mocks (fixtures)
- Validation uses real LLM (rate-limited)
- Budget caps per day
- Graceful degradation to rules-only

### Property-Based Testing

For complex logic (especially PII detection):

```typescript
import { fc } from "fast-check";

describe("PII Detection Properties", () => {
  it("should detect all email formats", () => {
    fc.assert(
      fc.property(fc.emailAddress(), (email) => {
        const result = detectPII(email);
        return result.some(pii => pii.type === "email");
      })
    );
  });

  it("should be idempotent", () => {
    fc.assert(
      fc.property(fc.string(), (input) => {
        const first = sanitize(input);
        const second = sanitize(first.sanitized);
        return first.sanitized === second.sanitized;
      })
    );
  });
});
```

### Fuzz Testing for PII

```typescript
import { generatePIITestCases } from "./fuzz-generators";

describe("PII Fuzz Tests", () => {
  it("should detect adversarial PII patterns", () => {
    const testCases = generatePIITestCases(1000);

    for (const testCase of testCases) {
      const result = detectPII(testCase.input);
      expect(result.length).toBeGreaterThan(0);
      expect(result.some(pii => pii.type === testCase.expectedType)).toBe(true);
    }
  });
});
```

## Performance Testing

### Performance Test Structure

```typescript
describe("Performance Tests", () => {
  it("hook execution should be < 100ms", async () => {
    const start = performance.now();
    await hookHandler({ type: "UserPromptSubmit", data: sampleData });
    const duration = performance.now() - start;

    expect(duration).toBeLessThan(100);
  });

  it("should handle 100 concurrent events", async () => {
    const events = Array(100).fill(null).map(() => createEvent());
    const start = performance.now();

    await Promise.all(events.map(e => processEvent(e)));

    const duration = performance.now() - start;
    expect(duration).toBeLessThan(5000); // 50ms per event avg
  });
});
```

### Load Testing

```typescript
// tests/load/sanitization-load.test.ts
describe("Sanitization Load Tests", () => {
  it("should maintain performance at 10k conversations", async () => {
    const conversations = generateConversations(10000);

    const start = performance.now();
    for (const conv of conversations) {
      await sanitizeConversation(conv);
    }
    const duration = performance.now() - start;

    const avgDuration = duration / conversations.length;
    expect(avgDuration).toBeLessThan(2000); // < 2s per conversation
  });
});
```

## Error Handling

### Test Execution Errors

**Flaky Tests**:
```typescript
// Detect flaky tests
const results = [];
for (let i = 0; i < 10; i++) {
  results.push(await runTest());
}

const allPassed = results.every(r => r.passed);
const allFailed = results.every(r => !r.passed);

if (!allPassed && !allFailed) {
  throw new Error("Flaky test detected!");
}
```

**Timeout Handling**:
```typescript
// tests/utils/timeout.ts
export async function withTimeout<T>(
  promise: Promise<T>,
  timeoutMs: number
): Promise<T> {
  const timeout = new Promise<never>((_, reject) =>
    setTimeout(() => reject(new Error("Test timeout")), timeoutMs)
  );
  return Promise.race([promise, timeout]);
}
```

### Subagent Failures

**Retry Logic**:
```typescript
async function runSubagentWithRetry(
  subagentName: string,
  maxRetries = 3
): Promise<SubagentResult> {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await runSubagent(subagentName);
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await sleep(2 ** i * 1000); // Exponential backoff
    }
  }
  throw new Error("Max retries exceeded");
}
```

**Fallback Strategies**:
- Test generator fails → Manual test creation prompt
- Implementation fails → Simplify requirements
- Validator fails → Manual review required

## Integration with Claude Agent SDK

### Agent Configuration

```typescript
import { query } from "@anthropic-ai/claude-agent-sdk";

export async function implementFeatureWithTDD(feature: string) {
  const response = query({
    prompt: `Implement ${feature} following strict TDD`,
    options: {
      model: "claude-sonnet-4-5",
      agents: {
        "unit-test-generator": unitTestGeneratorConfig,
        "implementation-agent": implementationAgentConfig,
        "test-quality-validator": testQualityValidatorConfig,
        "coverage-validator": coverageValidatorConfig,
        "code-quality-validator": codeQualityValidatorConfig
      },
      mcpServers: {
        "test-runner": testRunnerServer
      }
    }
  });

  for await (const message of response) {
    if (message.type === 'system' && message.subtype === 'subagent_start') {
      console.log(`Starting: ${message.agent_name}`);
    }
    if (message.type === 'system' && message.subtype === 'subagent_end') {
      console.log(`Completed: ${message.agent_name}`);
    }
  }
}
```

### Local Sandboxing

**Security**: Subagents run in restricted environment

```typescript
// Disable shell access unless explicitly allowed
const subagentConfig = {
  tools: ["Write", "Read"], // NO Bash by default
  allowedPaths: ["/project/src", "/project/tests"], // Restrict file access
  networkAccess: false, // No network by default
  maxExecutionTime: 300000 // 5 minute timeout
};
```

## Monitoring and Metrics

### Key Metrics

**Test Performance**:
- Test execution time (target: < 30s for unit tests)
- Coverage percentage (target: > 85%)
- Flaky test count (target: 0)
- Test quality score (target: > 0.8)

**Quality Gates**:
- Gate pass rate (target: 100%)
- Blocker issues (target: 0)
- Security vulnerabilities (target: 0)
- Performance budget violations (target: 0)

**Subagent Performance**:
- Invocation count per type
- Success rate per subagent
- Average duration per subagent
- Retry rate

### Logging

```typescript
// Structured logging for test runs
logger.info("test_run_started", {
  testType: "unit",
  pattern: "*.test.ts",
  coverage: true,
  timestamp: Date.now()
});

logger.info("test_run_completed", {
  passed: 42,
  failed: 0,
  duration: 15234,
  coverage: { lines: 87.5, branches: 82.3 },
  timestamp: Date.now()
});
```

## Related Documents

### Architecture
- [Global Context Network](./architecture-global-context-network-2025-01-16.md)
- [Subagent System](./architecture-subagent-system-2025-01-16.md)

### Guides
- [TDD Workflow](../guides/guide-tdd-workflow-2025-01-16.md)
- [Testing Harness Usage](../guides/guide-testing-harness-usage-2025-01-16.md)

### Reference
- [Testing Strategy](../reference/reference-testing-strategy-2025-01-16.md)
- [Claude Agent SDK API](../reference/reference-claude-agent-sdk-api-2025-01-16.md)
````

## File: architecture/INDEX.md
````markdown
# Architecture Documentation

> Last updated: 2025-01-16

## Overview

This directory contains system design documents, component architectures, and technical specifications for the Global Context Network MVP.

## Documents

### Active Documents

| Date | Document | Status | Description |
|------|----------|--------|-------------|
| 2025-01-16 | [architecture-global-context-network-2025-01-16.md](./architecture-global-context-network-2025-01-16.md) | Active | Complete system architecture overview |
| 2025-01-16 | [architecture-subagent-system-2025-01-16.md](./architecture-subagent-system-2025-01-16.md) | Active | Subagent-driven development architecture |
| 2025-01-16 | [architecture-testing-harness-2025-01-16.md](./architecture-testing-harness-2025-01-16.md) | Active | Claude-powered testing infrastructure |
| 2025-01-16 | [architecture-hooks-event-capture-2025-01-16.md](./architecture-hooks-event-capture-2025-01-16.md) | Active | Hook implementation and event capture system |
| 2025-01-16 | [architecture-sanitization-pipeline-2025-01-16.md](./architecture-sanitization-pipeline-2025-01-16.md) | Active | PII detection and sanitization architecture |
| 2025-01-16 | [architecture-learning-extraction-2025-01-16.md](./architecture-learning-extraction-2025-01-16.md) | Active | Learning extraction and quality scoring |
| 2025-01-16 | [architecture-mcp-server-2025-01-16.md](./architecture-mcp-server-2025-01-16.md) | Active | MCP server implementation for agent queries |
| 2025-01-16 | [architecture-database-schema-2025-01-16.md](./architecture-database-schema-2025-01-16.md) | Active | Database schema design and migrations |
| 2025-01-16 | [architecture-async-processing-2025-01-16.md](./architecture-async-processing-2025-01-16.md) | Active | Complete async job queue architecture with workers, leasing, and retry logic |

## Key Architectural Patterns

### Privacy-First Design
ALL data is sanitized BEFORE storage. The system guarantees zero PII leaks through:
- Rule-based PII detection (fast, deterministic)
- AI-powered context-aware sanitization
- Hybrid validation pipeline
- Audit logging for continuous improvement

### Subagent-Driven Development
Every component is implemented by specialized subagents:
- Implementation subagents build features
- Test subagents generate and validate tests
- Quality gate subagents enforce standards
- Integration subagents verify component interactions

### Async-First Architecture
Non-blocking design with persistent queues:
- Event capture never blocks user
- Sanitization runs asynchronously
- Learning extraction happens in background
- Mining/upload processes independently

## System Components

1. **Event Capture Layer** (Hooks + Queue)
2. **Sanitization Pipeline** (Rules + AI)
3. **Storage Layer** (SQLite + Migrations)
4. **Async Processing** (Job Queue + Workers)
5. **Learning Extraction** (Analysis + Scoring)
6. **Query Interface** (MCP Server)
7. **Network Layer** (IPFS + Blockchain)

## Related Categories

- [Decisions](../decisions/INDEX.md) - ADRs explaining architectural choices
- [Plans](../plans/INDEX.md) - Implementation roadmaps
- [Reference](../reference/INDEX.md) - Technical specifications

## Quick Tips

- Start with the global architecture document for overview
- Review component-specific docs for deep dives
- Check ADRs for rationale behind decisions
- Use diagrams for understanding data flow
````

## File: decisions/decision-async-processing-model-2025-01-16.md
````markdown
---
title: ADR-006: Async Processing Model with Job Queue
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [async, job-queue, performance, reliability]
---

# ADR-006: Async Processing Model with Job Queue

## Status

Accepted

Date: 2025-01-16

## Context

The Global Context Network has several processing stages that must not block user interaction:

1. **Event capture** - Hooks must complete < 100ms
2. **Sanitization** - PII detection takes 1-2s per conversation
3. **Learning extraction** - AI analysis takes 5-10s
4. **Upload to network** - IPFS/blockchain can take 10-30s
5. **Quality validation** - Multi-stage validation takes time

**Requirements**:
- **Never block user** - Claude Code must remain responsive
- **Persist across restarts** - Jobs survive crashes/shutdowns
- **Retry failed jobs** - Network issues, temporary failures
- **Ordered processing** - Respect dependencies (sanitize before extract)
- **Idempotency** - Safe to retry jobs
- **Offline tolerance** - Queue locally, sync when online
- **Observable** - Monitor queue depth, latency, errors

**Why Async is Critical**:
- User experience constraint: < 100ms p95 for hooks
- Processing is inherently slow: sanitization, learning extraction, uploads
- Network operations are unpredictable: IPFS, blockchain
- Offline capability: must work without constant connectivity
- Predictable local operation before global publishing

## Decision

Use SQLite-based persistent job queue with async workers.

**Architecture**:
```
Hook (< 100ms) → Event Queue → Async Workers → Results
                       ↓
                   (Persisted)
```

**Job Types**:
1. `sanitize_conversation` - Run PII sanitization pipeline
2. `extract_learning` - Generate learnings from conversation
3. `mine_upload` - Upload to IPFS/blockchain

**Queue Properties**:
- Persistent (survives restarts)
- Priority-based (critical jobs first)
- Retry with exponential backoff
- Dead letter queue for failed jobs
- Atomic enqueue (outbox pattern)

**Worker Design**:
- Independent processes
- Poll queue for jobs
- Lease-based execution
- Idempotent job handlers
- Graceful shutdown

## Consequences

### Positive

- **Never blocks user** - Hooks return immediately (< 50ms)
- **Persists across restarts** - Jobs survive crashes
- **Retry with backoff** - Handles transient failures
- **No external dependencies** - SQLite-based, local-first
- **Offline tolerance** - Queue locally, flush when online
- **Predictable local operation** - No network required for queueing
- **Observable** - Full visibility into queue state
- **Ordered processing** - Dependency management built-in

### Negative

- **Single-process limitation** - One worker at a time (MVP acceptable)
- **No distributed processing** - Can't scale horizontally (fine for MVP)
- **Polling overhead** - Workers poll queue continuously
- **At-least-once semantics** - Exactly-once impossible (need idempotency)
- **Delayed feedback** - Users don't see immediate results

### Neutral

- **Job states** - Need to track queued, in-progress, completed, failed
- **Lease expiry** - Jobs can be retried if worker crashes
- **Metrics tracking** - Monitor queue depth, age, success rate
- **Backpressure handling** - May need to sample or drop if overwhelmed

## Alternatives Considered

### Alternative 1: Synchronous Processing

**Description**: Process everything in the hook itself.

**Pros**:
- Simpler code
- Immediate feedback
- No queue complexity
- Easier debugging

**Cons**:
- **Blocks user** - Unacceptable UX (2-10s delays)
- **Hook timeout** - Claude Code enforces timeout
- **Poor offline support** - Fails if network down
- **No retry** - Transient failures permanent

**Why not chosen**: Violates UX constraint (< 100ms hooks). Blocking user is unacceptable.

### Alternative 2: Redis/RabbitMQ/SQS

**Description**: Use external queue service.

**Pros**:
- Production-ready
- Distributed by design
- Excellent tooling
- Battle-tested

**Cons**:
- **External dependency** - Violates local-first principle
- **Setup overhead** - Install, configure, manage
- **Network dependency** - Offline mode broken
- **Operational complexity** - Monitor, backup, upgrade
- **Overkill for MVP** - Single user doesn't need distributed queue

**Why not chosen**: Too much operational overhead for single-user MVP. SQLite meets all requirements.

### Alternative 3: OS Job Schedulers (cron, systemd timers, launchd)

**Description**: Use OS-level job scheduling.

**Pros**:
- No custom queue needed
- OS handles execution
- Standard tooling

**Cons**:
- **No dynamic queueing** - Can't enqueue at runtime
- **Poor job management** - Hard to track state
- **No retries** - Must implement separately
- **Platform-specific** - Different on macOS/Linux/Windows

**Why not chosen**: Not suitable for dynamic event-driven queueing.

### Alternative 4: Simple setTimeout/setInterval

**Description**: Use JavaScript timers for delayed execution.

**Pros**:
- Very simple
- No dependencies
- Easy to understand

**Cons**:
- **No persistence** - Lost on restart
- **No retry** - Failures disappear
- **No ordering** - Race conditions
- **No observability** - Can't inspect pending jobs

**Why not chosen**: Violates "never lose data" requirement. Jobs must persist across restarts.

### Alternative 5: Temporal/Prefect/Dagster

**Description**: Use workflow orchestration framework.

**Pros**:
- Excellent DAG orchestration
- Built-in retry and error handling
- Great observability
- Scales to production
- Durable execution

**Cons**:
- **Heavy** - Requires server infrastructure
- **Overkill** - Too complex for MVP
- **Operational overhead** - Deploy, manage orchestrator
- **Not local-first** - Network dependency

**Why not chosen**: Great for future scaling but too heavy for MVP. Can migrate post-MVP.

### Alternative 6: Embedded Job Queue Libraries

**Description**: Use existing SQLite-based queue libraries.

**Pros**:
- Proven implementation
- Less code to write
- Community support

**Cons**:
- External dependency
- May not fit exact needs
- Learning curve for team
- Less control

**Why not chosen**: Considered but decided custom implementation is simple enough and gives more control. Can re-evaluate if complexity grows.

## Implementation

### Job Queue Schema

```typescript
CREATE TABLE job_queue (
  id TEXT PRIMARY KEY,
  type TEXT NOT NULL, -- 'sanitize_conversation', 'extract_learning', 'mine_upload'
  status TEXT NOT NULL, -- 'queued', 'in_progress', 'completed', 'failed', 'dead_letter'
  priority INTEGER NOT NULL DEFAULT 5, -- 1 (highest) to 10 (lowest)
  payload TEXT NOT NULL, -- JSON
  idempotency_key TEXT UNIQUE, -- For deduplication
  created_at INTEGER NOT NULL,
  scheduled_at INTEGER NOT NULL, -- When to execute (for delays)
  started_at INTEGER, -- When worker picked up
  completed_at INTEGER,
  lease_expires_at INTEGER, -- Worker lease expiry
  retry_count INTEGER DEFAULT 0,
  max_retries INTEGER DEFAULT 3,
  error TEXT, -- Last error message
  result TEXT -- JSON result
);

CREATE INDEX idx_jobs_status_priority ON job_queue(status, priority, created_at);
CREATE INDEX idx_jobs_scheduled ON job_queue(scheduled_at) WHERE status = 'queued';
CREATE INDEX idx_jobs_lease_expiry ON job_queue(lease_expires_at) WHERE status = 'in_progress';
CREATE INDEX idx_jobs_idempotency ON job_queue(idempotency_key);
```

### Outbox Pattern (Atomic Enqueue)

```typescript
// Enqueue job atomically with conversation insert
async function captureAndEnqueue(conversation: Conversation) {
  db.transaction(() => {
    // 1. Insert conversation
    db.prepare(`
      INSERT INTO conversations (id, created_at, status)
      VALUES (?, ?, ?)
    `).run(conversation.id, Date.now(), "pending_sanitization");

    // 2. Enqueue sanitization job (same transaction)
    db.prepare(`
      INSERT INTO job_queue (id, type, status, priority, payload, idempotency_key, created_at, scheduled_at)
      VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    `).run(
      generateId(),
      "sanitize_conversation",
      "queued",
      1, // High priority
      JSON.stringify({ conversationId: conversation.id }),
      `sanitize-${conversation.id}`, // Idempotency key
      Date.now(),
      Date.now() // Execute immediately
    );
  })();
}
```

### Job States and Transitions

```typescript
enum JobStatus {
  QUEUED = "queued",           // Waiting to execute
  IN_PROGRESS = "in_progress", // Worker processing
  COMPLETED = "completed",     // Successfully finished
  FAILED = "failed",           // Failed but can retry
  DEAD_LETTER = "dead_letter"  // Failed max retries
}

// State transitions
const validTransitions = {
  [JobStatus.QUEUED]: [JobStatus.IN_PROGRESS],
  [JobStatus.IN_PROGRESS]: [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.QUEUED], // Queued = lease expired
  [JobStatus.FAILED]: [JobStatus.QUEUED, JobStatus.DEAD_LETTER],
  [JobStatus.COMPLETED]: [], // Terminal
  [JobStatus.DEAD_LETTER]: [] // Terminal
};
```

### Worker Implementation

```typescript
class JobWorker {
  private running = false;
  private pollInterval = 1000; // 1s
  private leaseDuration = 60000; // 60s

  async start() {
    this.running = true;
    console.log("Worker started");

    while (this.running) {
      try {
        const job = await this.claimJob();

        if (job) {
          await this.executeJob(job);
        } else {
          // No jobs, wait before polling again
          await sleep(this.pollInterval);
        }
      } catch (error) {
        console.error("Worker error:", error);
        await sleep(this.pollInterval);
      }
    }

    console.log("Worker stopped");
  }

  async stop() {
    this.running = false;
    // Graceful shutdown: finish current job
  }

  private async claimJob(): Promise<Job | null> {
    const now = Date.now();

    return db.transaction(() => {
      // Find next job to process
      const job = db.prepare(`
        SELECT * FROM job_queue
        WHERE status = 'queued'
          AND scheduled_at <= ?
        ORDER BY priority ASC, created_at ASC
        LIMIT 1
      `).get(now);

      if (!job) {
        // Also check for expired leases
        const expiredJob = db.prepare(`
          SELECT * FROM job_queue
          WHERE status = 'in_progress'
            AND lease_expires_at < ?
          ORDER BY priority ASC, created_at ASC
          LIMIT 1
        `).get(now);

        if (expiredJob) {
          // Reset to queued for retry
          db.prepare(`
            UPDATE job_queue
            SET status = 'queued',
                lease_expires_at = NULL,
                started_at = NULL
            WHERE id = ?
          `).run(expiredJob.id);

          return expiredJob;
        }

        return null;
      }

      // Claim job with lease
      db.prepare(`
        UPDATE job_queue
        SET status = 'in_progress',
            started_at = ?,
            lease_expires_at = ?
        WHERE id = ?
      `).run(now, now + this.leaseDuration, job.id);

      return job;
    })();
  }

  private async executeJob(job: Job) {
    try {
      console.log(`Executing job ${job.id} (${job.type})`);

      // Get handler for job type
      const handler = this.getHandler(job.type);

      // Execute with idempotency
      const result = await handler(JSON.parse(job.payload));

      // Mark completed
      db.prepare(`
        UPDATE job_queue
        SET status = 'completed',
            completed_at = ?,
            result = ?
        WHERE id = ?
      `).run(Date.now(), JSON.stringify(result), job.id);

      console.log(`✓ Job ${job.id} completed`);
    } catch (error) {
      console.error(`✗ Job ${job.id} failed:`, error);

      // Retry logic
      await this.handleFailure(job, error);
    }
  }

  private async handleFailure(job: Job, error: Error) {
    const retryCount = job.retry_count + 1;

    if (retryCount >= job.max_retries) {
      // Move to dead letter queue
      db.prepare(`
        UPDATE job_queue
        SET status = 'dead_letter',
            retry_count = ?,
            error = ?
        WHERE id = ?
      `).run(retryCount, error.message, job.id);

      // Alert
      await alert({
        severity: "ERROR",
        message: `Job ${job.id} moved to dead letter queue`,
        error: error.message
      });
    } else {
      // Retry with exponential backoff
      const backoff = this.calculateBackoff(retryCount);

      db.prepare(`
        UPDATE job_queue
        SET status = 'queued',
            retry_count = ?,
            scheduled_at = ?,
            error = ?,
            lease_expires_at = NULL,
            started_at = NULL
        WHERE id = ?
      `).run(retryCount, Date.now() + backoff, error.message, job.id);

      console.log(`Retrying job ${job.id} in ${backoff}ms (attempt ${retryCount})`);
    }
  }

  private calculateBackoff(retryCount: number): number {
    // Exponential backoff with jitter
    const baseDelay = 1000; // 1s
    const maxDelay = 60000; // 60s
    const exponential = Math.min(baseDelay * Math.pow(2, retryCount), maxDelay);
    const jitter = Math.random() * 1000; // 0-1s jitter

    return exponential + jitter;
  }

  private getHandler(jobType: string): JobHandler {
    const handlers = {
      sanitize_conversation: sanitizeConversationHandler,
      extract_learning: extractLearningHandler,
      mine_upload: mineUploadHandler
    };

    return handlers[jobType] || (() => {
      throw new Error(`Unknown job type: ${jobType}`);
    });
  }
}
```

### Idempotency

```typescript
// Every job handler must be idempotent
async function sanitizeConversationHandler(payload: { conversationId: string }) {
  const { conversationId } = payload;

  // Check if already sanitized
  const conversation = db.prepare(
    "SELECT * FROM conversations WHERE id = ?"
  ).get(conversationId);

  if (conversation.status === "sanitized") {
    console.log(`Conversation ${conversationId} already sanitized`);
    return { skipped: true, reason: "already-sanitized" };
  }

  // Perform sanitization...
  const sanitized = await sanitize(conversation.content);

  // Update atomically
  db.transaction(() => {
    db.prepare(`
      UPDATE conversations
      SET sanitized_content = ?,
          status = 'sanitized',
          updated_at = ?
      WHERE id = ?
    `).run(sanitized, Date.now(), conversationId);
  })();

  return { success: true };
}
```

### Graceful Shutdown

```typescript
process.on("SIGTERM", async () => {
  console.log("SIGTERM received, shutting down gracefully");

  // Stop accepting new jobs
  worker.stop();

  // Wait for current job to finish (with timeout)
  await Promise.race([
    worker.waitForCompletion(),
    sleep(30000) // 30s max wait
  ]);

  // Close database
  db.close();

  process.exit(0);
});
```

### Metrics and Monitoring

```typescript
interface QueueMetrics {
  queueDepth: number; // Jobs waiting
  queueAge: number; // Oldest queued job age (ms)
  inProgressCount: number;
  p95Latency: number; // 95th percentile job duration
  successRate: number; // Completed / Total
  deadLetterCount: number;
}

async function getQueueMetrics(): Promise<QueueMetrics> {
  const stats = db.prepare(`
    SELECT
      status,
      COUNT(*) as count,
      MIN(created_at) as oldest,
      AVG(completed_at - started_at) as avg_duration,
      MAX(completed_at - started_at) as max_duration
    FROM job_queue
    WHERE created_at > ? -- Last 24 hours
    GROUP BY status
  `).all(Date.now() - 86400000);

  // Calculate metrics...
  return metrics;
}

// Expose metrics for monitoring
setInterval(async () => {
  const metrics = await getQueueMetrics();

  if (metrics.queueDepth > 100) {
    await alert({
      severity: "WARNING",
      message: `Queue depth high: ${metrics.queueDepth}`
    });
  }

  if (metrics.queueAge > 3600000) { // 1 hour
    await alert({
      severity: "WARNING",
      message: `Old jobs in queue: ${metrics.queueAge}ms`
    });
  }
}, 60000); // Check every minute
```

### Backpressure Handling

```typescript
async function enqueueWithBackpressure(job: Job) {
  const queueDepth = db.prepare(
    "SELECT COUNT(*) as count FROM job_queue WHERE status = 'queued'"
  ).get().count;

  const maxQueueSize = 10000;

  if (queueDepth >= maxQueueSize) {
    // Queue full - implement backpressure
    if (job.priority > 5) {
      // Drop low-priority jobs
      console.warn(`Queue full, dropping low-priority job ${job.type}`);
      return { dropped: true };
    } else {
      // Block and wait for high-priority jobs
      await waitForQueueSpace(maxQueueSize);
    }
  }

  // Enqueue job
  db.prepare(`
    INSERT INTO job_queue (...)
    VALUES (...)
  `).run(...);

  return { enqueued: true };
}
```

## Risks and Mitigations

### Risk: Jobs Lost on Crash

**Impact**: Medium - Work needs to be redone

**Mitigation**:
- Persistent queue (survives crashes)
- Lease-based execution (requeue if worker dies)
- Idempotent handlers (safe to retry)
- Monitor dead letter queue

### Risk: Queue Buildup

**Impact**: Medium - Delayed processing

**Mitigation**:
- Monitor queue depth
- Alert on high queue age
- Backpressure mechanism
- Can add more workers post-MVP

### Risk: Duplicate Processing

**Impact**: Low - Wasted resources

**Mitigation**:
- Idempotency keys prevent duplicates
- Idempotent job handlers
- Check state before processing

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Async Processing Layer](../architecture/architecture-async-processing-2025-01-16.md)

### Decisions
- [ADR-001: Use Claude Hooks](./decision-use-claude-hooks-2025-01-16.md)
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md)
- [ADR-005: Use SQLite](./decision-use-sqlite-2025-01-16.md)

### Plans
- [Phase 4: Async Processing](../plans/plan-phase-4-async-processing-2025-01-16.md)

### Reference
- [Job Types Reference](../reference/reference-job-types-2025-01-16.md)
````

## File: decisions/decision-claude-testing-harness-2025-01-16.md
````markdown
---
title: ADR-003: Claude-Powered Testing Harness
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [testing, quality, claude-agent-sdk, tdd]
---

# ADR-003: Claude-Powered Testing Harness

## Status

Accepted

Date: 2025-01-16

## Context

The Global Context Network has critical quality requirements:

1. **Zero PII leaks** - Privacy is non-negotiable, must be proven
2. **Correctness of learnings** - Extracted insights must be accurate
3. **Reliability** - Async processing must be robust with retries
4. **Performance** - Hooks < 100ms, queries < 200ms
5. **Security** - No injection vulnerabilities or exposed secrets

Standard unit tests underrepresent these risks because:
- **Edge cases** - Hard to manually enumerate all PII patterns
- **Context-aware** - Need to test sanitization in realistic scenarios
- **End-to-end validation** - Component tests miss integration issues
- **Quality variance** - Manual test quality varies across developers
- **Coverage gaps** - Easy to miss critical paths

We need validation beyond code coverage - we need **risk coverage** including:
- Privacy violations (PII leakage)
- Security vulnerabilities
- Performance regressions
- Correctness of learned facts
- Async failure scenarios

## Decision

Use Claude Agent SDK for test generation, validation, and quality gates.

**Components**:
1. **Test Generator Subagents** - Generate comprehensive test suites
2. **Test Quality Validator** - Score test quality and completeness
3. **Coverage Analyzer** - Ensure > 85% code coverage
4. **Privacy Red Team** - Generate adversarial PII test cases
5. **Implementation Validator** - Verify code matches tests

**TDD Workflow**:
```
RED: Test generator creates failing test
  ↓
VALIDATE: Test quality validator scores test
  ↓
GREEN: Implementation subagent passes test
  ↓
VERIFY: Implementation validator checks correctness
  ↓
REFACTOR: Code quality validator ensures standards
```

**Quality Gates** (all must pass):
- Test coverage > 85%
- All tests pass
- PII leakage score = 0
- Security scan clean
- Performance SLOs met

## Consequences

### Positive

- **Comprehensive coverage** - AI finds edge cases humans miss
- **Privacy assurance** - Red team generates PII test cases
- **Automated quality scoring** - Objective test quality metrics
- **Edge case discovery** - AI explores boundary conditions
- **Self-validating system** - Tests validate themselves
- **Consistent quality** - Same standards across all components
- **Fast iteration** - Automated test generation accelerates TDD

### Negative

- **API costs** - Claude API usage for test generation
- **Non-determinism** - Generated tests may vary between runs
- **Prompt dependency** - Test quality depends on prompt engineering
- **Maintenance** - Must maintain test generator prompts
- **Learning curve** - Team must understand AI-powered testing
- **Debugging** - Harder to debug generated test failures

### Neutral

- **Requires Claude API** - Adds external dependency
- **Token budget needed** - Must allocate costs for CI
- **Test data governance** - Ensure generated tests don't contain PII
- **Flaky test handling** - Need quarantine for non-deterministic tests

## Alternatives Considered

### Alternative 1: Manual Test Writing Only

**Description**: Developers write all tests manually following TDD.

**Pros**:
- Full control over test logic
- Deterministic results
- No API dependency
- Standard industry practice
- Easier debugging

**Cons**:
- Slow (bottleneck on human creativity)
- Inconsistent quality across developers
- Misses edge cases
- No automated quality validation
- High maintenance burden

**Why not chosen**: Too slow for aggressive MVP timeline, lacks comprehensive edge case coverage.

### Alternative 2: Property-Based Testing (Hypothesis/FastCheck)

**Description**: Use property-based testing frameworks for structured components.

**Pros**:
- Excellent for finding edge cases
- Deterministic with seeds
- No API costs
- Good for pure functions
- Shrinking finds minimal failing examples

**Cons**:
- Requires defining properties (still manual)
- Not suitable for all components
- Doesn't validate test quality
- No privacy-specific red team
- Learning curve for team

**Why not chosen**: Complementary approach, should be used alongside Claude harness. Will incorporate for structured components.

### Alternative 3: Mutation Testing (Stryker, PIT)

**Description**: Measure test suite sensitivity by mutating code.

**Pros**:
- Validates test effectiveness
- Finds weak tests
- Deterministic results
- No external dependency

**Cons**:
- Slow (must re-run tests for each mutation)
- Doesn't generate tests
- Doesn't understand domain (privacy, security)
- High computational cost

**Why not chosen**: Useful for validation but doesn't generate tests. Can add as quality check later.

### Alternative 4: Golden File / Record-Replay Testing

**Description**: Record inputs/outputs, replay for regression testing.

**Pros**:
- Deterministic
- Catches regressions
- Fast execution
- Easy to maintain

**Cons**:
- Doesn't generate new tests
- Brittle to intentional changes
- No edge case discovery
- Not suitable for privacy testing

**Why not chosen**: Useful for integration tests but doesn't solve test generation problem.

### Alternative 5: Static Analysis and Linters Only

**Description**: Rely on ESLint, TypeScript, security scanners.

**Pros**:
- Fast feedback
- Deterministic
- No API costs
- Catches common issues
- Standard tooling

**Cons**:
- No runtime behavior testing
- Can't verify correctness
- Misses logic bugs
- No PII detection testing
- Limited to structural issues

**Why not chosen**: Necessary but insufficient. Will use alongside testing harness.

## Implementation

### Test Generator Configuration

```typescript
interface TestGeneratorConfig {
  component: string;
  testType: "unit" | "integration" | "e2e";
  coverageTarget: number; // 0.0 - 1.0
  edgeCaseFocus: string[]; // ["pii", "performance", "errors"]
  model: "sonnet" | "opus";
  temperature: number;
}

const unitTestGenerator = {
  component: "sanitization-pipeline",
  testType: "unit",
  coverageTarget: 0.95, // Critical component
  edgeCaseFocus: ["pii", "errors", "boundaries"],
  model: "sonnet",
  temperature: 0.3
};
```

### Privacy Red Team Generator

Generate adversarial PII test cases:

```typescript
const piiRedTeamPrompt = `Generate PII edge cases for sanitization testing.

Include:
- Obfuscated emails (user[at]domain[dot]com)
- International phone formats
- API keys with unusual formats
- Names in code (variable names vs person names)
- Paths with usernames in unexpected places
- Combined PII (email + phone in same string)
- Unicode/emoji in PII
- Base64-encoded secrets
- Uncommon PII patterns

Generate 50 unique test cases.
Each must be realistic and challenging.`;
```

### Test Quality Scoring

```typescript
interface TestQualityScore {
  structure: number; // 0-1: proper describe/it/expect
  clarity: number; // 0-1: clear test names
  assertions: number; // 0-1: meaningful assertions
  edgeCases: number; // 0-1: boundary conditions covered
  isolation: number; // 0-1: proper mocking/independence
  maintainability: number; // 0-1: clear and DRY
  overall: number; // weighted average
}

// Require overall > 0.8 to pass
```

### Determinism for CI

```typescript
// For CI runs: low temperature, fixed seed
const ciTestConfig = {
  temperature: 0.1,
  seed: "ci-run-20250116",
  maxRetries: 1, // Don't retry in CI
  timeout: 300000 // 5min max
};

// For exploration: higher temperature
const exploreConfig = {
  temperature: 0.7,
  seed: null, // Random
  maxRetries: 3
};
```

### Cost Budgets

```typescript
interface TestingBudget {
  maxTokensPerComponent: number;
  maxTokensPerCIRun: number;
  smokeTestTokens: number; // Quick validation
  fullTestTokens: number; // Nightly comprehensive
}

const budget = {
  maxTokensPerComponent: 20000,
  maxTokensPerCIRun: 100000, // Cap CI costs
  smokeTestTokens: 10000, // PR validation
  fullTestTokens: 500000 // Nightly deep testing
};
```

### Test Data Governance

Ensure generated tests don't contain real PII:

```typescript
async function validateGeneratedTests(tests: string[]): Promise<boolean> {
  // Scan generated test code for PII
  const detector = new PIIDetector();

  for (const test of tests) {
    const findings = await detector.scan(test);
    if (findings.length > 0) {
      throw new Error(`Generated test contains PII: ${findings}`);
    }
  }

  return true;
}
```

### Smoke vs Exhaustive Suites

```typescript
// Smoke tests: Fast, run on every PR
const smokeTests = {
  coverage: 0.70, // Lower bar
  timeout: 60000, // 1min
  tokenBudget: 10000,
  tests: ["happy-path", "basic-errors"]
};

// Exhaustive tests: Comprehensive, run nightly
const exhaustiveTests = {
  coverage: 0.95, // High bar
  timeout: 600000, // 10min
  tokenBudget: 500000,
  tests: ["all-edge-cases", "red-team", "property-based"]
};
```

### Flaky Test Quarantine

```typescript
interface FlakyTestHandler {
  maxFailureRate: number; // 0.05 = 5%
  minRuns: number; // Need 20 runs to determine flakiness
  quarantineDuration: number; // 7 days

  async handleFlaky(test: Test): Promise<void> {
    // Move to quarantine
    await moveToQuarantine(test);

    // Create issue for investigation
    await createIssue({
      title: `Flaky test: ${test.name}`,
      labels: ["flaky-test", "needs-investigation"]
    });

    // Notify team
    await notify(`Test ${test.name} quarantined due to flakiness`);
  }
}
```

### Integration with Property-Based Testing

```typescript
// Combine Claude generation with property-based testing
const hybridTests = {
  // Claude generates properties to test
  generateProperties: async (component: string) => {
    const properties = await claudeGenerateProperties(component);
    return properties;
  },

  // FastCheck validates properties
  validateWithFastCheck: (properties: Property[]) => {
    for (const prop of properties) {
      fc.assert(fc.property(prop.generators, prop.predicate));
    }
  }
};
```

### Coverage Validation

```typescript
interface CoverageRequirements {
  lines: number; // 0.85
  statements: number; // 0.85
  functions: number; // 0.90
  branches: number; // 0.80

  // Special requirements for critical paths
  criticalPaths: {
    [path: string]: number; // 1.0 for sanitization
  };
}

const requirements = {
  lines: 0.85,
  statements: 0.85,
  functions: 0.90,
  branches: 0.80,
  criticalPaths: {
    "src/sanitization/": 0.95, // High bar for privacy
    "src/hooks/": 0.90 // High bar for performance
  }
};
```

## Risks and Mitigations

### Risk: Non-Determinism Breaks CI

**Impact**: High - Unreliable builds

**Mitigation**:
- Low temperature (0.1) for CI
- Fixed seeds for reproducibility
- Retry same seed on failure
- Quarantine flaky tests
- Smoke tests with deterministic config

### Risk: API Cost Overruns

**Impact**: Medium - Budget concerns

**Mitigation**:
- Hard token limits per component
- Use Haiku for simple tests
- Cache test generation results
- Progressive test generation (smoke → full)
- Alert at 80% of budget

### Risk: Generated Tests Contain PII

**Impact**: High - Privacy violation

**Mitigation**:
- Scan generated tests for PII
- Use synthetic data only
- Review generated tests before commit
- Automated PII detection on test code

### Risk: Test Quality Variance

**Impact**: Medium - Unreliable validation

**Mitigation**:
- Quality scoring (require > 0.8)
- Human review for critical components
- Version and track prompts
- Continuous prompt improvement

## Related Documents

### Architecture
- [Testing Harness Architecture](../architecture/architecture-testing-harness-2025-01-16.md)
- [Subagent System](../architecture/architecture-subagent-system-2025-01-16.md)

### Decisions
- [ADR-002: Subagent-Driven Development](./decision-subagent-driven-development-2025-01-16.md)
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md)

### Guides
- [TDD Workflow Guide](../guides/guide-tdd-workflow-2025-01-16.md)
- [Testing Harness Usage](../guides/guide-testing-harness-usage-2025-01-16.md)

### Reference
- [Testing Strategy Reference](../reference/reference-testing-strategy-2025-01-16.md)
````

## File: decisions/decision-data-licensing-consent-2025-01-16.md
````markdown
---
title: ADR-007: Data Licensing and Consent Model (Local-First, Opt-In Global Sharing)
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [privacy, legal, licensing, consent, data-sharing, ethics]
---

# ADR-007: Data Licensing and Consent Model

## Status

Accepted

Date: 2025-01-16

## Context

The Global Context Network extracts **learnings** from user conversations with AI agents and has the capability to:

1. **Store locally** - Sanitized conversations and learnings in SQLite
2. **Share globally** - Upload learnings to IPFS/blockchain for public access
3. **Enable queries** - Allow other AI agents to query learnings via MCP

This raises critical legal, ethical, and trust questions:

### Legal Questions

- **Ownership**: Who owns the intellectual property of an extracted learning?
- **Licensing**: Under what license are shared learnings published?
- **Liability**: Who is responsible if learnings contain inaccurate or harmful information?
- **Revocation**: Can users delete learnings after global publication?
- **Attribution**: Should contributions be attributed or anonymous?

### Ethical Questions

- **Consent**: Do users understand what they're sharing and with whom?
- **Control**: Can users opt-out or selectively share?
- **Privacy**: Even with sanitization, are learnings truly de-identified?
- **Commercialization**: Should users be compensated for valuable learnings?
- **Derivatives**: Can third parties build commercial products on shared learnings?

### Trust Requirements

- **Transparency**: Users must understand exactly what is shared
- **Agency**: Users must have meaningful control
- **Reversibility**: Users must be able to change their mind
- **Safety**: Default behavior must be the safest option
- **Alignment**: System behavior must match user expectations

### External Review Findings

Both GPT-5 and Gemini 2.5 Pro reviews identified this as a **CRITICAL BLOCKER**:

**GPT-5 Review** (Critical Issue #7):
> "Security/consent/licensing gate before upload - Plans and ADRs note the need for consent/licensing ADR but it's not finalized. Upload gating/opt-in is critical before any global distribution."

**Gemini 2.5 Pro Review** (Critical Blocker #2):
> "Unaddressed Data Licensing and Ownership: The plan involves uploading user-generated learnings to a global, public IPFS network. There is **zero documentation** on the legal framework for this:
> - What license are the learnings shared under (e.g., MIT, CC0, proprietary)?
> - Who owns the intellectual property of an extracted learning? The user, the system, the network?
> - How is user consent for global, irrevocable publication obtained and managed?"

### Regulatory Context

- **GDPR** (EU): Requires explicit consent for data processing and sharing
- **CCPA** (California): Grants users right to know, delete, and opt-out
- **PIPEDA** (Canada): Requires meaningful consent and purpose limitation
- **LGPD** (Brazil): Brazilian data protection law similar to GDPR
- **UK GDPR**: Post-Brexit UK data protection requirements
- **COPPA** (US): Children's Online Privacy Protection Act - under-13 restrictions
- **Privacy-by-Design**: Default to most privacy-preserving option

**CRITICAL**: Under GDPR Recital 26, only **truly anonymized** data (no reasonably likely re-identification by anyone) can be published immutably. If learnings contain any personal data, GDPR erasure rights conflict with IPFS/blockchain permanence. "Anonymized" means **no reasonably likely re-identification**, not just pseudonymization.

**Controller/Processor Roles**: We must define who acts as data controller vs processor at each stage (local processing, consent collection, publishing, IPFS pinning). If we operate the software and pinning nodes, we likely act as controller for consent logs and host/intermediary for published datasets.

### Prior Art

Similar projects and their approaches:

1. **Wikipedia** - CC BY-SA 4.0, all contributions licensed, attribution optional
2. **Stack Overflow** - CC BY-SA 4.0, user contributions, reputation system
3. **OpenStreetMap** - ODbL (Open Database License), share-alike requirement
4. **Creative Commons** - Various licenses (CC0, BY, BY-SA, BY-NC)
5. **GitHub** - Code under repo license, issues/PRs under CC0
6. **Hugging Face Datasets** - Various licenses, user-specified per dataset

## Decision

Implement a **Local-First, Explicit Opt-In, Open License** model with manual approval gates for the MVP.

### Core Principles

1. **Default: Local-Only** - System operates locally by default, no global uploads
2. **Explicit Opt-In** - Users must actively enable global sharing
3. **Manual Approval** - Each upload requires explicit user confirmation (MVP)
4. **Learning-Only Sharing** - NEVER share raw conversations, only derived learnings
5. **True Anonymization** - Only publish genuinely anonymized data (no reasonably likely re-identification by anyone)
6. **Open License** - Shared learnings published under permissive open license (CC BY 4.0)
7. **Informed Consent** - Clear disclosure of what sharing means including IPFS permanence
8. **Age Gating** - Block global sharing for minors (under-16 EU, under-13 US)
9. **Contribution Agreement** - Users warrant they have rights to share, no confidential/PII/trade secrets
10. **Revocation Support** - Users can delete/revoke at any time (best-effort for published)
11. **Data Minimization** - Minimal consent logging, strict retention limits (12-24 months)
12. **Database Rights** - Dual-license: CC BY 4.0 per-learning, CC0/ODC-By for aggregated dataset
13. **Abuse Protection** - Fast-track takedown process, kill-switch, DMCA agent
14. **Cross-Jurisdiction** - GDPR, CCPA, LGPD, UK GDPR compliance with international transfer mechanisms

### Default Behavior

```
┌─────────────────────────────────────┐
│     USER INSTALLS SYSTEM            │
│                                     │
│  Default Mode: LOCAL-ONLY           │
│  ✓ Conversations captured           │
│  ✓ Learnings extracted              │
│  ✓ MCP queries work (local data)    │
│  ✗ NO global uploads                │
│  ✗ NO blockchain transactions       │
│  ✗ NO IPFS publishing               │
└─────────────────────────────────────┘
```

**Rationale**: Safest default. Users can explore value without legal/privacy concerns.

### Opt-In Flow (MVP)

```
Step 1: User Initiates
  → User runs: gcn enable-sharing

Step 2: Age Verification
  → System verifies user age:

  ┌───────────────────────────────────────────────────┐
  │ AGE VERIFICATION                                  │
  │                                                   │
  │ To enable global sharing, you must confirm:       │
  │                                                   │
  │ [ ] I am at least 16 years old (EU) or 13 (US)   │
  │                                                   │
  │ Global sharing is restricted for minors to comply │
  │ with GDPR and COPPA regulations.                  │
  │                                                   │
  │ Local-only mode is available for all ages.        │
  └───────────────────────────────────────────────────┘

Step 3: Data Contribution Agreement
  → User accepts contribution terms:

  ┌───────────────────────────────────────────────────┐
  │ DATA CONTRIBUTION AGREEMENT                       │
  │                                                   │
  │ By enabling sharing, you warrant that:            │
  │                                                   │
  │ [ ] I have the right to share these learnings     │
  │ [ ] Learnings contain NO personal information     │
  │ [ ] Learnings contain NO confidential/trade secrets│
  │ [ ] Learnings contain NO employer/NDA material    │
  │ [ ] I will NOT share customer/proprietary data    │
  │ [ ] I grant CC BY 4.0 license for shared content  │
  │ [ ] I waive moral rights and database rights      │
  │ [ ] I understand sharing may affect patentability │
  │ [ ] No medical, export-controlled, illegal content│
  │ [ ] No defamatory or harmful content              │
  │                                                   │
  │ You agree to indemnify the network for any        │
  │ violations of these warranties.                   │
  │                                                   │
  │ License granted to the extent any rights exist,   │
  │ including AI/human generated content ambiguity.   │
  └───────────────────────────────────────────────────┘

Step 4: Informed Consent
  → System displays consent dialog:

  ┌───────────────────────────────────────────────────┐
  │ ENABLE GLOBAL LEARNING SHARING                    │
  │                                                   │
  │ What this means:                                  │
  │ • Your anonymized learnings will be shared publicly│
  │ • Published to IPFS (content-addressed storage)   │
  │ • Blockchain record created (public, permanent)   │
  │ • Other AI agents can query your learnings        │
  │ • License: CC BY 4.0 (attribution required)       │
  │                                                   │
  │ What is NOT shared:                               │
  │ • Raw conversations (never shared)                │
  │ • Personal information (anonymized before publish)│
  │ • Your identity (anonymous by default)            │
  │ • Non-trivial code snippets (blocked by policy)   │
  │ • Identifiable incidents or specific individuals  │
  │ • Sensitive topics (health, race, religion, etc.) │
  │                                                   │
  │ Your rights:                                      │
  │ • Revoke at any time (stops new uploads)          │
  │ • Delete learnings (removes from local + network) │
  │ • View pending uploads before approval            │
  │ • Data access, correction, restriction, portability│
  │ • Lodge complaint with supervisory authority      │
  │ • Withdraw consent (applies to local data/logs)   │
  │                                                   │
  │ CRITICAL: IPFS data is hard to delete. Once       │
  │ published, copies may persist on other nodes.     │
  │ Only truly anonymized learnings will be published.│
  │ Erasure rights apply to local data and consent    │
  │ records, not anonymized public learnings.         │
  │                                                   │
  │ [ ] I have read and accept the Privacy Policy     │
  │ [ ] I have read and accept the Terms of Service   │
  │ [ ] I have read the Data Contribution Agreement   │
  │ [ ] I understand IPFS immutability risks          │
  │ [ ] I confirm all learnings will be anonymized    │
  │                                                   │
  │ Required documents:                               │
  │ • Privacy Policy: ./docs/legal/PRIVACY.md         │
  │ • Terms of Service: ./docs/legal/TERMS.md         │
  │ • Data Contribution Agreement: ./docs/legal/CONTRIBUTION.md│
  │ • Abuse/Takedown Policy: ./docs/legal/ABUSE.md    │
  │ • CC BY 4.0 License: [link]                       │
  │ • Anonymization Standard: ./docs/standards/ANONYMIZATION.md│
  └───────────────────────────────────────────────────┘

Step 5: Manual Approval (MVP)
  → For each learning ready to upload:

  ┌───────────────────────────────────────────────────┐
  │ APPROVE UPLOAD?                                   │
  │                                                   │
  │ Learning Preview:                                 │
  │ "When debugging TypeScript decorators, enable     │
  │  experimentalDecorators in tsconfig.json and      │
  │  check the emitted JavaScript to verify..."       │
  │                                                   │
  │ Metadata:                                         │
  │ • Size: 1.2 KB                                    │
  │ • Tags: typescript, debugging, decorators         │
  │ • Quality Score: 8.7/10                           │
  │ • PII Check: PASSED (multi-model scan)            │
  │ • Code Check: PASSED (no non-trivial code)        │
  │ • Secret Scan: PASSED (no API keys/credentials)   │
  │ • Re-identification Risk: LOW (k-anonymity ≥10)   │
  │                                                   │
  │ License: CC BY 4.0                                │
  │ Attribution: Anonymous                            │
  │                                                   │
  │ [Approve] [Reject] [View Full Learning]           │
  └───────────────────────────────────────────────────┘

Step 6: Upload & Record
  → Upload to IPFS
  → Record blockchain transaction
  → Update local upload_status table
  → Show confirmation with IPFS CID
```

### License: Creative Commons Attribution 4.0 (CC BY 4.0)

**Chosen License**: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)

**What it allows**:
- ✓ Share — Copy and redistribute in any format
- ✓ Adapt — Remix, transform, build upon
- ✓ Commercial use — Use for any purpose, including commercial

**Requirements**:
- ✓ Attribution — Must give appropriate credit
- ✓ Indicate changes — If remixed, must indicate changes
- ✓ No additional restrictions — Cannot add legal/technical restrictions

**Rationale for CC BY 4.0**:
1. **Permissive** - Enables maximum reuse and innovation
2. **Attribution** - Credit to contributors (if opted-in)
3. **Standard** - Widely recognized, legally tested
4. **Commercial-friendly** - Allows commercial AI training/products
5. **Compatible** - Works with most other open licenses
6. **Irrevocable** - Once licensed, can't be un-licensed (legal clarity)
7. **Moral rights waiver** - Includes waiver to extent possible
8. **AI-friendly** - License applies regardless of AI/human authorship ambiguity

**Dual Licensing for Database Rights**:

To address EU sui generis database rights, we employ dual licensing:

- **Per-Learning Content**: CC BY 4.0 (attribution required)
- **Aggregated Dataset/Metadata**: CC0 or ODC-By (public domain or attribution)

This reduces friction for bulk use cases while ensuring individual learnings receive proper attribution.

**Attribution for Anonymous Contributions**:

When attribution preference is "anonymous", we provide this standard attribution string for downstream users:

```
"Global Context Network contributor (anonymous), Learning CID: Qm..., CC BY 4.0"
```

This satisfies CC BY attribution requirements while preserving user anonymity.

### Attribution Options

Users choose attribution preference:

```typescript
enum AttributionPreference {
  ANONYMOUS = "anonymous",           // No attribution (default)
  PSEUDONYMOUS = "pseudonymous",     // Attribute to public key/handle
  ATTRIBUTED = "attributed"          // Attribute to real identity
}
```

**Default: Anonymous**
- Most privacy-preserving
- No identity linkage
- Still contributes to network

**Pseudonymous**:
- Attribute to Ethereum address or handle
- Build reputation without real identity
- Enable token rewards tracking
- **Note**: Blockchain addresses may be personal data if linkable to identity
- Requires explicit consent and disclosure of traceability risk

**Attributed**:
- Real name or GitHub profile
- Build public portfolio
- Maximum recognition

### What Is Shared

**Allowed (Learnings Only)**:
```json
{
  "id": "learning-xyz789",
  "content": "When implementing React hooks, useEffect with empty dependency array runs once on mount...",
  "tags": ["react", "hooks", "useEffect"],
  "context": "debugging-infinite-render-loop",
  "quality_score": 8.5,
  "language": "typescript",
  "framework": "react",
  "created_at": "2025-01-16T10:30:00Z",
  "license": "CC-BY-4.0",
  "attribution": "anonymous"
}
```

**Prohibited (Never Shared)**:
- ❌ Raw conversation transcripts
- ❌ User prompts (even sanitized)
- ❌ Assistant responses (even sanitized)
- ❌ Tool calls and results
- ❌ File contents
- ❌ Thinking/reasoning traces
- ❌ Session metadata (timestamps, durations, etc.)
- ❌ Personal information (even sanitized placeholders)
- ❌ Database schemas, API keys, internal details
- ❌ Non-trivial code snippets (>10 lines or copyrightable)
- ❌ Identifiable individuals, companies, or specific incidents
- ❌ Rare job titles or highly specific circumstances (re-identification risk)
- ❌ Special categories: health, race, religion, politics (if person-linked)
- ❌ Medical advice, export-controlled, or illegal content
- ❌ Defamatory, harmful, or regulated content
- ❌ Customer data, trade secrets, NDA material, confidential information

**Rationale**: Only share the distilled, abstracted insight. Never raw data or content with re-identification risk.

### Revocation and Deletion

Users can revoke consent and delete learnings:

```bash
# Stop new uploads
gcn disable-sharing

# Delete specific learning (local + network revocation)
gcn delete-learning <learning-id>

# Delete all learnings
gcn delete-all-learnings --confirm

# Publish revocation to blockchain
gcn revoke-upload <ipfs-cid>
```

**Revocation Behavior**:

1. **Local Deletion** - Immediate, permanent
2. **Upload Queue** - Cancelled immediately
3. **Blockchain Revocation** - Publishes revocation event
4. **IPFS Unpinning** - Unpin from our nodes
5. **Best-Effort Network Removal** - Request other nodes unpin

**Limitations** (disclosed to user):
- IPFS is a distributed system; copies may persist on other nodes
- Blockchain records are permanent (revocation event added, not removed)
- Third parties may have cached or indexed learnings
- Revocation is "best-effort", not guaranteed deletion

### Compliance Alignment

**GDPR Compliance**:
- ✓ Explicit consent (opt-in)
- ✓ Purpose limitation (only learnings, not raw data)
- ✓ Data minimization (sanitization + learning extraction)
- ✓ Right to access (view pending uploads, local data, consent records)
- ✓ Right to rectification (correct local data)
- ✓ Right to erasure (deletion of local data and consent records)
- ✓ Right to restriction (pause processing)
- ✓ Right to data portability (export in machine-readable format)
- ✓ Right to withdraw consent (disable-sharing, local data deleted)
- ✓ Right to lodge complaint (with supervisory authority)
- ✓ Transparency (clear consent dialog, controller identity, DPO contact)
- ✓ Controller/processor roles defined
- ✓ International transfer mechanisms (SCCs for cross-border)
- ✓ Subprocessor list (IPFS pinning, gateways, blockchain infra)
- ✓ DPIA completed before enabling sharing (Article 35)
- ✓ True anonymization (GDPR Recital 26) - erasure applies to local data only

**CCPA Compliance**:
- ✓ Notice at collection (consent dialog)
- ✓ Right to know (view learnings before upload)
- ✓ Right to delete (revocation mechanism)
- ✓ Right to opt-out (local-only mode)
- ✓ No sale of personal information (sanitized, anonymous)

**Privacy-by-Design**:
- ✓ Proactive not reactive (default local-only)
- ✓ Privacy as default setting (opt-in required)
- ✓ Privacy embedded in design (no raw data sharing)
- ✓ Full functionality (local mode fully functional)
- ✓ End-to-end security (sanitize before storage, encrypt SQLite at rest)
- ✓ Visibility and transparency (manual approval)
- ✓ Respect for user privacy (revocation support)

**LGPD Compliance (Brazil)**:
- ✓ Lawful basis (consent)
- ✓ Purpose specification
- ✓ Data subject rights (access, correction, deletion, portability)
- ✓ International transfer safeguards

**UK GDPR Compliance**:
- ✓ Same requirements as EU GDPR
- ✓ Post-Brexit UK data protection standards
- ✓ ICO as supervisory authority

## Consequences

### Positive

#### Trust & Adoption
- **User confidence** - Clear, safe defaults build trust
- **Regulatory compliance** - Aligns with GDPR, CCPA, PIPEDA
- **Ethical alignment** - Respects user agency and consent
- **Transparent operation** - Users understand what happens to their data

#### Legal Protection
- **Clear licensing** - CC BY 4.0 is well-understood and tested
- **Liability reduction** - Users explicitly consent to sharing
- **IP clarity** - Licensing terms prevent disputes
- **Revocation trail** - Blockchain records consent changes

#### Community Growth
- **Open license** - Encourages derivatives and innovation
- **Attribution options** - Enables reputation building
- **Quality focus** - Manual approval ensures high-quality learnings
- **Network effects** - More sharing = more value for all

### Negative

#### User Experience
- **Friction** - Manual approval slows uploads (MVP)
- **Complexity** - Users must understand licensing/consent
- **Decision fatigue** - Approve every upload individually
- **Onboarding overhead** - Longer setup for global sharing

#### Network Growth
- **Slower adoption** - Default local-only delays network growth
- **Lower contribution rate** - Manual approval reduces uploads
- **Cold start problem** - Fewer learnings initially
- **Asymmetric value** - Early adopters contribute more than they gain

#### Technical Overhead
- **Approval UI** - Must build manual approval interface
- **Revocation system** - Complex blockchain + IPFS coordination
- **State management** - Track consent status, preferences
- **Migration complexity** - Changing consent model later is hard

### Neutral

#### Implementation Requirements
- Consent dialog UI (CLI for MVP)
- Approval workflow and queue UI
- License metadata in uploads
- Revocation blockchain events
- Attribution tracking (optional)
- Compliance documentation

#### Operational Considerations
- User education materials
- Legal terms of service
- Privacy policy
- Consent logging and audit trail
- Revocation request handling

## Alternatives Considered

### Alternative 1: Default Opt-In (Share by Default)

**Description**: Users share globally by default, can opt-out.

**Pros**:
- Faster network growth
- More learnings from day one
- Better network effects
- Simpler UX (one-time opt-out)

**Cons**:
- **Violates privacy-by-design** - Not the safest default
- **Regulatory risk** - May not meet GDPR explicit consent
- **Trust issues** - Users may feel tricked or surprised
- **Higher liability** - More accidental PII sharing risk

**Why not chosen**: Violates core privacy-first principle. Default must be safest option.

### Alternative 2: Proprietary License (All Rights Reserved)

**Description**: System retains all rights, users grant license to share.

**Pros**:
- Control over use
- Can monetize later
- Prevent commercial exploitation
- Restrict derivatives

**Cons**:
- **Limits innovation** - Derivatives require permission
- **Legal complexity** - Must define license terms ourselves
- **Trust issues** - Users may not want to grant broad rights
- **Network effects limited** - Closed ecosystem grows slower

**Why not chosen**: Contradicts open, collaborative vision. We want learnings to spread freely.

### Alternative 3: Share-Alike License (CC BY-SA 4.0 or ODbL)

**Description**: Require derivatives to use same license.

**Pros**:
- Prevents proprietary capture (copyleft)
- Ensures improvements shared back
- Community remains open
- Aligns with open-source values

**Cons**:
- **Less permissive** - May reduce adoption
- **Licensing complexity** - Commercial users may avoid
- **Compatibility issues** - Harder to mix with other licenses
- **Enforcement burden** - Must monitor compliance

**Why not chosen**: Too restrictive for AI training use case. Want maximum reuse.

### Alternative 4: Public Domain (CC0)

**Description**: No rights reserved, complete public domain.

**Pros**:
- Maximum freedom
- No attribution required
- Simplest legal structure
- Zero restrictions

**Cons**:
- **No attribution** - Contributors get no credit
- **No incentive** - Can't build reputation
- **Irrevocable** - Can't change later
- **No quality signal** - Anyone can use without attribution

**Why not chosen**: Attribution provides incentive and quality signal. CC BY better aligns with reputation/token system.

### Alternative 5: Automatic Upload (No Manual Approval)

**Description**: Auto-upload learnings after extraction, no approval.

**Pros**:
- Zero friction
- Faster network growth
- Better UX
- More contributions

**Cons**:
- **Less control** - Users can't review before sharing
- **Quality risk** - Bad learnings slip through
- **PII risk** - Sanitization failures auto-published
- **Consent concerns** - May not meet "informed consent" bar

**Why not chosen**: MVP requires manual approval for safety. Can automate post-MVP with confidence thresholds.

### Alternative 6: Tiered Consent (Local/Team/Global)

**Description**: Three sharing levels: local-only, team/org, global.

**Pros**:
- Granular control
- Enables enterprise use
- Private sharing within teams
- Progressive trust model

**Cons**:
- **Complexity** - Three modes to implement
- **Scope creep** - MVP doesn't need team sharing
- **Access control** - Must implement team permissions
- **UX complexity** - Users must understand three modes

**Why not chosen**: Great for post-MVP, but MVP only needs local vs. global. Keep it simple.

## Implementation

### Phase 1: Local-Only Default (Week 1)

No consent required, everything local:

```typescript
// Default config
const defaultConfig = {
  mode: "local-only",
  sharing_enabled: false,
  manual_approval_required: true,
  license: "CC-BY-4.0",
  attribution: "anonymous"
};
```

### Phase 2: Opt-In Flow (Week 4-5)

Add consent dialog and preference storage:

```typescript
interface ConsentRecord {
  user_id: string;
  consent_given_at: string | null;  // ISO-8601 timestamp
  consent_withdrawn_at: string | null;
  consent_version: string;  // "v1.0" for tracking changes
  consent_text_hash: string;  // Hash of exact text shown (audit integrity)
  sharing_enabled: boolean;
  manual_approval_required: boolean;
  attribution_preference: AttributionPreference;
  license_accepted: string;  // "CC-BY-4.0"
  age_confirmed: boolean;  // 16+ EU, 13+ US
  user_agent_hash: string;  // Salted hash only (not raw UA)
  // ip_address removed: creates GDPR obligations without clear necessity
  retention_delete_after: string;  // Auto-delete date (12-24 months)
}

// Consent dialog
async function requestConsent(): Promise<boolean> {
  const dialog = new ConsentDialog({
    title: "Enable Global Learning Sharing",
    content: CONSENT_TEXT,  // Full disclosure text (layered, plain-language)
    examples: {
      allowed: [
        "✓ 'useEffect with empty deps runs once on mount'",
        "✓ 'TypeScript decorators need experimentalDecorators flag'",
        "✓ 'SQL EXPLAIN shows query execution plan'"
      ],
      prohibited: [
        "✗ 'My company XYZ uses this pattern...'",
        "✗ 'When John from engineering told me...'",
        "✗ 'I have a rare genetic condition...'",
        "✗ Code snippets >10 lines",
        "✗ API keys, credentials, internal URLs"
      ]
    },
    warnings: [
      "IPFS permanence: Deletion is best-effort, not guaranteed",
      "Patent implications: Public disclosure may affect patentability",
      "Do not share: personal info, work secrets, confidential data"
    ],
    checkboxes: [
      "I understand learnings will be shared publicly",
      "I understand IPFS data is hard to delete",
      "I accept the CC BY 4.0 license for my contributions",
      "I will not share identifiable individuals or incidents"
    ],
    links: {
      license: "https://creativecommons.org/licenses/by/4.0/",
      privacy_policy: "./docs/legal/PRIVACY.md",
      terms_of_service: "./docs/legal/TERMS.md",
      contribution_agreement: "./docs/legal/CONTRIBUTION.md",
      anonymization_standard: "./docs/standards/ANONYMIZATION.md"
    }
  });

  const accepted = await dialog.show();

  if (accepted) {
    const consentText = CONSENT_TEXT;  // Full disclosure text
    const retentionMonths = 24;  // 12-24 month retention

    await db.consent.insert({
      user_id: getCurrentUserId(),
      consent_given_at: new Date().toISOString(),
      consent_version: "v1.0",
      consent_text_hash: hashConsentText(consentText),  // SHA-256 hash
      sharing_enabled: true,
      manual_approval_required: true,
      attribution_preference: "anonymous",
      license_accepted: "CC-BY-4.0",
      age_confirmed: true,  // From age verification step
      user_agent_hash: hashWithSalt(getUserAgent()),  // Salted hash
      retention_delete_after: addMonths(new Date(), retentionMonths).toISOString()
    });
  }

  return accepted;
}
```

### Phase 3: Manual Approval UI (Week 5)

Review and approve learnings before upload:

```typescript
interface UploadApproval {
  learning_id: string;
  preview: string;
  full_content: string;
  metadata: {
    size_bytes: number;
    tags: string[];
    quality_score: number;
    pii_check_status: "passed" | "warning" | "failed";
    code_snippet_check: "passed" | "warning" | "failed";  // >10 lines blocked
    secret_scan_status: "passed" | "warning" | "failed";  // API keys, credentials
    reidentification_risk: "low" | "medium" | "high";  // k-anonymity score
    sensitive_category_check: "passed" | "warning" | "failed";  // health, race, etc.
    language?: string;
    framework?: string;
  };
  license: string;
  attribution: AttributionPreference;
  created_at: string;
}

async function approveUploads() {
  const pending = await db.learnings.findPending();

  for (const learning of pending) {
    const approval: UploadApproval = {
      learning_id: learning.id,
      preview: learning.content.slice(0, 200),
      full_content: learning.content,
      metadata: {
        size_bytes: Buffer.byteLength(learning.content),
        tags: learning.tags,
        quality_score: learning.quality_score,
        pii_check_status: learning.pii_check_status,
        code_snippet_check: learning.code_snippet_check,
        secret_scan_status: learning.secret_scan_status,
        reidentification_risk: learning.reidentification_risk,
        sensitive_category_check: learning.sensitive_category_check
      },
      license: "CC-BY-4.0",
      attribution: getAttributionPreference(),
      created_at: learning.created_at
    };

    const decision = await showApprovalDialog(approval);

    if (decision === "approve") {
      await uploadLearning(learning);
    } else if (decision === "reject") {
      await db.learnings.update(learning.id, {
        upload_status: "rejected_by_user"
      });
    }
  }
}
```

### Phase 4: License Metadata (Week 6)

Embed license in all uploads:

```typescript
interface LearningUpload {
  // Content
  learning_id: string;
  content: string;
  tags: string[];
  quality_score: number;

  // License metadata
  license: {
    type: "CC-BY-4.0",
    url: "https://creativecommons.org/licenses/by/4.0/",
    full_text_cid: string  // IPFS CID of full license text
  };

  // Attribution
  attribution: {
    type: AttributionPreference;
    identifier?: string;  // Ethereum address, handle, or null
    timestamp: string;
  };

  // Provenance
  source: {
    system: "global-context-network",
    version: "0.1.0",
    timestamp: string;
    sanitization_version: string;
  };

  // Signature (for verification and anti-spam)
  signature: string;  // REQUIRED cryptographic signature
}

// Upload with metadata
async function uploadLearning(learning: Learning) {
  // Rate limiting check
  await enforceRateLimit(getUserId());

  const upload: LearningUpload = {
    learning_id: learning.id,
    content: learning.content,
    tags: learning.tags,
    quality_score: learning.quality_score,
    license: {
      type: "CC-BY-4.0",
      url: "https://creativecommons.org/licenses/by/4.0/",
      full_text_cid: CC_BY_40_LICENSE_CID
    },
    attribution: {
      type: getAttributionPreference(),
      identifier: getAttributionIdentifier(),
      timestamp: new Date().toISOString()
    },
    source: {
      system: "global-context-network",
      version: getVersion(),
      timestamp: new Date().toISOString(),
      sanitization_version: getSanitizationVersion()
    },
    signature: await signUpload(learning)  // REQUIRED for spam prevention
  };

  // Upload to IPFS
  const cid = await ipfs.add(JSON.stringify(upload));

  // Record on blockchain
  await blockchain.recordUpload({
    cid,
    learning_id: learning.id,
    license: "CC-BY-4.0",
    attribution: upload.attribution.type,
    timestamp: Date.now()
  });

  // Update local record
  await db.uploads.insert({
    learning_id: learning.id,
    ipfs_cid: cid,
    uploaded_at: new Date().toISOString(),
    license: "CC-BY-4.0"
  });
}
```

### Phase 5: Revocation System (Week 7)

Enable deletion and revocation:

```typescript
interface RevocationEvent {
  learning_id: string;
  ipfs_cid: string;
  revoked_at: string;
  reason: "user-requested" | "privacy-violation" | "quality-issue";
  user_signature?: string;
}

async function revokeLearning(learningId: string) {
  const upload = await db.uploads.findOne({ learning_id: learningId });

  if (!upload) {
    throw new Error("Learning not uploaded");
  }

  // 1. Delete from local database
  await db.learnings.delete({ id: learningId });
  await db.uploads.delete({ learning_id: learningId });

  // 2. Unpin from IPFS
  await ipfs.unpin(upload.ipfs_cid);

  // 3. Publish revocation event to blockchain
  const revocation: RevocationEvent = {
    learning_id: learningId,
    ipfs_cid: upload.ipfs_cid,
    revoked_at: new Date().toISOString(),
    reason: "user-requested"
  };

  await blockchain.publishRevocation(revocation);

  // 4. Add to revocation list
  await db.revocations.insert(revocation);

  // 5. Publish tombstone CID to revocation registry
  await publishTombstone(upload.ipfs_cid);

  // 6. Request other nodes unpin (best effort)
  await requestNetworkUnpin(upload.ipfs_cid);

  // 7. Notify known pinning partners
  await notifyPinningPartners(upload.ipfs_cid, "revoked");
}

async function publishTombstone(cid: string) {
  // Public revocation registry for cache purge and mirrors
  const tombstone = {
    type: "revocation",
    cid,
    revoked_at: new Date().toISOString(),
    reason: "user-requested"
  };

  // Publish to revocation registry (separate IPFS collection)
  const tombstoneCid = await ipfs.add(JSON.stringify(tombstone));
  await blockchain.recordTombstone({ original_cid: cid, tombstone_cid: tombstoneCid });
}

async function withdrawConsent() {
  // Update consent record
  await db.consent.update({
    consent_withdrawn_at: new Date().toISOString(),
    sharing_enabled: false
  });

  // Cancel all pending uploads
  await db.learnings.updateMany(
    { upload_status: "queued" },
    { upload_status: "cancelled_by_user" }
  );

  // Optionally: revoke all existing uploads
  const uploads = await db.uploads.findAll();
  for (const upload of uploads) {
    await revokeLearning(upload.learning_id);
  }
}

// Kill-switch for catastrophic sanitization failure
async function emergencyKillSwitch(reason: string) {
  console.error(`EMERGENCY KILL-SWITCH ACTIVATED: ${reason}`);

  // 1. Immediately halt all uploads system-wide
  await db.config.update({ uploads_enabled: false, kill_switch_active: true });

  // 2. Cancel all pending uploads
  await db.learnings.updateMany(
    { upload_status: "queued" },
    { upload_status: "cancelled_emergency" }
  );

  // 3. Alert administrators
  await alertAdministrators({
    severity: "CRITICAL",
    reason,
    timestamp: new Date().toISOString()
  });

  // 4. Log incident for audit
  await db.incidents.insert({
    type: "kill_switch_activation",
    reason,
    timestamp: new Date().toISOString()
  });
}
```

### Database Schema Updates

```sql
-- Consent tracking (minimized per GPT-5 feedback)
CREATE TABLE consent (
  id TEXT PRIMARY KEY,
  user_id TEXT NOT NULL,
  consent_given_at TEXT,  -- ISO-8601
  consent_withdrawn_at TEXT,
  consent_version TEXT NOT NULL,
  consent_text_hash TEXT NOT NULL,  -- Hash of exact text shown
  sharing_enabled INTEGER NOT NULL DEFAULT 0,
  manual_approval_required INTEGER NOT NULL DEFAULT 1,
  attribution_preference TEXT NOT NULL DEFAULT 'anonymous',
  license_accepted TEXT NOT NULL,
  age_confirmed INTEGER NOT NULL DEFAULT 0,  -- 16+ EU, 13+ US
  user_agent_hash TEXT,  -- Salted hash only, not raw UA
  -- ip_address removed: not necessary, creates GDPR obligations
  created_at TEXT NOT NULL,
  updated_at TEXT NOT NULL,
  retention_delete_after TEXT  -- Auto-delete date (12-24 months)
);

-- Upload approvals
CREATE TABLE upload_approvals (
  id TEXT PRIMARY KEY,
  learning_id TEXT NOT NULL,
  reviewed_at TEXT NOT NULL,
  decision TEXT NOT NULL,  -- 'approved', 'rejected', 'pending'
  reviewer TEXT NOT NULL,  -- 'user' or 'automated'
  notes TEXT,
  FOREIGN KEY (learning_id) REFERENCES learnings(id)
);

-- Revocations
CREATE TABLE revocations (
  id TEXT PRIMARY KEY,
  learning_id TEXT NOT NULL,
  ipfs_cid TEXT NOT NULL,
  revoked_at TEXT NOT NULL,
  reason TEXT NOT NULL,
  user_signature TEXT,
  created_at TEXT NOT NULL
);

-- Update learnings table
ALTER TABLE learnings ADD COLUMN license TEXT DEFAULT 'CC-BY-4.0';
ALTER TABLE learnings ADD COLUMN attribution_type TEXT DEFAULT 'anonymous';
ALTER TABLE learnings ADD COLUMN upload_status TEXT DEFAULT 'not_approved';
  -- 'not_approved', 'approved', 'uploaded', 'rejected', 'revoked'
```

## Risks and Mitigations

### Risk: Users Don't Understand Implications

**Impact**: High - Uninformed consent is not valid consent

**Mitigation**:
- Clear, plain-language consent dialog
- Visual examples of what is/isn't shared
- Link to privacy policy and license
- Require checkbox acknowledgments
- Educational documentation
- Warning about IPFS permanence

### Risk: IPFS Data Not Actually Deleted

**Impact**: Medium - Users expect deletion to work

**Mitigation**:
- Clear disclosure: "best-effort, not guaranteed"
- Unpin from our nodes immediately
- Publish revocation event to blockchain
- Request other nodes unpin (DHT announcement)
- Document limitations in consent dialog
- Consider alternative storage with deletion guarantees post-MVP

### Risk: License Change Needed Later

**Impact**: Medium - CC BY 4.0 is irrevocable

**Mitigation**:
- Version consent records ("v1.0")
- New consent required for license changes
- Grandfather old uploads under old license
- Clear versioning in metadata
- Consider dual-licensing from start

### Risk: PII in Learnings Despite Sanitization

**Impact**: Critical - Privacy violation

**Mitigation**:
- Multi-layer sanitization (rules + AI)
- Manual approval shows PII check status
- Block uploads with failed PII checks
- Post-upload audits
- Kill-switch to halt all uploads
- Revocation mechanism for mistakes

### Risk: Low Adoption Due to Friction

**Impact**: Medium - Network growth slowed

**Mitigation**:
- MVP: Accept manual approval overhead
- Phase 2: Add confidence-based auto-approval
- Phase 3: One-click bulk approval
- Metrics: Track approval/rejection rates
- Iterate: Simplify based on user feedback

### Risk: Legal Liability for Shared Content

**Impact**: High - Legal exposure if learnings are harmful

**Mitigation**:
- Terms of service disclaimer
- User warranty: "I have rights to share"
- Safe harbor provisions (DMCA, platform immunity)
- Quality scoring filters low-quality learnings
- Community reporting mechanism
- Revocation for policy violations

### Risk: Re-Identification Despite Anonymization (GPT-5)

**Impact**: Critical - GDPR violation if personal data published

**Mitigation**:
- Quantitative re-identification testing (k-anonymity checks)
- Adversarial review on samples before publish
- Block high-uniqueness content (rare job titles, specific incidents)
- Multi-model PII detection (rules + AI + NER)
- Special category blocklist (health, race, religion, politics if person-linked)
- Reject learnings with "small k" cohort hints

### Risk: Third-Party/Confidential Content Leakage (GPT-5)

**Impact**: High - Legal liability, NDA violations, trade secret exposure

**Mitigation**:
- Contributor warranty: "No confidential/proprietary information"
- Automated secret scanning (API keys, internal URLs, company names)
- Provenance checks: verify user has rights to share
- Policy: "No employer/client/NDA material"
- Indemnity clause in Data Contribution Agreement

### Risk: Code Snippet Copyright Infringement (GPT-5)

**Impact**: High - Copyright violation if non-trivial code shared

**Mitigation**:
- MVP: Block all non-trivial code snippets by policy
- Automated detection: flag code blocks >10 lines
- Future: Separate code license (MIT) with provenance
- User education: "Share concepts, not copy-pasted code"

### Risk: Defamation/Illegal Content (GPT-5)

**Impact**: High - Legal liability, harm to individuals

**Mitigation**:
- Automated classification to block regulated content
- Policy: No medical advice, export-controlled, illegal content
- Fast-track takedown process
- Abuse reporting mechanism with agent contact
- Kill-switch for immediate unpinning

### Risk: Dataset Poisoning/Spam (GPT-5)

**Impact**: Medium - Network quality degradation

**Mitigation**:
- Cryptographic signatures required on all uploads
- Rate limiting per user
- Reputation system / allowlists (post-MVP)
- Post-publication flagging and quarantine workflow
- Quality scoring threshold enforcement

### Risk: Attribution Privacy Leakage (GPT-5)

**Impact**: Medium - Pseudonymous attribution can be personal data

**Mitigation**:
- Treat blockchain addresses as personal data if linkable
- Explicit consent for pseudonymous attribution
- Disclosure: "Address may be traceable to identity"
- Default: Anonymous (no attribution)

### Risk: Consent Logging Contains Personal Data (GPT-5)

**Impact**: Medium - IP/user-agent storage creates GDPR obligations

**Mitigation**:
- Minimize: Store salted hash of user-agent, avoid IP unless fraud prevention
- Strict retention: 12-24 months maximum, auto-delete
- Access controls: Limit who can view consent logs
- Store hash of exact consent text shown for audit integrity
- Document necessity justification or remove entirely

### Risk: Missing DPIA (GPT-5)

**Impact**: High - GDPR compliance requirement for high-risk processing

**Mitigation**:
- Conduct Data Protection Impact Assessment before enabling sharing
- Document: processing purposes, necessity, risks, mitigations
- Identify residual risks and acceptance criteria
- DPO review if required (organizations >250 employees)
- Update regularly as system evolves

### Risk: Minors Using System (GPT-5)

**Impact**: High - COPPA/GDPR violations

**Mitigation**:
- Age gating: Require confirmation of 16+ (EU) or 13+ (US)
- Block global sharing entirely for minors in MVP
- Future: Verifiable parental consent mechanism
- Clear disclosure in onboarding

### Risk: Cross-Jurisdiction Compliance (GPT-5)

**Impact**: Medium - Different laws in different countries

**Mitigation**:
- Address LGPD (Brazil), UK GDPR explicitly
- International transfer mechanisms (SCCs) for consent logs
- List subprocessors (IPFS pinning, gateways, blockchain infra)
- General international clause in Privacy Policy

### Risk: Security and Local Data Breaches (GPT-5)

**Impact**: High - Exposure of local conversations, consent records, personal data

**Mitigation**:
- Encrypt SQLite database at rest
- Key handling and secure storage
- Least privilege access controls
- Incident response plan documented
- Breach notification procedures (72 hours GDPR, state laws)
- Regular security audits
- Secure coding practices
- Input validation and sanitization

### Risk: Token/Rewards Future Implementation (GPT-5)

**Impact**: High - AML/KYC obligations if tokens have monetary value

**Mitigation**:
- AML/KYC requirements for token distributions
- Tax reporting obligations
- Sanctions screening
- Securities law compliance
- Clear disclosures about token nature and risks
- Consult legal counsel before token launch

## Post-MVP Enhancements

### Automated Approval (Phase 2)

```typescript
// Auto-approve high-confidence learnings
if (learning.quality_score >= 9.0 &&
    learning.pii_check_status === "passed" &&
    user.auto_approve_enabled) {
  await uploadLearning(learning);
} else {
  await queueForManualApproval(learning);
}
```

### Bulk Operations (Phase 2)

```bash
# Approve all pending
gcn approve-all --min-quality 8.0

# Batch review
gcn review-pending --show 10
```

### Tiered Sharing (Phase 3)

```typescript
enum SharingTier {
  LOCAL = "local",          // Only this machine
  TEAM = "team",            // Organization/team
  GLOBAL = "global"         // Public network
}
```

### Licensing Options (Phase 4)

Allow users to choose license per learning:
- CC BY 4.0 (default)
- CC BY-SA 4.0 (share-alike)
- CC0 (public domain)
- ODbL (database license)

### Smart Contract Consent (Phase 5)

On-chain consent records:
```solidity
contract ConsentRegistry {
  event ConsentGiven(address user, uint256 timestamp, string version);
  event ConsentWithdrawn(address user, uint256 timestamp);

  mapping(address => ConsentRecord) public consents;
}
```

## Pre-Launch Requirements (GPT-5 Priority Actions)

Before enabling global sharing in production, the following MUST be completed:

### 1. Data Protection Impact Assessment (DPIA)

**Required by**: GDPR Article 35 (high-risk processing)

**Must document**:
- Processing purposes and necessity justification
- Data flows (local → sanitization → learning extraction → IPFS/blockchain)
- Risks to data subjects (re-identification, immutability, etc.)
- Mitigations and controls (anonymization, manual approval, etc.)
- Residual risks and acceptance criteria
- Controller/processor roles and responsibilities
- DPO review (if organization >250 employees)

**Deliverable**: `docs/legal/DPIA-2025.md`

### 2. Legal Documentation

**Privacy Policy** (`docs/legal/PRIVACY.md`):
- Controller identity and contact details
- DPO contact (if required)
- Processing purposes and legal bases
- Data categories processed (consent logs, learnings)
- Retention periods (12-24 months for consent, indefinite for anonymized learnings)
- Data subject rights (access, deletion, portability, restriction)
- International transfers (SCCs, subprocessors)
- Complaint mechanism (supervisory authority)

**Terms of Service** (`docs/legal/TERMS.md`):
- Service description
- User obligations
- Acceptable use policy
- Prohibited content
- Intellectual property
- Limitation of liability
- Dispute resolution
- Governing law

**Data Contribution Agreement** (`docs/legal/CONTRIBUTION.md`):
- Warranties (rights to share, no PII, no confidential info)
- License grant (CC BY 4.0)
- Moral rights waiver
- Database rights waiver
- Indemnification clause
- Age confirmation
- No medical/illegal/regulated content covenant

**Abuse and Takedown Policy** (`docs/legal/ABUSE.md`):
- DMCA agent contact
- Takedown request process
- Repeat infringer policy
- Kill-switch procedures
- Appeal process

### 3. Anonymization Standard

**Deliverable**: `docs/standards/ANONYMIZATION.md`

Must define:
- Quantitative thresholds (k-anonymity ≥10, l-diversity for sensitive attributes)
- Multi-model PII detection pipeline (rules + AI + NER)
- Domain-specific rules (rare job titles, unique incidents, geo/time granularity)
- Special category blocklist (health, race, religion if person-linked)
- Re-identification testing methodology
- Adversarial review process
- Rejection workflow for high-risk content

### 4. Technical Implementation

- Age gating UI/verification
- Consent text hash storage (SHA-256)
- User-agent hash (salted, not raw)
- Consent log retention auto-deletion (12-24 months)
- SQLite encryption at rest (local data security)
- Multi-model PII detection (rules + AI + NER)
- Code snippet detection and blocking (>10 lines)
- Secret scanning (API keys, credentials, internal URLs)
- Re-identification risk scoring (k-anonymity ≥10)
- Sensitive category detection (health, race, religion if person-linked)
- Cryptographic signatures on all uploads (required)
- Rate limiting per user (spam prevention)
- Revocation registry and tombstone CIDs
- Kill-switch mechanism
- Least privilege access controls
- Audit logging for consent access

### 5. Operational Procedures

- Takedown request handling (SLA: <24 hours)
- Kill-switch activation criteria and process
- Incident response plan
- Breach notification procedures (72 hours GDPR)
- Consent log access controls
- Subprocessor list maintenance

## Success Metrics

### Compliance Metrics
- 100% of uploads have valid consent record
- 0 uploads without license metadata
- <1% revocation requests
- 0 regulatory complaints

### Trust Metrics
- User satisfaction with consent process ≥4.5/5
- "I understand what is shared" agreement ≥90%
- Opt-in rate ≥20% of users (high trust signal)

### Network Metrics
- Learnings uploaded per user per week
- Approval rate (approved / total pending)
- Revocation rate (revoked / total uploaded)

## Related Documents

### Standards
- [Project Standards - Section 16: Consent & Licensing](../STANDARDS.md#16-consent--licensing-standard)

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [IPFS Upload System](../architecture/architecture-ipfs-upload-2025-01-16.md)

### Decisions
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md) - Never share raw data
- [ADR-005: Use SQLite](./decision-use-sqlite-2025-01-16.md) - Local storage
- [ADR-006: Async Processing Model](./decision-async-processing-model-2025-01-16.md) - Upload queue

### Plans
- [Original User Vision](../plans/plan-original-user-vision-2025-01-16.md) - Intent to share globally
- [Implementation Roadmap](../plans/plan-implementation-roadmap-2025-01-16.md) - Phase 7: Global Network

### External Reviews
- [GPT-5 Holistic Review](../reviews/gpt5-holistic-review-2025-01-16.txt) - Critical Issue #7
- [Gemini 2.5 Pro Review](../reviews/gemini-holistic-review-2025-01-16.txt) - Critical Blocker #2

### Legal
- [Privacy Policy](../legal/PRIVACY.md) (to be created)
- [Terms of Service](../legal/TERMS.md) (to be created)
- [Data Contribution Agreement](../legal/CONTRIBUTION.md) (to be created)
- [Abuse and Takedown Policy](../legal/ABUSE.md) (to be created)
- [DPIA](../legal/DPIA-2025.md) (to be created before launch)
- [CC BY 4.0 License Text](https://creativecommons.org/licenses/by/4.0/legalcode)

### Standards
- [Anonymization Standard](../standards/ANONYMIZATION.md) (to be created before launch)

### Review
- [GPT-5 Review of ADR-007](../reviews/gpt5-adr007-review-2025-01-16.txt) - Legal and ethical validation

---

**GPT-5 Validation**: This ADR was comprehensively reviewed by GPT-5 for legal and ethical soundness. ALL recommended improvements have been incorporated:

**Anonymization Rigor**:
- Quantitative k-anonymity thresholds (k ≥10, l-diversity for sensitive attributes)
- Multi-model PII detection pipeline (rules + AI + NER)
- Re-identification risk scoring and blocking
- Special category blocklist (health, race, religion if person-linked)
- Adversarial review process documented

**Legal Framework**:
- DPIA requirement before launch (GDPR Article 35)
- Controller/processor roles defined
- Data Contribution Agreement with warranties and indemnification
- Privacy Policy, Terms, Abuse Policy requirements documented
- International transfer mechanisms (SCCs, subprocessor list)

**Age Protection**:
- Age gating: 16+ (EU), 13+ (US) required
- Global sharing blocked for minors in MVP
- Age confirmation in contribution agreement

**Consent Minimization**:
- IP address removed (no necessity, creates GDPR obligations)
- User-agent hashed only (salted, not raw)
- 12-24 month retention with auto-deletion
- Consent text hash for audit integrity

**Additional Risks Addressed**:
- Re-identification (k-anonymity, adversarial testing)
- Confidential content leakage (secret scanning, warranties)
- Code copyright (>10 lines blocked, separate license future)
- Defamation/illegal content (classification, fast-track takedown)
- Dataset poisoning (cryptographic signatures, rate limiting)
- Attribution privacy (blockchain addresses treated as personal data)
- Security/local data (SQLite encryption, incident response)
- Token/rewards (AML/KYC documented for future)

**Legal Documentation Required**:
- Privacy Policy with controller identity, DPO, data subject rights
- Terms of Service with acceptable use, limitations
- Data Contribution Agreement with warranties, license grants, indemnity
- Abuse and Takedown Policy with DMCA agent, kill-switch
- Anonymization Standard with quantitative thresholds
- DPIA before enabling global sharing

**Technical Implementation**:
- Cryptographic signatures required on all uploads
- Multi-layer PII/secret/code detection
- Re-identification risk scoring
- Kill-switch mechanism
- Revocation registry with tombstone CIDs
- SQLite encryption at rest
- Rate limiting and spam prevention

**Cross-Jurisdiction**:
- GDPR (EU), CCPA (California), LGPD (Brazil), UK GDPR explicitly addressed
- International transfer mechanisms documented
- Subprocessor list requirement

**Database Rights**:
- Dual licensing: CC BY 4.0 per-learning, CC0/ODC-By for aggregated dataset
- Standard attribution string for anonymous contributions
- Moral rights and database rights waiver in contribution agreement

**Status**: This ADR now addresses ALL critical blockers identified by GPT-5 and Gemini 2.5 Pro reviews. It establishes a comprehensive, legally defensible, trust-first foundation for global learning sharing with quantifiable privacy protections and clear legal obligations.

**Disclaimer**: This is not legal advice. Consult qualified privacy/IP counsel to finalize DPIA, legal documents, and cross-border transfer posture before production deployment. All pre-launch requirements MUST be completed before enabling global sharing.
````

## File: decisions/decision-data-model-schema-versioning-2025-01-16.md
````markdown
---
title: ADR-012: Data Model Schema Versioning Strategy
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [schema, versioning, migrations, compatibility, json-schema]
reviewed_by: GPT-5 (holistic review)
---

# ADR-012: Data Model Schema Versioning Strategy

## Status

Accepted (Revised per GPT-5 holistic review feedback)

Date: 2025-01-16
Last Updated: 2025-01-16

## Context

The Global Context Network has multiple data schemas that will evolve over time:

1. **Database schema** - SQLite tables (conversations, messages, learnings, job_queue, uploads, sanitization_log)
2. **Event schemas** - JSON structures passed through hooks (UserPromptSubmit, Stop, etc.)
3. **Learning schemas** - Extracted insights and patterns
4. **Upload schemas** - Data serialized for global network
5. **MCP response schemas** - Query result formats

**The Challenge**: As the system evolves, we need to:
- Add new fields to existing schemas
- Change validation rules
- Deprecate old fields
- Support clients on different versions
- Migrate existing data without data loss
- Maintain backward compatibility for MCP queries

**The Risk**: Without a versioning strategy:
- **Breaking changes** - Old clients fail on new schemas
- **Data migration failures** - Existing data becomes incompatible
- **Undefined behavior** - Unknown schema versions processed incorrectly
- **Cross-client incompatibility** - Different clients expect different formats
- **Upgrade friction** - Users afraid to upgrade due to breaking changes
- **No rollback path** - Cannot safely revert failed migrations

**Real-world scenarios**:
1. Adding a new field to `messages` table (e.g., `metadata` column)
2. Changing learning extraction format (adding confidence scores)
3. Adding new PII categories to sanitization
4. Updating MCP query response format
5. Adding new status values to job_queue

**Review feedback** (GPT-5 holistic review 2025-01-16):

Critical issues addressed in this revision:
1. **Sanitized-only guarantee**: ALL content fields MUST contain only pre-sanitized data (NEVER raw), per STANDARDS.md zero-trust privacy flow
2. **Canonical status enums**: Adopts `queued | in_progress | completed | failed | dead_letter` from STANDARDS.md (no drift)
3. **Single migration tool**: Atlas (NOT goose) for all database migrations
4. **Backward compatibility direction**: Servers accept older minor versions; no forward compatibility guarantee
5. **MCP version negotiation**: Server advertises versions, client requests, fallback to latest, error codes specified
6. **Broken links removed**: Referenced ADRs noted as pending; no broken cross-references

## Decision

Implement **canonical event schemas with embedded versioning** and **database migrations with backward compatibility guarantees**.

### 1. Event Schema Versioning

All events include a `schema_version` field:

```typescript
interface BaseEvent {
  schema_version: string;  // Semantic version: "1.0.0", "1.1.0", "2.0.0"
  event_type: string;
  timestamp: string;       // ISO-8601 UTC
  session_id: string;      // ULID
}

interface UserPromptSubmitEvent extends BaseEvent {
  schema_version: "1.0.0";
  event_type: "UserPromptSubmit";
  conversation_id: string;  // ULID
  role: "user";
  content: string;          // MUST be sanitized-only per STANDARDS.md (NEVER raw)
}
```

**Semantic versioning rules**:
- **Major version** (2.0.0): Breaking changes (removed fields, incompatible types, enum value removal)
- **Minor version** (1.1.0): Backward-compatible additions (new optional fields, new enum values)
- **Patch version** (1.0.1): Bug fixes, documentation, no schema changes

**Compatibility direction** (backward compatible only):
- **Servers MUST accept older minor versions** - A v1.1.0 server accepts v1.0.0 events (backward compatibility)
- **NO forward compatibility guarantee** - A v1.0.0 server MAY reject v1.1.0 events (no obligation to support newer versions)
- **Cross-major versions are incompatible** - v2.x servers MUST reject v1.x events (breaking change)

**Status enum evolution** (adopts canonical JobStatus from STANDARDS.md):
```typescript
type JobStatus =
  | 'queued'        // Initial state
  | 'in_progress'   // Worker claimed
  | 'completed'     // Succeeded
  | 'failed'        // Failed but retriable
  | 'dead_letter';  // Failed permanently
```
- New values: Minor version bump (additive only)
- Removed/changed values: Major version bump (breaking change)

### 2. JSON Schema Validation

Use JSON Schema for runtime validation with exact-version routing:

```typescript
import Ajv from "ajv";
import addFormats from "ajv-formats";

// Initialize Ajv with formats (required for date-time validation in Ajv v8+)
const ajv = new Ajv();
addFormats(ajv);

// Version 1.0.0 schema (exact version)
const userPromptSubmitSchemaV1_0_0 = {
  $schema: "http://json-schema.org/draft-07/schema#",
  type: "object",
  required: ["schema_version", "event_type", "timestamp", "session_id", "conversation_id", "role", "content"],
  properties: {
    schema_version: {
      type: "string",
      const: "1.0.0"  // Exact version for strict validation
    },
    event_type: {
      type: "string",
      const: "UserPromptSubmit"
    },
    timestamp: { type: "string", format: "date-time" },
    session_id: { type: "string", pattern: "^[0-9A-HJKMNP-TV-Z]{26}$" },  // ULID pattern
    conversation_id: { type: "string", pattern: "^[0-9A-HJKMNP-TV-Z]{26}$" },
    role: {
      type: "string",
      const: "user"
    },
    content: { type: "string", minLength: 1 }  // Sanitized content only (NEVER raw)
  },
  additionalProperties: false  // Strict: reject unknown fields
};

// Version 1.1.0 schema (backward compatible - adds optional metadata)
const userPromptSubmitSchemaV1_1_0 = {
  $schema: "http://json-schema.org/draft-07/schema#",
  type: "object",
  required: ["schema_version", "event_type", "timestamp", "session_id", "conversation_id", "role", "content"],
  properties: {
    schema_version: {
      type: "string",
      const: "1.1.0"
    },
    event_type: {
      type: "string",
      const: "UserPromptSubmit"
    },
    timestamp: { type: "string", format: "date-time" },
    session_id: { type: "string", pattern: "^[0-9A-HJKMNP-TV-Z]{26}$" },
    conversation_id: { type: "string", pattern: "^[0-9A-HJKMNP-TV-Z]{26}$" },
    role: {
      type: "string",
      const: "user"
    },
    content: { type: "string", minLength: 1 },  // Sanitized content only (NEVER raw)
    metadata: { type: "object" }  // New optional field in v1.1.0
  },
  additionalProperties: false
};

// Validation function with exact-version routing
function validateEvent(event: unknown): asserts event is UserPromptSubmitEvent {
  if (typeof event !== "object" || event === null) {
    throw new Error("Event must be an object");
  }

  const versionedEvent = event as { schema_version?: string; event_type?: string };
  const version = versionedEvent.schema_version;
  const eventType = versionedEvent.event_type;

  if (!version || !eventType) {
    throw new Error("Event missing schema_version or event_type");
  }

  // Fetch exact schema from registry
  const schema = getSchema(eventType, version);
  const validate = ajv.compile(schema);

  if (!validate(event)) {
    throw new Error(`Schema validation failed: ${ajv.errorsText(validate.errors)}`);
  }
}
```

**Tolerant reader option** (for servers accepting minor version ranges):
```typescript
// Alternative: Tolerant minor-version schema (accepts any v1.x.x)
const userPromptSubmitSchemaV1_tolerant = {
  $schema: "http://json-schema.org/draft-07/schema#",
  type: "object",
  required: ["schema_version", "event_type", "timestamp", "session_id", "conversation_id", "role", "content"],
  properties: {
    schema_version: {
      type: "string",
      pattern: "^1\\.[0-9]+\\.[0-9]+$"  // Accepts 1.0.0, 1.1.0, 1.2.0, etc. (any v1.x.x)
    },
    event_type: {
      type: "string",
      const: "UserPromptSubmit"
    },
    timestamp: { type: "string", format: "date-time" },
    session_id: { type: "string", pattern: "^[0-9A-HJKMNP-TV-Z]{26}$" },
    conversation_id: { type: "string", pattern: "^[0-9A-HJKMNP-TV-Z]{26}$" },
    role: {
      type: "string",
      const: "user"
    },
    content: { type: "string", minLength: 1 }  // Sanitized content only (NEVER raw)
  },
  additionalProperties: true  // Tolerant: allow unknown fields from newer minor versions
};
```

**Decision**: Use exact-version routing for strictness. Servers achieve backward compatibility by registering multiple schema versions and routing to the requested version. Tolerant readers can be used by servers that want to accept any v1.x.x minor version with unknown field tolerance.

### 3. Database Migrations

**Migration tool**: Use Atlas (Ariga) for type-safe, versioned migrations.

**Why Atlas**:
- Native SQLite support with proper syntax handling
- Automatic checksum and version tracking
- Dry-run mode for migration preview
- Declarative schema-as-code option
- Built-in rollback support

**Migration file structure**:
```
migrations/
  20250116000001_initial_schema.sql       # Timestamp prefix
  20250116000002_add_metadata_column.sql
  20250116000003_add_sanitization_log.sql
  atlas.hcl                                # Atlas config
```

**Atlas configuration** (`atlas.hcl`):
```hcl
env "local" {
  src = "file://migrations"
  url = "sqlite://context.db"
  dev = "sqlite://file?mode=memory"

  migration {
    dir = "file://migrations"
    format = atlas
  }
}
```

**Migration format** (Atlas native, no goose markers):
```sql
-- 20250116000002_add_metadata_column.sql
-- Create migration (up)
ALTER TABLE messages ADD COLUMN metadata TEXT DEFAULT '{}';
CREATE INDEX idx_messages_metadata ON messages((json_extract(metadata, '$.priority')));
```

**Rollback migration** (separate down file):
```sql
-- 20250116000002_add_metadata_column.down.sql
-- Rollback migration (down)
DROP INDEX idx_messages_metadata;
-- Note: SQLite DROP COLUMN requires ≥3.35 or table reconstruction
ALTER TABLE messages DROP COLUMN metadata;
```

**SQLite compatibility notes**:
- **DROP COLUMN**: Requires SQLite ≥3.35 (2021). For older versions, use table reconstruction pattern.
- **JSON indexes**: Requires JSON1 extension (enabled by default in modern SQLite).
- **Expression indexes**: `json_extract` in CREATE INDEX requires expression index support (SQLite ≥3.9).
- **Transactions**: Atlas automatically wraps migrations in transactions for rollback safety.

**Atlas migration metadata** (managed automatically by Atlas):
- Atlas uses its own `atlas_schema_revisions` table
- Tracks version, checksum, applied_at, execution time
- No need for custom `schema_migrations` table

### 4. Backward Compatibility Strategy

**Rule**: Old data must remain readable after schema updates.

**Approach 1: Optional fields**
```typescript
// v1.0.0
interface Message {
  id: string;
  content: string;
}

// v1.1.0 (backward compatible)
interface Message {
  id: string;
  content: string;
  metadata?: Record<string, unknown>;  // New optional field
}
```

**Approach 2: Default values**
```sql
-- Add column with default value
ALTER TABLE messages ADD COLUMN metadata TEXT DEFAULT '{}';
```

**Approach 3: Data transformers**
```typescript
function migrateMessageV1ToV2(v1: MessageV1): MessageV2 {
  return {
    ...v1,
    metadata: {}  // Default for old records
  };
}
```

### 5. Version Detection and Routing

```typescript
interface VersionedEvent {
  schema_version: string;
  [key: string]: unknown;
}

async function processEvent(event: VersionedEvent) {
  const version = semver.parse(event.schema_version);

  if (!version) {
    throw new Error(`Invalid schema_version: ${event.schema_version}`);
  }

  // Route to appropriate handler
  if (semver.satisfies(version, "^1.0.0")) {
    return processEventV1(event as UserPromptSubmitEventV1);
  } else if (semver.satisfies(version, "^2.0.0")) {
    return processEventV2(event as UserPromptSubmitEventV2);
  } else {
    throw new Error(`Unsupported schema version: ${event.schema_version}`);
  }
}
```

### 6. Schema Registry

Centralized schema definitions:

```typescript
// src/schemas/registry.ts
export const SCHEMA_REGISTRY = {
  "UserPromptSubmit": {
    "1.0.0": userPromptSubmitSchemaV1_0_0,
    "1.1.0": userPromptSubmitSchemaV1_1_0,
    "2.0.0": userPromptSubmitSchemaV2_0_0
  },
  "Stop": {
    "1.0.0": stopSchemaV1_0_0
  }
};

export function getSchema(eventType: string, version: string): JSONSchema {
  const eventSchemas = SCHEMA_REGISTRY[eventType];
  if (!eventSchemas) {
    throw new Error(`Unknown event type: ${eventType}`);
  }

  const schema = eventSchemas[version];
  if (!schema) {
    throw new Error(`Unknown schema version ${version} for event ${eventType}`);
  }

  return schema;
}

export function getSupportedVersions(eventType: string): string[] {
  const eventSchemas = SCHEMA_REGISTRY[eventType];
  if (!eventSchemas) {
    return [];
  }
  return Object.keys(eventSchemas);
}
```

### 7. MCP Version Negotiation

MCP servers expose schema versions via capabilities and handle client version requests:

**Step 1: Server advertises supported versions**
```typescript
// MCP server initialization response includes supported schema versions
{
  "capabilities": {
    "resources": {
      "schema_version": "1.1.0",                      // Server's current/latest version
      "supported_versions": ["1.0.0", "1.1.0"]        // All versions server can serve
    }
  }
}
```

**Step 2: Client requests specific version**
```typescript
// Client includes optional schema_version in resource query
{
  "method": "resources/read",
  "params": {
    "uri": "context://conversations/latest",
    "schema_version": "1.0.0"  // Optional: client requests v1.0.0 explicitly
  }
}
```

**Step 3: Server responds with negotiated version**
```typescript
// Server returns data in requested version (or latest if not specified)
{
  "contents": [{
    "schema_version": "1.0.0",  // Actual version returned (matches request or server default)
    "uri": "context://conversations/latest",
    "mimeType": "application/json",
    "text": "{...}"  // Data formatted according to schema v1.0.0
  }]
}
```

**Version negotiation logic**:
```typescript
function negotiateVersion(
  clientRequestedVersion: string | undefined,
  serverSupportedVersions: string[]
): string {
  // If client specifies version, validate it's supported
  if (clientRequestedVersion) {
    if (!serverSupportedVersions.includes(clientRequestedVersion)) {
      throw new MCPError(
        ErrorCode.UNSUPPORTED_SCHEMA_VERSION,
        `Unsupported schema version ${clientRequestedVersion}. ` +
        `Server supports: ${serverSupportedVersions.join(", ")}`
      );
    }
    return clientRequestedVersion;
  }

  // Fallback: return latest supported version (last in array)
  return serverSupportedVersions[serverSupportedVersions.length - 1];
}
```

**Error codes**:
- `UNSUPPORTED_SCHEMA_VERSION` (-32001): Client requested version not in server's supported list
- `INVALID_SCHEMA_VERSION` (-32002): Malformed version string (not semver format)
- `SCHEMA_VERSION_REQUIRED` (-32003): Server requires explicit version (no default fallback)

**Fallback behavior**:
- If client omits `schema_version`: Server returns latest supported version
- If client requests unsupported version: Server returns error (no silent fallback to avoid breaking client assumptions)
- If client requests older version: Server transforms response to match older schema (backward compatibility)

### 8. Breaking Change Policy

**When breaking changes are necessary**:

1. **Announce deprecation** - 2 minor versions before removal
2. **Support both versions** - Old and new schemas during transition
3. **Provide migration path** - Document upgrade steps
4. **Version bump** - Major version increment
5. **Update docs** - Migration guide

**Example deprecation**:
```typescript
// v1.5.0 - Deprecate field
interface Message {
  id: string;
  content: string;
  /** @deprecated Use metadata.priority instead. Will be removed in v2.0.0 */
  priority?: number;
  metadata?: {
    priority?: number;
  };
}

// v2.0.0 - Remove deprecated field
interface Message {
  id: string;
  content: string;
  metadata: {
    priority?: number;
  };
}
```

## Consequences

### Positive

- **Predictable upgrades** - Semantic versioning communicates impact
- **Cross-client interoperability** - Clients declare supported versions
- **Safe migrations** - Rollback capability with down migrations
- **Runtime validation** - JSON Schema catches invalid data early
- **Audit trail** - Schema migrations table tracks all changes
- **Developer experience** - Clear versioning strategy reduces confusion
- **No data loss** - Backward compatibility preserves old data

### Negative

- **Migration complexity** - Writing migrations requires care
- **Version maintenance** - Must support multiple versions during transitions
- **Testing burden** - Must test all supported version combinations
- **Schema registry overhead** - Centralized registry needs maintenance
- **Breaking change friction** - Major version bumps require coordination

### Neutral

- **Version detection cost** - Small runtime overhead to parse version
- **Storage overhead** - Version field in every event
- **Documentation debt** - Must document migration paths

## Alternatives Considered

### Alternative 1: No Versioning (Breaking Changes Allowed)

**Description**: Evolve schemas freely, clients must stay up-to-date.

**Pros**:
- Simplest implementation
- No version overhead
- Faster iteration

**Cons**:
- **Breaks existing clients** - Forced upgrades
- **Data migration failures** - No backward compatibility
- **Poor user experience** - Unpredictable breaking changes
- **Cross-client issues** - Clients on different versions fail

**Why not chosen**: Violates stability principle. Users expect data to survive upgrades.

### Alternative 2: Avro/Protocol Buffers

**Description**: Use binary schema formats with built-in versioning.

**Pros**:
- **Stricter typing** - Schema evolution rules enforced
- **Smaller payloads** - Binary encoding
- **Better tooling** - Code generation, validation
- **Forward/backward compatibility** - Built into format

**Cons**:
- **Complexity** - Requires schema compiler
- **JSON incompatible** - Can't use with existing JSON tools
- **Learning curve** - Team must learn Avro/Protobuf
- **Ecosystem** - Less JavaScript/TypeScript support than JSON Schema

**Why not chosen**: Overkill for MVP. JSON Schema provides sufficient validation with better ecosystem fit. **Reconsider post-MVP** if binary formats needed for performance.

### Alternative 3: Database-Only Versioning (No Event Versioning)

**Description**: Version database schema only, not events.

**Pros**:
- Simpler event handling
- Fewer version checks
- Less overhead

**Cons**:
- **No cross-client safety** - Clients can send invalid events
- **Runtime errors** - Invalid data discovered late
- **No migration path** - Can't transform old events
- **Poor debugging** - Unknown event formats

**Why not chosen**: Events are the API contract. Must version the contract.

### Alternative 4: Copy-On-Write Versioning

**Description**: Create new tables for each schema version (e.g., `messages_v1`, `messages_v2`).

**Pros**:
- No migration needed
- Old data untouched
- Easy rollback

**Cons**:
- **Table proliferation** - Multiple tables for same entity
- **Query complexity** - Must UNION across versions
- **Storage duplication** - Data copied across versions
- **Index maintenance** - Multiple indexes to maintain

**Why not chosen**: Doesn't scale. Too many tables to manage.

### Alternative 5: Schema Evolution via Views

**Description**: Use SQL views to provide version compatibility.

**Pros**:
- Single source table
- Views provide version mapping
- No data duplication

**Cons**:
- **View complexity** - Complex transformations hard to express
- **Performance** - Views may be slower than direct queries
- **Limited transformations** - Can't express all migrations as views
- **SQLite limitations** - No materialized views

**Why not chosen**: Limited by SQLite view capabilities. Migrations more explicit.

## Implementation

### Phase 1: Add Schema Version to Events (Week 1)

**Tasks**:
1. Add `schema_version` field to all event interfaces
2. Update hook code to include version in events
3. Add version validation in event processing
4. Create schema registry with v1.0.0 schemas

**Acceptance**:
- All events include `schema_version: "1.0.0"`
- Invalid versions rejected with clear error
- Schema registry accessible

### Phase 2: JSON Schema Validation (Week 1)

**Tasks**:
1. Install `ajv`, `ajv-formats` dependencies
2. Write JSON Schema for all event types (v1.0.0)
3. Add validation to event processing pipeline
4. Generate TypeScript types from JSON Schemas (using `json-schema-to-typescript`)
5. Add validation tests with valid/invalid events

**Acceptance**:
- All events validated against JSON Schema
- TypeScript types auto-generated from schemas (no drift)
- Invalid events logged with detailed error
- 100% test coverage for validation

**Type generation**:
```bash
# Generate TypeScript types from JSON Schemas
npm install -D json-schema-to-typescript
npx json-schema-to-typescript src/schemas/*.schema.json -o src/schemas/types/
```

### Phase 3: Database Migrations (Week 2)

**Tasks**:
1. Install Atlas migration tool
2. Create initial migration (20250116000001_initial_schema.sql)
3. Configure atlas.hcl with SQLite environment
4. Write migration up/down tests
5. Verify SQLite version ≥3.35 for DROP COLUMN support
6. Document migration workflow (apply, rollback, dry-run)

**Acceptance**:
- Migrations apply successfully with `atlas migrate apply`
- Rollback (down) works correctly with .down.sql files
- Migration metadata tracked in `atlas_schema_revisions` (Atlas-managed)
- CI runs migrations in test environment
- Dry-run mode tested (`atlas migrate apply --dry-run`)

### Phase 4: Backward Compatibility Tests (Week 2)

**Tasks**:
1. Create test suite for version compatibility matrix
2. Test old events on new schema (v1.0.0 → v1.1.0)
3. Test new events on old schema (should fail gracefully or be rejected)
4. Test migration paths (v1 → v2) with data transformers
5. Add property-based tests for schema validation
6. Create deprecation tracker and CI warnings

**Version matrix testing**:
```typescript
const versionMatrix = [
  { client: "1.0.0", server: "1.0.0", expected: "success" },
  { client: "1.0.0", server: "1.1.0", expected: "success" }, // Backward compat
  { client: "1.1.0", server: "1.0.0", expected: "error" },   // No forward compat
  { client: "1.0.0", server: "2.0.0", expected: "error" },   // Major version incompatible
];
```

**Acceptance**:
- Old events (v1.0.0) work on v1.1.0 server (backward compatibility)
- v1.1.0 events rejected by v1.0.0 server (no forward compatibility)
- Cross-major versions incompatible (v2.x rejects v1.x)
- Data migration preserves all fields
- Property-based tests pass (generate random valid/invalid events)

## Risks and Mitigations

### Risk: Migration Failure on Production Database

**Impact**: Critical - Data loss, system downtime

**Mitigation**:
- **Always backup before migration** - Automated pre-migration backup
- **Test migrations on copy** - Run on production copy first
- **Dry-run mode** - Show SQL without applying
- **Rollback script** - Every migration has down script
- **Canary migrations** - Test on subset first

### Risk: Schema Drift Across Components

**Impact**: High - Components expect different schemas

**Mitigation**:
- **Single schema registry** - Centralized source of truth
- **Type generation** - Generate TypeScript from schemas
- **CI validation** - Schema changes require review
- **Version matrix testing** - Test all version combinations

### Risk: Forgotten Deprecations

**Impact**: Medium - Old fields never removed

**Mitigation**:
- **Deprecation tracker** - Document all deprecations with removal version
- **CI warnings** - Alert on deprecated field usage
- **Sunset timeline** - 2 versions before removal
- **Changelog** - Track all deprecations

### Risk: Over-Versioning

**Impact**: Low - Too many versions to maintain

**Mitigation**:
- **Minimize breaking changes** - Prefer backward-compatible additions
- **Batch changes** - Group breaking changes into single major version
- **Support policy** - Support N-1 major versions only

## Success Metrics

1. **Zero data loss** - All migrations preserve existing data
2. **Backward compatibility** - Old events (v1.0.0) work on current system
3. **Migration success rate** - 100% successful migrations in CI
4. **Version coverage** - All events include schema_version
5. **Documentation completeness** - Every schema version documented

## Related Documents

### Standards
- [Global Context Network Standards](../STANDARDS.md) - Canonical schema definitions, status enums, privacy guarantees

### Decisions
- ADR-004: Sanitize Before Storage - Mandates sanitized-only content in all schemas
- ADR-005: Use SQLite - Database choice and constraints
- ADR-006: Async Processing Model - Job queue and status workflow

### Reviews
- [GPT-5 Holistic Review](../reviews/gpt5-holistic-review-2025-01-16.txt) - Identified need for schema versioning strategy

**Note**: Referenced ADRs (004, 005, 006) are pending creation. This ADR establishes the versioning foundation that those ADRs will reference.

---

## Summary

This ADR establishes the canonical schema versioning strategy for all data models in the Global Context Network:

**Key decisions**:
1. **Semantic versioning** with embedded `schema_version` field in all events
2. **JSON Schema validation** with exact-version routing via central registry (ajv + ajv-formats)
3. **Atlas migrations** for database schema evolution with automatic rollback support (NOT goose)
4. **Backward compatibility** guaranteed for minor versions (servers MUST accept older events)
5. **No forward compatibility** for minor versions (older servers MAY reject newer events)
6. **MCP version negotiation** with server version advertisement, client requests, fallback to latest, and error codes
7. **Status enum evolution** follows canonical JobStatus from STANDARDS.md (queued, in_progress, completed, failed, dead_letter)
8. **Sanitized-only content guarantee** - ALL content fields MUST contain ONLY pre-sanitized data (NEVER raw input), per STANDARDS.md privacy flow

**Enforcement**:
- All events MUST include `schema_version` field
- All schemas MUST validate via JSON Schema with ajv + ajv-formats (NOT plain ajv without formats)
- All database changes MUST use Atlas migrations with up/down scripts (NOT goose)
- All content fields MUST contain ONLY sanitized data (NEVER raw input, per STANDARDS.md zero-trust privacy flow)
- All status enums MUST use canonical vocabulary from STANDARDS.md (queued, in_progress, completed, failed, dead_letter)
- All MCP servers MUST advertise supported_versions and handle version negotiation with proper error codes

*This ADR is the authoritative source for schema versioning. Any schema changes must follow these rules to ensure stability and interoperability.*
````

## File: decisions/decision-global-sharing-architecture-2025-01-16.md
````markdown
---
title: ADR-008: Global Sharing Architecture (IPFS + Blockchain)
category: decision
date: 2025-01-16
status: proposed
deciders: Claude + Dennison
tags: [ipfs, blockchain, celestia, evm, sharing, decentralization]
---

# ADR-008: Global Sharing Architecture (IPFS + Blockchain)

## Status

**Accepted** (with post-review revisions)

Date: 2025-01-16

**External Review**: GPT-4o and O1 reviews completed (see /tmp/gpt4o-adr008-review.txt, /tmp/o1-adr008-review.txt)

**Key Revisions Applied**:
- Added explicit privacy validation in upload code
- Clarified Celestia vs EVM separation (per STANDARDS.md #15)
- Enhanced consent/licensing requirements (per STANDARDS.md #16)
- Added ULID and ISO-8601 timestamp references
- Strengthened revocation list documentation
- Added legal consultation recommendation

## Context

The Global Context Network aims to enable AI agents to share learnings globally, creating a decentralized knowledge network. After learnings are extracted locally, we need a mechanism to:

1. **Store content immutably** - Learnings should be permanent and tamper-proof
2. **Share globally** - Any agent should be able to access shared learnings
3. **Maintain provenance** - Track who contributed what and when
4. **Enable incentives** - Reward quality contributions (future)
5. **Preserve privacy** - Only share derived learnings, never raw conversations
6. **Support validation** - Enable future validator network (post-MVP)

The user's original vision specified:
- "Put that data on chain or we put it into IPFS and put a hash on the blockchain"
- "Or we could put it into something like Celestia"
- Decentralized storage for global accessibility
- Blockchain for provenance and rewards

**Critical Clarifications** (from STANDARDS.md Section 15):
- **Celestia is NOT EVM-compatible** - It's a data availability layer
- **Smart contracts require EVM chain** - Base, Arbitrum, Optimism, or Polygon
- **Don't conflate the two** - EVM contracts ≠ Celestia DA

**Privacy Constraints** (STANDARDS.md #1, #6):
- NEVER upload raw conversations (contains PII even after sanitization)
- ONLY upload derived learnings (already aggregated, abstracted, generalized)
- Learnings are privacy-safe by design (see learning extraction spec)
- **Excludes chain-of-thought** per STANDARDS.md #6 (provider policy compliance)
- **Code-level validation**: Upload functions MUST reject any content flagged as raw/unsanitized

**Immutability vs Right-to-Delete Tension**:
- Blockchain and IPFS are immutable by design
- GDPR/CCPA may require deletion capability
- Mitigation: Only upload content we're comfortable making permanent
- **Revocation list approach** (see below) provides logical deletion per STANDARDS.md #16

## Decision

Use a hybrid architecture for MVP:
- **IPFS** for content storage (learnings only)
- **EVM L2 blockchain** for transaction records and provenance
- **Celestia** deferred to post-MVP for data availability optimization

### Content Storage: IPFS

**What we store**:
- Derived learnings ONLY (never raw conversations)
- Learning metadata (title, summary, tags, context)
- IPLD schema-compliant documents

**Why IPFS**:
- Content-addressed storage (CIDs are deterministic hashes)
- Globally accessible via public gateways
- Decentralized by design
- No single point of failure
- Free to store (pay for pinning services)
- Built-in deduplication (same content = same CID)

**Pinning Strategy**:
```
Local IPFS node (primary) → Pinning service (backup) → Public gateways (access)
```

**IPLD Schema for Learnings**:
```ipld
type Learning struct {
  id String           # ULID (per STANDARDS.md #4)
  title String
  summary String
  context String
  tags [String]
  category String     # "pattern" | "solution" | "pitfall" | "technique"
  sourceProject String?
  created String      # ISO-8601 timestamp (per STANDARDS.md #7)
  version String      # Schema version
  license String      # "CC-BY-4.0"
}
```

**NOTE**: All learnings are **derived only** (aggregated, sanitized, abstracted). No raw conversation content is ever included in this schema.

### Transaction Layer: EVM L2 Blockchain

**Chain Selection for MVP**: **Base** (Coinbase L2)

**Why Base**:
- Low transaction fees (~$0.01 per transaction)
- Fast finality (~2 seconds)
- Ethereum-compatible (standard tooling)
- Good developer experience
- Growing ecosystem
- Production-ready and stable

**Alternative EVM L2s considered**:
- Arbitrum: Similar fees, larger ecosystem
- Optimism: Similar tech to Base
- Polygon: Lower fees but less Ethereum-aligned
- **Decision**: Base for simplicity and cost-effectiveness

**What we record on-chain**:
- IPFS CID of learning
- Contributor address (optional pseudonymous identity)
- Timestamp
- Metadata hash (for verification)
- License type

**Smart Contract (Minimal MVP)**:
```solidity
// LearningRegistry.sol - Minimal upload tracking
contract LearningRegistry {
  struct Upload {
    string ipfsCid;      // IPFS content identifier
    address contributor; // Who uploaded
    uint256 timestamp;   // When uploaded
    bytes32 metadataHash; // Hash of metadata for verification
    string license;      // "CC-BY-4.0"
  }

  mapping(string => Upload) public uploads;
  string[] public cids;

  event LearningUploaded(
    string indexed ipfsCid,
    address indexed contributor,
    uint256 timestamp
  );

  function uploadLearning(
    string memory ipfsCid,
    bytes32 metadataHash,
    string memory license
  ) public {
    require(bytes(uploads[ipfsCid].ipfsCid).length == 0, "Already exists");

    uploads[ipfsCid] = Upload({
      ipfsCid: ipfsCid,
      contributor: msg.sender,
      timestamp: block.timestamp,
      metadataHash: metadataHash,
      license: license
    });

    cids.push(ipfsCid);

    emit LearningUploaded(ipfsCid, msg.sender, block.timestamp);
  }

  function getLearning(string memory ipfsCid)
    public
    view
    returns (Upload memory)
  {
    return uploads[ipfsCid];
  }

  function getTotalUploads() public view returns (uint256) {
    return cids.length;
  }
}
```

**Post-MVP Enhancements**:
- Token rewards for validated contributions
- Validator quorum mechanisms
- Quality scoring on-chain
- Reputation system
- Governance mechanisms

### Data Availability Layer: Celestia (Deferred)

**Why defer Celestia**:
- MVP doesn't need DA layer optimization
- Adds complexity without immediate benefit
- IPFS + pinning services sufficient for MVP scale
- Can integrate post-MVP without breaking changes

**CRITICAL** (per STANDARDS.md #15):
- **Celestia is a data availability layer, NOT an execution layer**
- **Celestia is NOT EVM-compatible** - cannot run smart contracts
- **Don't conflate Celestia (DA) with Base L2 (execution)**
- Celestia would be used alongside EVM L2, not instead of it

**Future Celestia Use Case**:
- Store content commitments on Celestia (data availability proofs)
- Reduce on-chain storage costs for large datasets
- Enable data availability sampling
- Support high-volume uploads with DA guarantees

**Integration Path** (Post-MVP):
```
IPFS (content storage) + Celestia (DA proofs) + Base L2 (smart contracts/transactions)
```

All three layers serve different purposes and complement each other.

## Consequences

### Positive

- **Immutable learnings** - IPFS CIDs guarantee content integrity
- **Global accessibility** - Anyone can fetch from IPFS
- **Decentralized** - No single point of control
- **Provenance tracking** - Blockchain records who contributed what
- **Privacy-safe** - Only derived learnings uploaded (no raw conversations)
- **Cost-effective** - Base L2 fees are negligible (~$0.01/upload)
- **Future-proof** - Can add Celestia, validators, tokens later
- **License clarity** - CC-BY-4.0 enables reuse
- **Standard tooling** - IPFS and EVM have mature ecosystems

### Negative

- **Immutability challenge** - Can't delete uploaded learnings
- **Pinning costs** - Need to pay pinning services for reliability
- **Blockchain dependency** - Requires wallet, gas fees
- **Complexity** - More complex than centralized storage
- **Censorship risk** - Pinning services could refuse content
- **Gateway reliance** - Public gateways could go down (mitigated by multiple gateways)
- **Gas price volatility** - L2 fees could spike (unlikely but possible)

### Neutral

- **IPLD schema versioning** - Need migration strategy for schema changes
- **CID format** - Using CIDv1 with base32 encoding
- **Pinning service choice** - Pinata, Web3.Storage, or self-hosted
- **Wallet management** - Users need private keys (can use session keys)

## Alternatives Considered

### Alternative 1: Centralized Cloud Storage (S3/GCS)

**Description**: Store learnings in AWS S3 or Google Cloud Storage.

**Pros**:
- Simple implementation
- No blockchain complexity
- Predictable costs
- Fast access
- Easy deletion (GDPR compliance)

**Cons**:
- **Single point of control** - Platform can censor or delete
- **Not decentralized** - Violates core vision
- **No provenance** - Hard to prove who contributed
- **Trust required** - Users must trust platform
- **Vendor lock-in** - Hard to migrate

**Why not chosen**: Violates decentralization principle. Goes against user's original vision.

### Alternative 2: Arweave (Permanent Storage)

**Description**: Use Arweave for permanent, pay-once storage.

**Pros**:
- Truly permanent (pay once, store forever)
- Decentralized
- No pinning costs
- Good for immutable data

**Cons**:
- **Expensive upfront** - ~$5-10 per MB
- **No deletion ever** - Stronger immutability than IPFS
- **Smaller ecosystem** - Less tooling than IPFS
- **GraphQL queries** - Different query model than IPFS

**Why not chosen**: Higher upfront cost. IPFS pinning services provide similar guarantees with more flexibility. Could reconsider post-MVP for truly critical learnings.

### Alternative 3: Filecoin (Decentralized Storage Market)

**Description**: Use Filecoin for decentralized storage with economic incentives.

**Pros**:
- Decentralized storage market
- Built on IPFS
- Storage proofs
- Economic incentives for storage providers

**Cons**:
- **More complex** - Deal-making, retrieval markets
- **Higher costs** - Storage deals can be expensive
- **Slower** - Deal negotiation takes time
- **Overkill for MVP** - Complexity not justified

**Why not chosen**: Too complex for MVP. IPFS with pinning services is simpler. Could integrate Filecoin post-MVP for larger-scale storage.

### Alternative 4: OrbitDB (P2P Database)

**Description**: Use OrbitDB for distributed, eventually-consistent database.

**Pros**:
- Built on IPFS
- Peer-to-peer
- Database-like queries
- No central server

**Cons**:
- **Eventually consistent** - No strong consistency guarantees
- **Requires peers online** - Data availability depends on peers
- **Smaller ecosystem** - Less mature than pure IPFS
- **Query performance** - Slower than indexed databases

**Why not chosen**: Eventual consistency model not ideal for learnings. IPFS + blockchain provides stronger guarantees. Could use post-MVP for collaborative features.

### Alternative 5: Ethereum Mainnet (No L2)

**Description**: Use Ethereum mainnet for transactions instead of L2.

**Pros**:
- Maximum security
- Largest ecosystem
- Most decentralized
- Best tooling

**Cons**:
- **High gas fees** - $5-50 per transaction
- **Not scalable** - Too expensive for frequent uploads
- **Slower** - ~12 second block times
- **Prohibitive for MVP** - Cost kills adoption

**Why not chosen**: Gas fees are prohibitively expensive. L2s provide 99%+ cost reduction with minimal security trade-offs for this use case.

### Alternative 6: Celestia Only (No EVM)

**Description**: Use Celestia for both data availability and transactions.

**Pros**:
- Single layer
- Optimized for data availability
- Lower costs than Ethereum

**Cons**:
- **Not EVM-compatible** - Can't run Solidity smart contracts
- **Smaller ecosystem** - Less tooling and infrastructure
- **No smart contract layer** - Would need separate execution layer
- **Immature** - Celestia is newer, less battle-tested

**Why not chosen**: Celestia is a DA layer, not an execution layer. We need smart contracts for upload registry and future token mechanics. Could use Celestia for DA post-MVP alongside EVM L2.

## Implementation

### Upload Flow

```typescript
import { ulid } from 'ulid'; // STANDARDS.md #4

async function uploadLearning(learning: Learning): Promise<UploadResult> {
  // 1. CRITICAL: Validate learning is derived (not raw conversation)
  // Per STANDARDS.md #1 and #6 - never upload raw or unsanitized content
  if (learning.sourceType === 'raw_conversation') {
    throw new Error('PRIVACY VIOLATION: Cannot upload raw conversations');
  }

  if (!learning.isDerived) {
    throw new Error('PRIVACY VIOLATION: Only derived learnings can be uploaded');
  }

  // Verify no chain-of-thought content (STANDARDS.md #6)
  if (learning.containsChainOfThought) {
    throw new Error('POLICY VIOLATION: Cannot upload chain-of-thought content');
  }

  // 2. Convert to IPLD format (all learnings are already sanitized/derived)
  const ipldDoc = formatAsIPLD(learning);

  // 3. Upload to IPFS
  const ipfsClient = create({ url: 'http://localhost:5001' });
  const { cid } = await ipfsClient.add(JSON.stringify(ipldDoc));

  // 4. Pin to pinning service (backup)
  await pinToPinata(cid.toString());

  // 5. Generate metadata hash
  const metadataHash = keccak256(JSON.stringify({
    title: learning.title,
    summary: learning.summary,
    created: learning.created
  }));

  // 6. Record on Base L2
  const contract = new ethers.Contract(
    LEARNING_REGISTRY_ADDRESS,
    LEARNING_REGISTRY_ABI,
    signer
  );

  const tx = await contract.uploadLearning(
    cid.toString(),
    metadataHash,
    'CC-BY-4.0'
  );

  await tx.wait();

  // 7. Record upload in local database
  // Use ULID for ID (STANDARDS.md #4) and ISO-8601 timestamp (STANDARDS.md #7)
  await db.uploads.insert({
    id: ulid(),
    learning_id: learning.id,
    ipfs_cid: cid.toString(),
    blockchain_tx: tx.hash,
    blockchain_timestamp: tx.blockNumber, // Block timestamp from chain
    status: 'completed',
    created_at: new Date().toISOString() // ISO-8601 local timestamp
  });

  return {
    ipfsCid: cid.toString(),
    txHash: tx.hash,
    publicUrl: `https://ipfs.io/ipfs/${cid.toString()}`
  };
}
```

### Query Flow

```typescript
async function queryLearning(ipfsCid: string): Promise<Learning> {
  // 1. Verify on blockchain
  const contract = new ethers.Contract(
    LEARNING_REGISTRY_ADDRESS,
    LEARNING_REGISTRY_ABI,
    provider
  );

  const upload = await contract.getLearning(ipfsCid);

  if (!upload.ipfsCid) {
    throw new Error('Learning not found on blockchain');
  }

  // 2. Fetch from IPFS (try multiple gateways)
  const gateways = [
    `https://ipfs.io/ipfs/${ipfsCid}`,
    `https://gateway.pinata.cloud/ipfs/${ipfsCid}`,
    `https://${ipfsCid}.ipfs.dweb.link`
  ];

  for (const gateway of gateways) {
    try {
      const response = await fetch(gateway, { timeout: 5000 });
      const learning = await response.json();

      // 3. Verify metadata hash
      const computedHash = keccak256(JSON.stringify({
        title: learning.title,
        summary: learning.summary,
        created: learning.created
      }));

      if (computedHash !== upload.metadataHash) {
        throw new Error('Metadata hash mismatch');
      }

      return learning;
    } catch (err) {
      console.warn(`Gateway ${gateway} failed, trying next...`);
    }
  }

  throw new Error('All IPFS gateways failed');
}
```

### Consent and Manual Approval

**CRITICAL** (STANDARDS.md #16 - Consent & Licensing):
- **Default mode: Local-only** (no uploads)
- **Opt-in required**: User must explicitly enable global sharing
- **Manual approval gate**: Each upload requires confirmation (MVP)
- All uploads require explicit user consent with preview

```typescript
interface UploadApproval {
  learningId: string;
  preview: string;        // What will be uploaded
  ipfsCid?: string;       // Computed CID (deterministic)
  estimatedGasCost: string; // "~$0.01"
  license: string;        // "CC-BY-4.0"
  permanent: boolean;     // true (cannot delete)
}

async function requestUploadApproval(
  learning: Learning
): Promise<UploadApproval> {
  // Generate preview
  const preview = `
Title: ${learning.title}
Summary: ${learning.summary}
Tags: ${learning.tags.join(', ')}
License: CC-BY-4.0
Permanent: Yes (cannot be deleted once uploaded)
  `;

  // Compute CID deterministically
  const ipldDoc = formatAsIPLD(learning);
  const cid = await computeCID(JSON.stringify(ipldDoc));

  // Estimate gas cost
  const gasEstimate = await estimateGasCost();

  return {
    learningId: learning.id,
    preview,
    ipfsCid: cid,
    estimatedGasCost: `~$${(gasEstimate / 1e18).toFixed(4)}`,
    license: 'CC-BY-4.0',
    permanent: true
  };
}

// User must explicitly approve via MCP or CLI
// This is MANDATORY per STANDARDS.md #16 (not optional)
async function approveUpload(approval: UploadApproval): Promise<void> {
  // Confirmation dialog must be shown
  const confirmed = await showConfirmationDialog({
    title: "Upload Learning to Global Network",
    message: approval.preview,
    warning: "⚠️  This upload is PERMANENT and cannot be deleted",
    license: "CC-BY-4.0 (Attribution required)",
    cost: approval.estimatedGasCost,
    buttons: ["Cancel", "Upload Permanently"]
  });

  if (!confirmed) {
    // User declined - content remains local only
    throw new Error('Upload cancelled by user');
  }

  // User confirmed - trigger the actual upload
  await uploadLearning(approval.learningId);
}
```

### Pinning Service Integration

```typescript
interface PinningService {
  pin(cid: string): Promise<void>;
  unpin(cid: string): Promise<void>;
  status(cid: string): Promise<PinStatus>;
}

class PinataService implements PinningService {
  async pin(cid: string): Promise<void> {
    const response = await fetch('https://api.pinata.cloud/pinning/pinByHash', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.PINATA_JWT}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ hashToPin: cid })
    });

    if (!response.ok) {
      throw new Error(`Pinning failed: ${response.statusText}`);
    }
  }

  async status(cid: string): Promise<PinStatus> {
    const response = await fetch(
      `https://api.pinata.cloud/data/pinList?hashContains=${cid}`,
      {
        headers: {
          'Authorization': `Bearer ${process.env.PINATA_JWT}`
        }
      }
    );

    const data = await response.json();
    return data.rows.length > 0 ? 'pinned' : 'unpinned';
  }
}
```

### Revocation List (Immutability Mitigation)

**Per STANDARDS.md #16 (Consent & Licensing)**: Since IPFS and blockchain are immutable, we use a local revocation list for logical deletion. This provides right-to-delete compliance while respecting the immutability of decentralized storage.

```typescript
// Revocations table
interface Revocation {
  id: string;
  ipfs_cid: string;
  reason: string; // "user-requested" | "privacy-violation" | "quality-issue"
  revoked_at: string;
}

// Check before returning query results
async function isRevoked(ipfsCid: string): Promise<boolean> {
  const revocation = await db.revocations.findOne({ ipfs_cid: ipfsCid });
  return revocation !== null;
}

// MCP queries filter out revoked learnings (STANDARDS.md #16)
// This provides "logical deletion" while respecting blockchain immutability
async function searchLearnings(query: string): Promise<Learning[]> {
  const results = await fetchFromIPFS(query);

  // Filter out revoked content (user-requested deletions)
  const filtered = [];
  for (const result of results) {
    if (!await isRevoked(result.ipfsCid)) {
      filtered.push(result);
    }
  }

  return filtered;
}

// Note: Content remains on IPFS/blockchain but our application excludes it
// This balances immutability with user privacy rights
```

### Future Celestia Integration

Post-MVP path to add Celestia:

```typescript
// Phase 1: IPFS + Base (MVP)
upload → IPFS → Base contract

// Phase 2: Add Celestia DA layer
upload → IPFS (content) → Celestia (DA blob) → Base contract (CID + Celestia commitment)

// Celestia stores blob commitment, Base stores both CIDs
interface UploadV2 {
  ipfsCid: string;
  celestiaCommitment: string; // Celestia blob commitment
  celestiaHeight: number;     // Block height
  baseContractTx: string;     // Base L2 transaction
}
```

## Risks and Mitigations

### Risk: Immutability vs Right-to-Delete (GDPR)

**Impact**: High - Legal compliance issue

**Mitigation**:
- Only upload derived learnings (never raw conversations)
- Explicit consent with "permanent upload" warning (STANDARDS.md #16)
- Revocation list for logical deletion (application-level filtering)
- License clarity (CC-BY-4.0 = permissive but attributed)
- Document that learnings are privacy-safe by design
- **Legal consultation recommended** before production deployment for GDPR/CCPA validation
- **Post-MVP**: Consider legal review of revocation list approach for compliance certification

### Risk: Pinning Service Failure

**Impact**: Medium - Content becomes inaccessible

**Mitigation**:
- Use multiple pinning services (Pinata + Web3.Storage)
- Local IPFS node as primary
- Monitor pin status
- Alert if unpinned
- Re-pin automatically if detected

### Risk: Base L2 Gas Spikes

**Impact**: Low - Upload costs increase

**Mitigation**:
- Base historically very stable (~$0.01)
- Batch uploads if needed
- Queue uploads during low-fee periods
- Switch to cheaper L2 if sustained spike
- Most uploads are async (can wait)

### Risk: IPFS Gateway Downtime

**Impact**: Medium - Queries fail

**Mitigation**:
- Multi-gateway fallback strategy
- Local IPFS node for queries
- Cache frequently accessed learnings
- Retry logic with exponential backoff
- Monitor gateway health

### Risk: Smart Contract Bugs

**Impact**: High - Could lose provenance data

**Mitigation**:
- Minimal contract (simple upload registry)
- Audit before mainnet deployment
- Use OpenZeppelin libraries
- Testnet validation
- Upgrade mechanism (proxy pattern)
- Limited funds at risk (no token treasury in MVP)

### Risk: Privacy Violation (Uploading Raw Data)

**Impact**: Critical - Core privacy guarantee violated

**Mitigation**:
- Code-level validation (reject raw conversations)
- Manual approval shows preview
- Audit logs of all uploads
- Canary tests (attempt to upload PII, should fail)
- Clear documentation and warnings

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Learning Extraction](../architecture/architecture-learning-extraction-2025-01-16.md)

### Decisions
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md)
- [ADR-006: Async Processing Model](./decision-async-processing-model-2025-01-16.md)

### Plans
- [Original User Vision](../plans/plan-original-user-vision-2025-01-16.md)

### Standards
- [Section 15: Blockchain Standard](../STANDARDS.md#15-blockchain-standard-clarified)

### Reviews
- [GPT-5 Holistic Review](../reviews/gpt5-holistic-review-2025-01-16.txt)

---

## Implementation Checklist

- [ ] Set up local IPFS node
- [ ] Configure Pinata/Web3.Storage accounts
- [ ] Deploy LearningRegistry contract to Base testnet
- [ ] Test upload flow end-to-end
- [ ] Implement multi-gateway query fallback
- [ ] Add upload approval UI/CLI
- [ ] Create revocation list mechanism
- [ ] Write integration tests
- [ ] Document deployment process
- [ ] Audit contract (post-MVP, pre-mainnet)

---

*This ADR addresses the external review feedback noting the missing decision on IPFS + blockchain architecture. It clarifies the separation between Celestia (DA layer) and EVM chains (smart contracts), specifies Base as the concrete L2 choice, and ensures only derived learnings are uploaded to maintain privacy guarantees.*
````

## File: decisions/decision-observability-cost-slo-governance-2025-01-16.md
````markdown
---
title: "ADR-013: Observability, Cost Management, and SLO Governance"
category: decision
date: 2025-01-16
status: ready-for-implementation
authors: System Architecture Team
reviewers: [GPT-5]
tags: [observability, performance, cost-management, slo, telemetry, privacy]
related:
  - ../STANDARDS.md
  - ./decision-sanitize-before-storage-2025-01-16.md
  - ../architecture/architecture-hooks-event-capture-2025-01-16.md
  - ../reviews/gpt5-holistic-review-2025-01-16.txt
  - ../reviews/gpt5-adr013-review-2025-01-16.txt
  - ../reviews/gemini-holistic-review-2025-01-16.txt
---

# ADR-013: Observability, Cost Management, and SLO Governance

## Status

**READY FOR IMPLEMENTATION** - All GPT-5 priority fixes integrated (2025-01-16)

## Context

### Business Need

The Global Context Network relies on external Claude API calls for sanitization validation and learning extraction. Without proper monitoring, cost controls, and SLO enforcement:

1. **Runaway costs**: Uncontrolled API usage could exhaust budgets
2. **Performance degradation**: No visibility into SLO violations
3. **Operational blindness**: Cannot diagnose failures or bottlenecks
4. **User experience risk**: Slow responses or failures go undetected

### Review Feedback

External reviews identified critical gaps:

**GPT-5 Review** (lines 66-67):
> "Add a brief operational runbook (how to restart workers, recover queues, apply migrations, backup/restore) and a **cost/SLO budget table for Claude API use**."

**Gemini 2.5 Pro Review** (line 90):
> "**Cost Analysis & Management:** There's no plan for monitoring or controlling the operational costs of the Claude API calls, which will be substantial for sanitization and learning extraction."

### Requirements

1. **Privacy-safe telemetry**: No PII in logs or metrics
2. **Cost control**: Budget caps with graceful degradation
3. **SLO enforcement**: Meet performance targets from STANDARDS.md
4. **Local-first**: Telemetry stored locally by default
5. **Opt-in sharing**: Anonymous metrics sharing only with consent
6. **Actionable**: Metrics must enable debugging and optimization

## Decision

We will implement a **privacy-first observability stack** with cost controls and SLO enforcement:

### 1. Privacy-Safe Telemetry

#### Structured Logging (JSON)

**Format**:
```typescript
interface LogEntry {
  timestamp: string;        // ISO-8601 UTC
  level: 'debug' | 'info' | 'warn' | 'error';
  component: string;        // 'hook' | 'sanitizer' | 'learner' | 'worker' | 'mcp'
  event: string;            // 'message_captured' | 'sanitization_complete' | etc.
  correlation_id: string;   // ULID for request tracing
  duration_ms?: number;     // For performance tracking
  metadata: Record<string, unknown>;  // NEVER contains PII
}
```

**Privacy Rules** (Allowlist-Based):
- ✅ **DO log**: IDs, durations, counts, status codes, component names
- ✅ **DO log**: Sanitized content (if debugging enabled, user opt-in, with sampling/short retention)
- ❌ **NEVER log**: Raw content before sanitization
- ❌ **NEVER log**: PII (emails, paths, API keys, etc.)
- ❌ **NEVER log**: Chain-of-thought or hidden reasoning

**Allowlist-Based Event Schemas**:
```typescript
// Define strict allowed fields per event type
const EVENT_SCHEMAS = {
  message_captured: {
    allowed_keys: ['correlation_id', 'conversation_id', 'message_id', 'role', 'content_length', 'timestamp'],
    required_keys: ['correlation_id', 'message_id', 'role']
  },
  sanitization_complete: {
    allowed_keys: ['correlation_id', 'message_id', 'redaction_count', 'duration_ms', 'timestamp'],
    required_keys: ['correlation_id', 'message_id', 'redaction_count', 'duration_ms']
  },
  api_call_complete: {
    allowed_keys: ['correlation_id', 'operation', 'model', 'input_tokens', 'output_tokens', 'cost_cents', 'duration_ms', 'timestamp'],
    required_keys: ['correlation_id', 'operation', 'model', 'input_tokens', 'output_tokens', 'cost_cents']
  }
  // ... other event types
};

// Runtime validation with strict allowlist
function validateMetadata(event: string, metadata: Record<string, unknown>): Record<string, unknown> {
  const schema = EVENT_SCHEMAS[event];
  if (!schema) {
    throw new Error(`Unknown event type: ${event}`);
  }

  const validated: Record<string, unknown> = {};

  // Only include allowed keys
  for (const key of schema.allowed_keys) {
    if (key in metadata) {
      validated[key] = metadata[key];
    }
  }

  // Ensure required keys are present
  for (const key of schema.required_keys) {
    if (!(key in validated)) {
      throw new Error(`Missing required key '${key}' for event '${event}'`);
    }
  }

  // Reject any keys not in allowlist
  const extraKeys = Object.keys(metadata).filter(k => !schema.allowed_keys.includes(k));
  if (extraKeys.length > 0) {
    logger.warn('Rejected disallowed keys in metadata', { event, rejected_keys: extraKeys });
  }

  return validated;
}
```

**Example**:
```typescript
logger.info('Message sanitized', {
  correlation_id: '01HQZX...',
  conversation_id: '01HQZY...',
  message_id: '01HQZZ...',
  role: 'user',
  content_length: 1247,
  redaction_count: 3,
  duration_ms: 42
});
```

#### Correlation IDs and Context Propagation

**Purpose**: Trace requests across components

**Implementation**:
- Generate ULID **once** at hook entry point using `AsyncLocalStorage`
- Automatically propagate through all downstream calls without manual passing
- Include in all log entries and metrics for the request
- Enables end-to-end tracing: hook → queue → worker → API → storage
- Support nested spans with `parent_span_id` for detailed tracing

**AsyncLocalStorage Pattern**:
```typescript
import { AsyncLocalStorage } from 'node:async_hooks';

interface RequestContext {
  correlation_id: string;
  parent_span_id?: string;
  operation?: string;
}

const correlationContext = new AsyncLocalStorage<RequestContext>();

// Set correlation context at hook entry
export function withCorrelation<T>(fn: () => T): T {
  const context: RequestContext = {
    correlation_id: ulid()
  };
  return correlationContext.run(context, fn);
}

// Retrieve correlation ID anywhere without passing through signatures
export function getCorrelationId(): string | undefined {
  return correlationContext.getStore()?.correlation_id;
}

// Usage in hook
export const onMessage = withCorrelation((message) => {
  logger.info('Message received', {
    correlation_id: getCorrelationId(), // Automatically available
    message_id: message.id
  });
  // All downstream calls automatically have access to correlation_id
});
```

**Example flow**:
```
correlation_id: 01HQZX...
  ├─ [hook] Message received (0ms)
  ├─ [hook] Pre-sanitization complete (42ms)
  ├─ [hook] Persisted to messages table (58ms)
  ├─ [worker] AI validation started (1.2s)
  ├─ [worker] Claude API call complete (2.8s)
  └─ [worker] Sanitization log updated (2.85s)
```

#### Metrics Storage

**Local-Only by Default**:
- Store in SQLite: `metrics.db`
- **Encryption**: Use SQLCipher or OS-level encryption for privacy
- **File Permissions**: Create with mode 0600 (owner-only read/write)
- **Storage Location**: OS-specific app data directory
  - macOS: `~/Library/Application Support/gcn/metrics.db`
  - Linux: `~/.local/share/gcn/metrics.db`
  - Windows: `%APPDATA%\gcn\metrics.db`

- **Retention Policy**: Auto-prune logs after 30 days (configurable)
- Schema:
  ```sql
  CREATE TABLE performance_metrics (
    id TEXT PRIMARY KEY,
    timestamp TEXT NOT NULL,
    component TEXT NOT NULL,
    metric_name TEXT NOT NULL,
    value REAL NOT NULL,
    tags TEXT,  -- JSON object (low-cardinality only)
    created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
  );

  CREATE INDEX idx_metrics_component_time
    ON performance_metrics(component, timestamp);

  -- Logs table with retention
  CREATE TABLE logs (
    id TEXT PRIMARY KEY,
    timestamp TEXT NOT NULL,
    level TEXT NOT NULL,
    component TEXT NOT NULL,
    event TEXT NOT NULL,
    correlation_id TEXT NOT NULL,
    metadata TEXT,  -- JSON object, validated against allowlist
    created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
  );

  CREATE INDEX idx_logs_correlation
    ON logs(correlation_id);
  CREATE INDEX idx_logs_timestamp
    ON logs(timestamp);
  ```

**SQLite Configuration for Performance**:
```typescript
// Pragmas for <5ms impact target
db.pragma('journal_mode = WAL');
db.pragma('synchronous = NORMAL');
db.pragma('cache_size = -64000'); // 64MB cache
db.pragma('temp_store = MEMORY');

// Batch log writes every 100ms
const logBatchQueue: LogEntry[] = [];
setInterval(() => {
  if (logBatchQueue.length > 0) {
    const batch = [...logBatchQueue];
    logBatchQueue.length = 0;

    const insert = db.prepare(`
      INSERT INTO logs (id, timestamp, level, component, event, correlation_id, metadata)
      VALUES (?, ?, ?, ?, ?, ?, ?)
    `);

    const insertMany = db.transaction((entries: LogEntry[]) => {
      for (const entry of entries) {
        insert.run(
          ulid(),
          entry.timestamp,
          entry.level,
          entry.component,
          entry.event,
          entry.correlation_id,
          JSON.stringify(entry.metadata)
        );
      }
    });

    insertMany(batch);
  }
}, 100);
```

**Retention and Compaction**:
```typescript
// Daily cleanup job
function pruneOldLogs(db: Database.Database, retentionDays: number = 30): void {
  const cutoffDate = new Date();
  cutoffDate.setDate(cutoffDate.getDate() - retentionDays);

  db.prepare('DELETE FROM logs WHERE timestamp < ?').run(cutoffDate.toISOString());
  db.prepare('DELETE FROM performance_metrics WHERE timestamp < ?').run(cutoffDate.toISOString());

  // Compact database
  db.pragma('vacuum');
  db.pragma('optimize');
}

// Run daily at 3 AM
schedule.daily('03:00', () => pruneOldLogs(db));
```

**Opt-In Anonymous Sharing**:
- User must explicitly enable in config
- Only aggregate metrics (no correlation IDs, no logs)
- Future: Upload to privacy-preserving analytics service
- MVP: Local-only (no sharing implemented)

### 2. Service Level Objectives (SLOs)

Enforced SLOs from [STANDARDS.md Section 12](../STANDARDS.md#12-performance-budgets):

| Component | SLO (p95) | Measurement Point | Alerting Threshold |
|-----------|-----------|-------------------|-------------------|
| Hook execution | <100ms | Event received → DB persist complete | >110ms for 5 consecutive events |
| Fast sanitization | <50ms | Pre-sanitization regex execution | >60ms for 5 consecutive events |
| Database writes | <20ms | WAL insert operation | >25ms for 10 consecutive writes |
| Database queries | <100ms | MCP query handler (search + serialize) | >120ms for 5 consecutive queries |
| MCP queries | <200ms | MCP request received → response sent | >220ms for 5 consecutive requests |
| AI sanitization | <2s | Claude API call for validation | >2.5s for 3 consecutive calls |
| Learning extraction | <5s | Claude API call for insights | >6s for 3 consecutive calls |

#### SLO Tracking

**Instrumentation**:
```typescript
import { performance } from 'node:perf_hooks';

// Sliding-window percentile tracking
class SlidingWindowTracker {
  private windows = new Map<string, number[]>();
  private readonly windowSize = 1000; // Last 1000 samples
  private readonly maxWindowDuration = 3600_000; // 1 hour in ms

  record(operation: string, durationMs: number): void {
    const key = operation;
    if (!this.windows.has(key)) {
      this.windows.set(key, []);
    }
    const window = this.windows.get(key)!;
    window.push(durationMs);

    // Keep only last N samples
    if (window.length > this.windowSize) {
      window.shift();
    }
  }

  getPercentiles(operation: string): { p50: number; p95: number; p99: number } | null {
    const window = this.windows.get(operation);
    if (!window || window.length === 0) return null;

    const sorted = [...window].sort((a, b) => a - b);
    const p50 = sorted[Math.floor(sorted.length * 0.50)];
    const p95 = sorted[Math.floor(sorted.length * 0.95)];
    const p99 = sorted[Math.floor(sorted.length * 0.99)];

    return { p50, p95, p99 };
  }

  checkSLOViolation(operation: string, sloThresholdMs: number): boolean {
    const percentiles = this.getPercentiles(operation);
    return percentiles ? percentiles.p95 > sloThresholdMs : false;
  }
}

const sloTracker = new SlidingWindowTracker();

async function trackPerformance<T>(
  component: string,
  operation: string,
  fn: () => Promise<T>
): Promise<T> {
  const correlationId = getCorrelationId(); // Use AsyncLocalStorage context
  const start = performance.now();

  try {
    const result = await fn();
    const duration = performance.now() - start;

    // Record in sliding window
    const operationKey = `${component}.${operation}`;
    sloTracker.record(operationKey, duration);

    // Record raw metric for reporting
    metrics.record({
      component,
      metric: `${operation}_duration_ms`,
      value: duration,
      correlation_id: correlationId
    });

    // Check SLO violation based on p95 across sliding window
    const slo = SLO_THRESHOLDS[component][operation];
    const percentiles = sloTracker.getPercentiles(operationKey);

    if (percentiles && percentiles.p95 > slo) {
      logger.warn('SLO p95 violation', {
        component,
        operation,
        current_duration_ms: duration,
        window_p50: percentiles.p50,
        window_p95: percentiles.p95,
        window_p99: percentiles.p99,
        slo_ms: slo,
        correlation_id: correlationId
      });
    }

    return result;
  } catch (error) {
    const duration = performance.now() - start;
    logger.error('Operation failed', {
      component,
      operation,
      duration_ms: duration,
      error: error.message,
      correlation_id: correlationId
    });
    throw error;
  }
}
```

**SLO Reporting**:
- Sliding-window percentiles (p50, p95, p99) computed over last 1000 samples or 1 hour
- Daily summary aggregates for each component/operation
- Percentile-based violation detection (not single-call warnings)
- Per-component breakdown with trend analysis
- Stored in local metrics database with periodic aggregates

### 3. Cost Management

#### LLM API Budget Controls

**Budget Structure** (Integer Cents Schema):
```typescript
interface CostBudget {
  daily_limit_cents: number;      // Integer cents to avoid float drift
  monthly_limit_cents: number;
  per_operation_limit_cents: number;
  current_daily_spend_cents: number;
  current_monthly_spend_cents: number;
  budget_period_start: string;    // ISO-8601
  last_reset_timestamp: string;   // ISO-8601, for startup reconciliation
}
```

**SQLite Schema**:
```sql
CREATE TABLE budgets (
  id INTEGER PRIMARY KEY,
  period TEXT NOT NULL,                    -- 'daily' | 'monthly'
  daily_limit_cents INTEGER NOT NULL,
  monthly_limit_cents INTEGER NOT NULL,
  per_operation_limit_cents INTEGER NOT NULL,
  current_daily_spend_cents INTEGER NOT NULL DEFAULT 0,
  current_monthly_spend_cents INTEGER NOT NULL DEFAULT 0,
  period_start TEXT NOT NULL,              -- ISO-8601
  last_reset_timestamp TEXT NOT NULL,
  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
  updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE api_call_metrics (
  id TEXT PRIMARY KEY,
  timestamp TEXT NOT NULL,
  operation TEXT NOT NULL,                 -- 'sanitization' | 'learning'
  model TEXT NOT NULL,
  input_tokens INTEGER NOT NULL,
  output_tokens INTEGER NOT NULL,
  cost_usd_cents INTEGER NOT NULL,         -- Integer cents
  correlation_id TEXT,
  status TEXT NOT NULL,                    -- 'success' | 'error' | 'cancelled'
  estimated_cost_cents INTEGER,            -- Pre-call estimation
  idempotency_key TEXT UNIQUE,             -- For retry safety
  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_api_calls_operation_time
  ON api_call_metrics(operation, timestamp);
CREATE INDEX idx_api_calls_correlation
  ON api_call_metrics(correlation_id);
CREATE INDEX idx_api_calls_idempotency
  ON api_call_metrics(idempotency_key);
```

**Default Budgets (MVP)**:
```typescript
const DEFAULT_BUDGETS = {
  daily_limit_cents: 500,          // $5.00 - Conservative for MVP
  monthly_limit_cents: 10000,      // $100.00 - ~20 days of full usage
  per_operation_limit_cents: 50,   // $0.50 - Safety per API call
  sanitization_per_call_cents: 10, // $0.10 - ~1000 tokens avg
  learning_per_call_cents: 25      // $0.25 - ~2500 tokens avg
};
```

#### Cost Tracking and Pre-Call Estimation

**Per-Operation Metering**:
```typescript
interface ApiCallMetrics {
  operation: 'sanitization' | 'learning';
  model: string;
  input_tokens: number;
  output_tokens: number;
  cost_usd_cents: number;  // Integer cents
  timestamp: string;
  correlation_id: string;
  estimated_cost_cents: number;  // Pre-call estimation
  idempotency_key: string;       // correlation_id-derived for retry safety
  status: 'success' | 'error' | 'cancelled';
}
```

**Model-Aware Pricing Configuration**:
```typescript
// Pricing per model (versioned, with last updated timestamp)
interface ModelPricing {
  model: string;
  input_per_mtok_cents: number;   // Integer cents per million input tokens
  output_per_mtok_cents: number;  // Integer cents per million output tokens
  last_updated: string;           // ISO-8601
  version: string;
}

const PRICING_CONFIG: ModelPricing[] = [
  {
    model: 'claude-sonnet-4-5',
    input_per_mtok_cents: 300,    // $3.00 per million input tokens
    output_per_mtok_cents: 1500,  // $15.00 per million output tokens
    last_updated: '2025-01-16',
    version: '1.0'
  }
  // Support multiple models and hot reload
];

function getPricing(model: string): ModelPricing {
  const pricing = PRICING_CONFIG.find(p => p.model === model);
  if (!pricing) {
    throw new Error(`No pricing configured for model: ${model}`);
  }
  return pricing;
}
```

**Pre-Call Cost Estimation**:
```typescript
function estimateTokens(content: string, operation: 'sanitization' | 'learning'): {
  input: number;
  output: number;
} {
  // Rough estimation: 1 token ≈ 4 characters
  const contentTokens = Math.ceil(content.length / 4);

  // Add system prompt tokens based on operation
  const systemPromptTokens = operation === 'sanitization' ? 200 : 500;
  const inputTokens = contentTokens + systemPromptTokens;

  // Estimate output tokens based on operation
  const outputTokens = operation === 'sanitization'
    ? Math.ceil(contentTokens * 1.1)  // Similar length + small additions
    : Math.ceil(contentTokens * 0.3); // Summary/insights

  return { input: inputTokens, output: outputTokens };
}

function calculateCost(input: number, output: number, model: string): number {
  const pricing = getPricing(model);
  const inputCostCents = Math.ceil((input / 1_000_000) * pricing.input_per_mtok_cents);
  const outputCostCents = Math.ceil((output / 1_000_000) * pricing.output_per_mtok_cents);
  return inputCostCents + outputCostCents;
}
```

**Atomic Budget Reservation with Pre-Call Enforcement**:
```typescript
import Database from 'better-sqlite3';

async function enforceApiCallBudget(
  db: Database.Database,
  operation: 'sanitization' | 'learning',
  content: string,
  model: string
): Promise<{ estimatedCostCents: number; idempotencyKey: string }> {
  // Estimate cost before API call
  const { input, output } = estimateTokens(content, operation);
  const estimatedCostCents = calculateCost(input, output, model);

  // Generate idempotency key from correlation ID
  const correlationId = getCorrelationId();
  const idempotencyKey = `${correlationId}-${operation}`;

  // Atomic budget reservation in transaction
  const reservation = db.transaction(() => {
    // Check for idempotent retry
    const existing = db.prepare(
      'SELECT cost_usd_cents, status FROM api_call_metrics WHERE idempotency_key = ?'
    ).get(idempotencyKey);

    if (existing) {
      // Already processed this call
      if (existing.status === 'success') {
        throw new Error('Request already processed successfully');
      }
      // Allow retry for errors
      return { estimatedCostCents: existing.cost_usd_cents, idempotencyKey };
    }

    // Load current budget
    const budget = db.prepare('SELECT * FROM budgets WHERE id = 1').get() as CostBudget;

    // Check if adding estimated cost exceeds limits
    if (budget.current_daily_spend_cents + estimatedCostCents > budget.daily_limit_cents) {
      throw new BudgetExceededError('Daily budget would be exceeded');
    }

    if (budget.current_monthly_spend_cents + estimatedCostCents > budget.monthly_limit_cents) {
      throw new BudgetExceededError('Monthly budget would be exceeded');
    }

    if (estimatedCostCents > budget.per_operation_limit_cents) {
      throw new BudgetExceededError('Per-operation budget would be exceeded');
    }

    // Reserve estimated budget atomically (BEGIN IMMEDIATE transaction)
    db.prepare(`
      UPDATE budgets
      SET current_daily_spend_cents = current_daily_spend_cents + ?,
          current_monthly_spend_cents = current_monthly_spend_cents + ?,
          updated_at = CURRENT_TIMESTAMP
      WHERE id = 1
    `).run(estimatedCostCents, estimatedCostCents);

    // Record reservation
    db.prepare(`
      INSERT INTO api_call_metrics (
        id, timestamp, operation, model, input_tokens, output_tokens,
        cost_usd_cents, correlation_id, status, estimated_cost_cents, idempotency_key
      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, 'reserved', ?, ?)
    `).run(
      ulid(),
      new Date().toISOString(),
      operation,
      model,
      input,
      output,
      estimatedCostCents,
      correlationId,
      estimatedCostCents,
      idempotencyKey
    );

    return { estimatedCostCents, idempotencyKey };
  })();

  return reservation;
}

// Post-call reconciliation
async function reconcileApiCallCost(
  db: Database.Database,
  idempotencyKey: string,
  actualInputTokens: number,
  actualOutputTokens: number,
  model: string,
  status: 'success' | 'error'
): Promise<void> {
  const actualCostCents = calculateCost(actualInputTokens, actualOutputTokens, model);

  db.transaction(() => {
    const record = db.prepare(
      'SELECT estimated_cost_cents FROM api_call_metrics WHERE idempotency_key = ?'
    ).get(idempotencyKey) as { estimated_cost_cents: number };

    const deltaCents = actualCostCents - record.estimated_cost_cents;

    // Update actual cost
    db.prepare(`
      UPDATE api_call_metrics
      SET input_tokens = ?,
          output_tokens = ?,
          cost_usd_cents = ?,
          status = ?
      WHERE idempotency_key = ?
    `).run(actualInputTokens, actualOutputTokens, actualCostCents, status, idempotencyKey);

    // Adjust budget for delta (credit if overestimated, debit if underestimated)
    db.prepare(`
      UPDATE budgets
      SET current_daily_spend_cents = current_daily_spend_cents + ?,
          current_monthly_spend_cents = current_monthly_spend_cents + ?
      WHERE id = 1
    `).run(deltaCents, deltaCents);
  })();
}
```

#### Graceful Degradation and Privacy Safeguards

When budget is exhausted:

**Sanitization**:
- Fall back to **rules-only mode** (regex pre-sanitization)
- Skip AI validation
- Log budget exhaustion event
- User notification: "AI-enhanced sanitization paused (budget limit)"
- **Privacy safeguard**: Continue using strict static rules

**Learning Extraction**:
- Queue jobs as `paused` status
- Do not call Claude API
- Resume when budget resets (daily/monthly)
- User notification: "Learning extraction paused (budget limit)"
- **Privacy safeguard for decentralized uploads**:
  - When AI validation is disabled, hold uploads to decentralized network
  - Require manual approval OR pass stricter static policy (whitelist patterns)
  - Only allow uploads that pass high-confidence static analysis
  - Prevent privacy risk from unvalidated content reaching network

**Budget Reset with Clock Skew Handling**:
- Daily: Reset at midnight UTC
- Monthly: Reset on 1st of month
- **Startup reconciliation**: Check if reset should have occurred during downtime
- **Persist reset timestamps**: Track `last_reset_timestamp` in budgets table
- **Dry-run projection**: CLI command to project daily cost based on current usage
- User can manually increase limits via config

**Budget Notification Thresholds**:
```typescript
const BUDGET_WARNING_THRESHOLDS = [0.80, 0.90, 1.00]; // 80%, 90%, 100%

function checkBudgetWarnings(budget: CostBudget): void {
  const dailyUsage = budget.current_daily_spend_cents / budget.daily_limit_cents;
  const monthlyUsage = budget.current_monthly_spend_cents / budget.monthly_limit_cents;

  for (const threshold of BUDGET_WARNING_THRESHOLDS) {
    if (dailyUsage >= threshold && !hasNotified('daily', threshold)) {
      notify(`Daily budget ${threshold * 100}% exhausted`);
      markNotified('daily', threshold);
    }
    if (monthlyUsage >= threshold && !hasNotified('monthly', threshold)) {
      notify(`Monthly budget ${threshold * 100}% exhausted`);
      markNotified('monthly', threshold);
    }
  }
}
```

### 4. Telemetry Implementation

#### Logger Interface

```typescript
interface Logger {
  debug(message: string, metadata?: Record<string, unknown>): void;
  info(message: string, metadata?: Record<string, unknown>): void;
  warn(message: string, metadata?: Record<string, unknown>): void;
  error(message: string, metadata?: Record<string, unknown>): void;
}

// Privacy-aware implementation with allowlist validation
class PrivacyLogger implements Logger {
  constructor(
    private storage: LogStorage,
    private sanitizer: ContentSanitizer
  ) {}

  private validateAndSanitizeMetadata(
    event: string,
    metadata: Record<string, unknown>
  ): Record<string, unknown> {
    // Apply allowlist validation
    const validated = validateMetadata(event, metadata);

    // Additional sanitization for content fields (if opted-in)
    if ('content' in validated && typeof validated.content === 'string') {
      validated.content = this.sanitizer.sanitize(validated.content);
      validated.sanitization_version = '1.0'; // Track transform version
    }

    return validated;
  }

  info(event: string, metadata?: Record<string, unknown>): void {
    const correlationId = getCorrelationId(); // From AsyncLocalStorage

    const entry: LogEntry = {
      timestamp: new Date().toISOString(),
      level: 'info',
      component: correlationContext.getStore()?.component || 'unknown',
      event,
      correlation_id: correlationId || ulid(),
      metadata: this.validateAndSanitizeMetadata(event, metadata || {})
    };

    // Add to batch queue (flushed every 100ms)
    logBatchQueue.push(entry);
  }

  // ... other levels
}

// HTTP client redaction wrapper
class RedactedHttpClient {
  private redactSensitiveHeaders(headers: Record<string, string>): Record<string, string> {
    const redacted = { ...headers };
    const sensitiveKeys = ['authorization', 'x-api-key', 'cookie', 'set-cookie'];

    for (const key of sensitiveKeys) {
      if (key.toLowerCase() in redacted) {
        redacted[key] = '[REDACTED]';
      }
    }

    return redacted;
  }

  private redactUrl(url: string): string {
    try {
      const parsed = new URL(url);
      // Redact query parameters that might contain sensitive data
      parsed.search = parsed.search ? '[REDACTED]' : '';
      return parsed.toString();
    } catch {
      return '[INVALID_URL]';
    }
  }

  async request(url: string, options: RequestInit): Promise<Response> {
    logger.debug('http_request', {
      url: this.redactUrl(url),
      method: options.method || 'GET',
      headers: this.redactSensitiveHeaders(options.headers as Record<string, string>)
    });

    return fetch(url, options);
  }
}
```

#### Metrics Interface

```typescript
interface Metrics {
  record(metric: MetricEntry): void;
  query(filter: MetricFilter): MetricEntry[];
  getSLOReport(component: string, duration: string): SLOReport;
}

interface MetricEntry {
  component: string;
  metric: string;
  value: number;
  timestamp?: string;  // Auto-generated if omitted
  tags?: Record<string, string>;
  correlation_id?: string;
}

interface SLOReport {
  component: string;
  period: { start: string; end: string };
  metrics: {
    operation: string;
    slo_ms: number;
    p50_ms: number;
    p95_ms: number;
    p99_ms: number;
    violation_count: number;
    total_count: number;
  }[];
}
```

### 5. Operational Dashboard (Future)

**MVP**: CLI-based queries with content redaction by default
```bash
# View SLO compliance
gcn metrics slo --component hook --period 7d

# View cost summary
gcn metrics cost --period month

# View correlation trace (content redacted by default)
gcn trace 01HQZX...

# View trace with sanitized content (requires explicit flag + confirmation)
gcn trace 01HQZX... --include-content
# Prompt: "This will display sanitized content. Continue? (y/N)"

# Export metrics (never includes content by default)
gcn metrics export --format csv --output metrics.csv

# Dry-run cost projection
gcn metrics cost --dry-run --period day
```

**CLI Safety Rules**:
- **Default behavior**: Redact all content fields from output
- **Explicit opt-in**: Require `--include-content` flag for sanitized content
- **Interactive confirmation**: Prompt user in interactive sessions
- **Never raw content**: Even with flags, only show sanitized content
- **Audit trail**: Log when content is viewed via CLI

**Post-MVP**: Local web UI (privacy-safe, runs on localhost)

## Alternatives Considered

### Alternative 1: No Telemetry

**Pros**:
- Zero privacy risk
- Simplest implementation

**Cons**:
- Impossible to debug production issues
- No SLO compliance visibility
- Cannot detect cost overruns
- Cannot optimize performance

**Rejected**: Operational blindness is unacceptable for production system

### Alternative 2: Always-On Cloud Telemetry

**Pros**:
- Centralized monitoring
- Cross-user insights
- Professional tooling (Datadog, New Relic, etc.)

**Cons**:
- Privacy concerns (even with sanitization)
- Cost (monthly SaaS fees)
- External dependency
- Against "local-first" principle

**Rejected**: Conflicts with privacy-first architecture

### Alternative 3: OpenTelemetry Standard

**Pros**:
- Industry standard
- Rich ecosystem
- Future interoperability

**Cons**:
- Overkill for MVP
- Complex setup
- Large dependency footprint

**Deferred**: Consider post-MVP for standardization

## Consequences

### Positive

1. **Cost control**: Budget caps prevent runaway spending
2. **SLO compliance**: Measurable performance guarantees
3. **Privacy-safe**: No PII in telemetry by design
4. **Debuggability**: Correlation IDs enable end-to-end tracing
5. **User trust**: Local-first, opt-in sharing respects privacy
6. **Graceful degradation**: System remains functional when budget exhausted
7. **Operational visibility**: Can identify and fix bottlenecks

### Negative

1. **Implementation overhead**: Additional code for tracking
2. **Storage cost**: Metrics database grows over time (mitigated by retention policy)
3. **Performance impact**: Logging adds ~1-5ms per operation (acceptable)
4. **Complexity**: More moving parts to test and maintain

### Risks and Mitigations

| Risk | Mitigation |
|------|-----------|
| Metrics database grows unbounded | Implement 30-day retention policy with auto-pruning |
| Logging impacts hook performance | Async log writes; batch to disk every 100ms |
| Budget limits too restrictive | User-configurable; clear notifications when limits hit |
| PII leaks into logs | Sanitize all metadata; automated canary tests |
| Cost calculation drift from actual | Periodic reconciliation with Anthropic billing API |

## Operational Runbook

### Database Management

#### Rotate and Compact Databases
```bash
# Manual rotation (creates backup with timestamp)
gcn db rotate --backup

# Compact and optimize
gcn db compact

# This performs:
# 1. VACUUM to reclaim space
# 2. PRAGMA optimize
# 3. Rebuild indices
# 4. Update statistics
```

#### Backup and Restore
```bash
# Backup all databases
gcn db backup --output ~/backups/gcn-$(date +%Y%m%d).tar.gz

# Backup includes:
# - metrics.db (performance metrics, logs)
# - budgets.db (cost tracking)
# - conversations.db (sanitized messages)

# Restore from backup
gcn db restore --input ~/backups/gcn-20250116.tar.gz

# Verify backup integrity
gcn db verify --backup ~/backups/gcn-20250116.tar.gz
```

#### Handle Corrupt Database
```bash
# Detect corruption
gcn db check

# Attempt auto-recovery
gcn db recover --auto

# Manual recovery steps:
# 1. Stop all GCN processes
# 2. Create backup of corrupt database
cp metrics.db metrics.db.corrupt.$(date +%Y%m%d)

# 3. Export salvageable data
sqlite3 metrics.db ".dump" > metrics_dump.sql

# 4. Create new database from dump
sqlite3 metrics_new.db < metrics_dump.sql

# 5. Replace corrupt database
mv metrics_new.db metrics.db

# 6. Restart GCN
gcn start
```

#### Schema Migration
```bash
# Check current schema version
gcn db version

# Apply pending migrations
gcn db migrate

# Rollback last migration
gcn db migrate --rollback

# Migration safety:
# - Always create backup before migration
# - Migrations are transactional
# - Rollback support for all migrations
```

### Budget and Cost Management

#### Reset Budgets
```bash
# Manual daily reset
gcn budget reset --period daily

# Manual monthly reset
gcn budget reset --period monthly

# View current budget status
gcn budget status

# Output:
# Daily: $3.47 / $5.00 (69%)
# Monthly: $47.23 / $100.00 (47%)
# Next reset: 2025-01-17 00:00:00 UTC
```

#### Adjust Budget Limits
```bash
# Increase daily limit
gcn budget set --daily 10.00

# Increase monthly limit
gcn budget set --monthly 200.00

# Set per-operation limit
gcn budget set --per-operation 1.00

# Config file location:
# ~/.config/gcn/budget.json
```

### Queue Management

#### Inspect Stuck Queues
```bash
# View queue status
gcn queue status

# Output:
# Sanitization queue: 3 pending, 0 processing, 12 completed
# Learning queue: 47 pending (paused: budget limit), 0 processing, 8 completed

# View stuck jobs (processing > 5 minutes)
gcn queue stuck

# Retry stuck jobs
gcn queue retry --job-id 01HQZX...

# Clear failed jobs
gcn queue clear --status failed
```

#### Resume Paused Queues
```bash
# Resume learning extraction (after budget reset)
gcn queue resume --type learning

# Process specific job
gcn queue process --job-id 01HQZX...
```

### Debugging and Diagnostics

#### Enable Debug Logging
```bash
# Enable debug logging (session-only)
gcn debug enable

# Enable with content logging (requires confirmation)
gcn debug enable --include-content
# Prompt: "This will log sanitized content. Continue? (y/N)"

# Disable debug logging
gcn debug disable

# View debug logs
gcn logs --level debug --tail 100
```

#### Trace Request Flow
```bash
# Trace correlation ID end-to-end
gcn trace 01HQZX... --verbose

# Output:
# correlation_id: 01HQZX...
#   [hook] Message received (0ms)
#   [hook] Pre-sanitization complete (42ms)
#   [hook] Persisted to messages table (58ms)
#   [worker] AI validation started (1.2s)
#   [worker] Claude API call complete (2.8s)
#   [worker] Sanitization log updated (2.85s)
# Total duration: 2.85s
```

#### Check SLO Violations
```bash
# View recent violations
gcn slo violations --period 24h

# View component breakdown
gcn slo violations --component hook --period 7d

# View percentile trends
gcn slo trends --component worker --operation ai_sanitization
```

### Recovery Procedures

#### Recover from Budget Overrun
```bash
# If budget was exceeded due to race condition:

# 1. Check actual spend
gcn budget status --detailed

# 2. Reconcile with Anthropic billing (manual)
# Compare gcn spend with Anthropic dashboard

# 3. Adjust budget if needed
gcn budget set --monthly 150.00

# 4. Resume queues
gcn queue resume --all
```

#### Recover from Worker Crash
```bash
# 1. Check worker status
gcn workers status

# 2. View crash logs
gcn logs --component worker --level error --tail 50

# 3. Restart workers
gcn workers restart

# 4. Retry failed jobs
gcn queue retry --status failed
```

#### Recover from Disk Full
```bash
# 1. Check database sizes
gcn db size

# 2. Prune old logs manually
gcn logs prune --older-than 7d

# 3. Compact databases
gcn db compact

# 4. Move databases to larger volume (if needed)
gcn db relocate --path /mnt/large-volume/gcn

# 5. Update config with new location
# Edit ~/.config/gcn/config.json
```

### Performance Monitoring

#### Check Performance Impact
```bash
# View hook execution overhead
gcn metrics overhead --component hook --period 24h

# Target: <5ms p95 overhead
# If exceeded, check:
# 1. Batch logging queue size
# 2. Database write latency
# 3. Disk I/O saturation
```

#### Benchmark Database Operations
```bash
# Run performance benchmark
gcn db benchmark

# Output:
# Insert (single): 0.8ms p95
# Insert (batch 100): 12ms p95
# Query (correlation_id): 3.2ms p95
# Query (timestamp range): 45ms p95
```

## Implementation Plan

### Phase 1: Core Telemetry (Week 1)
- [ ] Implement `PrivacyLogger` with SQLite storage and allowlist validation
- [ ] Add AsyncLocalStorage for correlation ID propagation
- [ ] Implement sliding-window percentile tracking
- [ ] Instrument hooks with performance tracking
- [ ] Add encryption and file permissions

### Phase 2: Cost Tracking (Week 2)
- [ ] Implement integer cents schema and concrete tables
- [ ] Add pre-call cost estimation
- [ ] Implement atomic budget reservations with transactions
- [ ] Add idempotency keys and retry handling
- [ ] Implement graceful degradation with privacy safeguards

### Phase 3: Reporting (Week 3)
- [ ] Implement CLI commands with content redaction
- [ ] Add SLO compliance reports with percentiles
- [ ] Add cost summaries and dry-run projections
- [ ] Implement correlation trace viewer
- [ ] Add budget warning thresholds

### Phase 4: Operations and Automation (Week 4)
- [ ] Add automated retention and compaction
- [ ] Implement database backup/restore
- [ ] Create queue management commands
- [ ] Add recovery procedures
- [ ] Document operational runbook
- [ ] Add performance benchmarks

## Acceptance Criteria

- [ ] All SLOs tracked using sliding-window percentiles (p50, p95, p99)
- [ ] AsyncLocalStorage propagates correlation IDs automatically
- [ ] Allowlist-based event schemas prevent PII leaks (verified by canary tests)
- [ ] Integer cents schema prevents floating-point drift
- [ ] Pre-call cost estimation prevents budget overruns
- [ ] Atomic budget reservations handle concurrent workers
- [ ] Idempotency keys prevent double-billing on retries
- [ ] Budget enforcement with 80%/90%/100% warning thresholds
- [ ] Graceful degradation with privacy safeguards for decentralized uploads
- [ ] CLI redacts content by default, requires --include-content flag
- [ ] SQLite encrypted (SQLCipher) or OS-level encryption enforced
- [ ] File permissions set to 0600 (owner-only)
- [ ] 30-day retention with automated pruning and compaction
- [ ] Performance impact <5ms per hook execution (verified by benchmarks)
- [ ] Operational runbook covers backup/restore/recovery procedures
- [ ] HTTP client redacts sensitive headers and URLs
- [ ] Model-aware pricing with version tracking
- [ ] Startup reconciliation handles clock skew for budget resets

## Related Documents

- [STANDARDS.md - Section 12: Performance Budgets](../STANDARDS.md#12-performance-budgets)
- [STANDARDS.md - Section 14: Logging Standards](../STANDARDS.md#14-logging-standards)
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md)
- [Architecture: Hooks & Event Capture](../architecture/architecture-hooks-event-capture-2025-01-16.md)
- [GPT-5 Holistic Review](../reviews/gpt5-holistic-review-2025-01-16.txt)
- [GPT-5 ADR-013 Review](../reviews/gpt5-adr013-review-2025-01-16.txt)
- [Gemini 2.5 Pro Holistic Review](../reviews/gemini-holistic-review-2025-01-16.txt)

## Notes

**Privacy Guarantee**: This ADR maintains the zero-trust PII policy. Telemetry is designed to be privacy-safe by default, with:
- Allowlist-based event schemas (not blocklists)
- AsyncLocalStorage context propagation
- HTTP client redaction wrapper
- CLI content redaction by default
- Encrypted storage with strict file permissions
- 30-day retention with automated cleanup

**Cost Philosophy**: Budgets are intentionally conservative for MVP. Real-world usage will inform appropriate limits. Key safeguards:
- Pre-call cost estimation prevents overruns
- Atomic reservations handle concurrent workers
- Idempotency keys prevent double-billing on retries
- Model-aware pricing with version tracking
- 80%/90%/100% warning thresholds

**SLO Philosophy**: Targets are aspirational but achievable:
- Sliding-window percentiles (not single-call warnings)
- p95 allows for occasional spikes while maintaining good UX
- Measured at app-observable points (not internal DB operations)
- Platform-aware (may need different targets for Windows/slow disks)
- Violations can trigger adaptive throttling or load shedding

**Implementation Notes**:
- SQLite WAL mode with NORMAL synchronous for <5ms impact
- Batch log writes every 100ms to reduce I/O
- Low-cardinality metrics (no correlation_id in time-series)
- In-memory HDR histograms for percentile computation
- Ring buffer fallback if disk unavailable

**GPT-5 Review Integration**: This ADR incorporates all 10 priority fixes from the GPT-5 review (2025-01-16):
1. AsyncLocalStorage for correlation context propagation
2. Sliding-window percentiles for SLO tracking
3. Atomic budget reservations with SQLite transactions
4. Integer cents schema with concrete budgets/api_call_metrics tables
5. Pre-call estimation with abort-on-exceed logic
6. Allowlist validation replacing blocklist approach
7. 30-day retention policies with compaction/vacuuming
8. Encryption guidance (SQLCipher or OS-level)
9. Operational runbook with DB rotation, backup/restore, recovery
10. CLI safety with content redaction by default

**Future Work**:
- Integration with Anthropic billing API for real-time cost tracking
- Predictive budget alerts based on usage trends
- Multi-user cost allocation (post-MVP)
- OpenTelemetry adoption for standardization
- Optional anonymous metric sharing for ecosystem insights
- Automated adaptive throttling on SLO breach
- Model fallback (cheaper models when approaching budget limits)
````

## File: decisions/decision-pii-detection-strategy-2025-01-16.md
````markdown
---
title: ADR-010: PII Detection Strategy (Hybrid Layered Approach)
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [privacy, security, pii, detection, sanitization, hybrid]
---

# ADR-010: PII Detection Strategy (Hybrid Layered Approach)

## Status

Accepted

Date: 2025-01-16

Supersedes: Portions of ADR-004 (splits out detection strategy into dedicated ADR)

## Context

The Global Context Network requires accurate PII detection to enforce our zero-trust privacy guarantee (ADR-004: never persist raw data). However, PII detection faces fundamental tradeoffs:

**Accuracy vs Speed**:
- Rule-based detection is fast (<50ms) but misses contextual PII
- AI detection is accurate but slow (>1s) and expensive
- False negatives expose user privacy
- False positives reduce learning utility

**Comprehensiveness vs Maintainability**:
- Simple regex patterns are easy to maintain but miss novel formats
- Complex ML models catch more but require training data and updates
- New PII patterns emerge (new API providers, token formats)

**Utility vs Privacy**:
- Over-redaction (high false positive rate) reduces learning value
- Under-redaction (high false negative rate) violates privacy
- Context matters: "John" in "John Smith" is PII; "john" in "const john = new User()" is not

**Current Gaps** (from external review):
- ADR-004 mixes detection strategy with storage policy
- PII taxonomy incomplete (missing JWTs, env vars, SSH keys)
- No explicit accuracy targets or validation plan
- Residual false negatives acknowledged but not quantified

## Decision

Implement a **2-stage hybrid detection pipeline** that balances speed, accuracy, and utility.

### Stage 1: Fast Rule-Based Detection (Synchronous, <50ms)

**Purpose**: Catch deterministic PII patterns with zero false negatives for known types.

**Method**: Compiled regex patterns for high-confidence PII.

**Target**:
- False Positive Rate: <1% overall (acceptable over-redaction for safety)
- False Negative Rate: 0 known FN on regression suite (design target, not provable guarantee across all inputs)
- Latency: <50ms p95 (synchronous in hook)
- Category-Specific FN Targets:
  - API Keys/Tokens/SSH Keys: Near-zero FN (<0.1%, accept higher FP for maximum safety)
  - Personal Names: Balanced FN/FP (<2% FN, <5% FP to preserve utility)
  - Credit Cards: 0 known FN on test suite, mask all but last 4 digits (PCI DSS compliance)

**Runs**: In Claude Code hook (userPromptSubmit, assistantResponse) BEFORE any persistence.

### Stage 2: AI Context-Aware Validation (Async, <2s)

**Purpose**: Catch context-dependent PII that rules miss (names vs variables, novel patterns).

**Method**: LLM-powered analysis with PII taxonomy and context understanding.

**Target**:
- False Positive Rate: <5% (balanced for utility)
- False Negative Rate: <5% (residual risk acknowledged)
- Latency: <2s p95 (async job, non-blocking)

**Runs**: Downstream async job AFTER sanitized content is persisted.

### PII Taxonomy (Comprehensive)

**Tier 1: High-Confidence Deterministic** (Stage 1 only):

*API Keys and Cloud Provider Secrets:*
1. OpenAI: sk-*, pk-*
2. Anthropic: sk-ant-*
3. AWS: AKIA[0-9A-Z]{16}, aws_access_key_id patterns
4. GitHub: ghp_*, gho_*, ghs_*, ghr_*, ghu_*
5. Google API: AIza[0-9A-Za-z-_]{35}
6. Google OAuth: ya29\.* tokens
7. GCP Service Account: private_key, client_email, client_id in JSON
8. Azure SAS: sv=, sig=, se=, sp= query parameters
9. Azure Storage: 44-character base64 keys
10. Cloudflare: CF-* tokens
11. Slack: xoxb-*, xoxp-*, xoxa-*, xoxs-*, xoxe-*, Slack signing secrets
12. Stripe: sk_live_*, sk_test_*, rk_live_*, whsec_*
13. Twilio: Account SID AC[0-9a-f]{32}, Auth Token [0-9a-f]{32}
14. Hugging Face: hf_[A-Za-z0-9]{36,}
15. Supabase: anon/service keys
16. Sentry: DSN patterns
17. Vercel: vercel_* tokens
18. Notion: notion_* tokens
19. Discord: bot tokens
20. Telegram: bot tokens
21. Generic API keys: (api_key|apikey|api-key)[:=]\s*[A-Za-z0-9_-]{20,}

*Tokens and Authentication:*
22. JWT Tokens: eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+
23. SSH/PEM Keys: -----BEGIN (PRIVATE|RSA PRIVATE|EC PRIVATE|OPENSSH PRIVATE) KEY-----
24. Authorization Headers: (Bearer|Basic|Api-Key|X-API-Key):\s*[A-Za-z0-9_=-]{20,}
25. Basic Auth in URLs: https?://[^:]+:[^@]+@

*Database Credentials:*
26. PostgreSQL: postgres://user:pass@host
27. MySQL: mysql://user:pass@host
28. MSSQL: mssql://user:pass@host
29. MongoDB: mongodb://user:pass@host, mongodb+srv://user:pass@host
30. Redis: redis://user:pass@host
31. AMQP: amqp://user:pass@host
32. JDBC: jdbc:(postgres|mysql|oracle|sqlserver)://.*password=

*Cookies and Session Data:*
33. Set-Cookie headers: sessionid, csrftoken, auth_token, connect.sid
34. Secure cookies: __Secure-*, __Host-*
35. Cookie header values: Cookie: sessionid=*, csrftoken=*

*Environment Variables and Config:*
36. Sensitive env vars: (SECRET|TOKEN|PASSWORD|PRIVATE|CLIENT_SECRET|API_KEY|AUTH|ACCESS_TOKEN|REFRESH_TOKEN)[:=][^\s]{8,}
37. Structured secrets in JSON/YAML: Keys matching (password|secret|token|api_key|client_secret) with high-entropy values

*High-Entropy Strings:*
38. Base64-like strings: Length ≥20, Shannon entropy >4.5, alphanumeric+/-/_ alphabet, with context keywords (secret, token, key, bearer, auth, x-amz-, x-goog-)

*Personal Identifiers:*
39. Credit Cards: Luhn-validated 13-19 digit sequences (mask all but last 4 for utility)
40. SSNs: XXX-XX-XXXX format
41. Email Addresses: RFC 5322 compliant patterns (exclude example.com/.org/.net)
42. Passport Numbers: Jurisdiction-specific patterns (context-dependent)
43. Driver's Licenses: Jurisdiction-specific patterns (context-dependent)
44. National IDs: UK NINO, CA SIN, IN Aadhaar, EU national IDs
45. IBANs: International bank account numbers
46. Bank Account/Routing Numbers: US and international formats

*Healthcare Identifiers (HIPAA PHI):*
47. Medical Record Numbers (MRN)
48. Health Plan Numbers

*Contact Information:*
49. Phone Numbers: E.164 and common international formats (libphonenumber-based parsing)

*Network Identifiers:*
50. IPv4 Addresses: Public IPs only (exclude RFC1918 private ranges 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, localhost 127.0.0.1, 0.0.0.0, RFC5737 documentation ranges)
51. IPv6 Addresses: Public only (exclude fd00::/8, fe80::/10, ::1)
52. MAC Addresses: XX:XX:XX:XX:XX:XX, XX-XX-XX-XX-XX-XX

*Device Identifiers:*
53. IMEI: 15-digit Luhn-validated
54. IMSI: 14-15 digit mobile subscriber IDs
55. IDFA: Apple Advertising IDs (UUID format)
56. Android Advertising ID: UUID format
57. Apple UDID: Legacy device IDs

*File Paths:*
58. User Paths: /Users/*, /home/*, C:\Users\*, C:\Documents and Settings\* (exclude C:\Users\Public)
59. Sensitive Windows Paths: AppData\Roaming\*, ProgramData\* (when containing usernames)

**Tier 2: Context-Dependent** (Stage 1 + Stage 2):

*Contextual Secrets:*
60. URLs with Tokens: Query params (token=, key=, auth=, api_key=, access_token=, code=, state=)
61. Obfuscated/encoded secrets: Base64-encoded secrets detected via decode + re-match heuristics

*Personal Data:*
62. Person Names: Requires context (vs code variables, exclude common programming keywords like const, function, class)
63. Street Addresses: US and international formats (with context and locale awareness)
64. Dates of Birth: With context indicators (DOB:, born:, birthdate:)
65. Geolocation: Latitude/longitude pairs with realistic ranges (-90 to 90, -180 to 180)
66. VINs/License Plates: Vehicle identification (context-dependent)

*Organizations:*
67. Organization Names: Requires context (vs product names, framework names)

**Tier 3: Novel/Emerging** (Stage 2 only):
68. New API provider patterns (monitor provider documentation continuously)
69. Custom authentication schemes (proprietary formats)
70. Domain-specific identifiers (industry-specific PII)
71. Obfuscated or base64-encoded secrets (deep inspection after normalization)
72. Multilingual PII (names, addresses, phone formats in non-English locales)

### Redaction Strategy

**Irreversible by Default** (STANDARDS-compliant):
```
Original: "My email is user@example.com"
Redacted: "My email is [REDACTED_EMAIL]"
```

**Optional Session-Scoped Pseudonymization** (disabled by default):
```
Original: "user@example.com sent email to user@example.com"
Pseudonymized: "<EMAIL_1> sent email to <EMAIL_1>"
```

**Pseudonymization Properties** (if enabled):
- Session-scoped only (conversation lifetime)
- **In-memory mapping ONLY** - NEVER persisted to disk (STANDARDS section 1 compliance)
- Mapping stored exclusively in process memory or ephemeral session cache (e.g., Redis with TTL, never SQLite)
- Auto-clear on session end or 24-hour TTL (whichever comes first)
- Never reversible across sessions (mapping is destroyed)
- Never shared globally across users
- **CRITICAL**: Storing the original→placeholder mapping on disk violates STANDARDS section 1 (never persist raw data), even if encrypted
- If product requirements mandate persistence, requires explicit exception ADR with:
  - KMS-backed envelope encryption (key rotation every 90 days)
  - Sealed storage with audit logs
  - <24h TTL with automated purge
  - Strong risk justification and DPO sign-off
  - Rigorous security audits

### Detection Pipeline

```typescript
interface DetectionResult {
  sanitized: string;
  detections: Detection[]; // In-memory only, with original
  persistedDetections: PersistedDetection[]; // For logging, no originals
  stage1Confidence: number; // Rules
  stage2Confidence: number; // AI (if run)
  method: 'rules-only' | 'hybrid';
}

interface Detection {
  category: PIICategory;
  original: string;        // NEVER persisted - in-memory only, discarded after hook returns
  placeholder: string;     // [REDACTED_TYPE] default, <TYPE_N> if pseudonymization enabled
  confidence: number;      // 0.0-1.0
  position: { start: number; end: number }; // Positions in original text
  detector: 'rules' | 'ai';
}

interface PersistedDetection {
  // Used for sanitization_log and audit records
  // CRITICAL: Does NOT include 'original' field - STANDARDS section 1 compliance
  category: PIICategory;
  placeholder: string;     // The replacement text used
  confidence: number;      // Detection confidence score
  position: { start: number; end: number }; // Positions relative to sanitized text
  detector: 'rules' | 'ai';
  detectorVersion: string; // Version for reproducibility and rollback
  // EXPLICITLY NO 'original' field - never log or persist raw PII values
}

// Stage 0: Normalization (before detection)
// CRITICAL: Makes pattern detection robust against obfuscation and encoding
function normalizeText(text: string): string {
  // 1. Unicode NFKC normalization (canonical decomposition + compatibility composition)
  let normalized = text.normalize('NFKC');

  // 2. Strip zero-width characters (used for obfuscation)
  normalized = normalized.replace(/[\u200B-\u200D\uFEFF]/g, '');

  // 3. Normalize whitespace (collapse multiple spaces/tabs/newlines to single space)
  normalized = normalized.replace(/\s+/g, ' ');

  // 4. Decode common HTML entities
  normalized = normalized
    .replace(/&lt;/g, '<')
    .replace(/&gt;/g, '>')
    .replace(/&amp;/g, '&')
    .replace(/&quot;/g, '"')
    .replace(/&#39;/g, "'")
    .replace(/&apos;/g, "'");

  // 5. URL-decode query parameters and paths (for token detection in URLs)
  // Apply selectively to avoid breaking legitimate encoded content
  normalized = normalized.replace(/(%[0-9A-Fa-f]{2})+/g, (match) => {
    try {
      return decodeURIComponent(match);
    } catch {
      return match; // Keep original if decode fails
    }
  });

  // 6. Inspect Base64/Base64URL windows when strongly indicated by context
  // This helps detect Authorization: Basic <base64> and embedded encoded secrets
  // Note: Apply heuristics carefully to avoid excessive decoding overhead

  return normalized;
}

// Stage 1: Synchronous (in hook)
function detectWithRules(text: string): Detection[] {
  const detections: Detection[] = [];

  // 1. Standard regex patterns (precompiled, no catastrophic backtracking)
  for (const [category, pattern] of piiPatterns) {
    const matches = text.matchAll(pattern);
    for (const match of matches) {
      detections.push({
        category,
        original: match[0],
        placeholder: `[REDACTED_${category}]`,
        confidence: 1.0, // Deterministic
        position: { start: match.index!, end: match.index! + match[0].length },
        detector: 'rules'
      });
    }
  }

  // 2. Structured key-value secret detection (JSON/YAML)
  // Faster and more precise than regex on unstructured text
  const structuredSecrets = detectStructuredSecrets(text);
  detections.push(...structuredSecrets);

  // 3. High-entropy string detection (novel secrets)
  // Apply with strict context/length limits to control FPs
  const entropyDetections = detectHighEntropyStrings(text);
  detections.push(...entropyDetections);

  return detections;
}

// Helper: Detect secrets in structured data (JSON/YAML)
function detectStructuredSecrets(text: string): Detection[] {
  const detections: Detection[] = [];
  const sensitiveKeys = [
    'password', 'secret', 'token', 'api_key', 'apikey', 'client_secret',
    'private_key', 'access_token', 'refresh_token', 'auth', 'authorization'
  ];

  try {
    // Attempt JSON parse
    const data = JSON.parse(text);
    // Recursively scan keys
    scanObject(data, '', detections);
  } catch {
    // Not valid JSON, skip structured detection
  }

  return detections;

  function scanObject(obj: any, path: string, dets: Detection[]): void {
    if (typeof obj !== 'object' || obj === null) return;
    for (const [key, value] of Object.entries(obj)) {
      const lowerKey = key.toLowerCase();
      if (sensitiveKeys.some(k => lowerKey.includes(k))) {
        if (typeof value === 'string' && value.length >= 8) {
          // Found sensitive key with string value
          dets.push({
            category: 'API_KEY', // Or map to specific category
            original: value,
            placeholder: '[REDACTED_API_KEY]',
            confidence: 0.9, // Structured context, high confidence
            position: { start: text.indexOf(value), end: text.indexOf(value) + value.length },
            detector: 'rules'
          });
        }
      }
      if (typeof value === 'object') {
        scanObject(value, `${path}.${key}`, dets);
      }
    }
  }
}

// Helper: Detect high-entropy strings (base64-like, potential secrets)
function detectHighEntropyStrings(text: string): Detection[] {
  const detections: Detection[] = [];
  const contextKeywords = [
    'secret', 'token', 'key', 'bearer', 'auth', 'password',
    'x-amz-', 'x-goog-', 'authorization', 'cookie'
  ];

  // Match base64/base64url-like strings (length >= 20)
  const entropyPattern = /[A-Za-z0-9+/\-_]{20,}/g;
  const matches = text.matchAll(entropyPattern);

  for (const match of matches) {
    const candidate = match[0];
    const entropy = calculateShannonEntropy(candidate);

    // High entropy threshold (> 4.5 bits per character)
    if (entropy > 4.5) {
      // Check for context keywords nearby (within 50 chars before)
      const contextStart = Math.max(0, match.index! - 50);
      const context = text.slice(contextStart, match.index!).toLowerCase();
      const hasContext = contextKeywords.some(kw => context.includes(kw));

      if (hasContext) {
        detections.push({
          category: 'HIGH_ENTROPY_SECRET',
          original: candidate,
          placeholder: '[REDACTED_SECRET]',
          confidence: 0.85, // Entropy + context heuristic
          position: { start: match.index!, end: match.index! + candidate.length },
          detector: 'rules'
        });
      }
    }
  }

  return detections;
}

// Helper: Calculate Shannon entropy
function calculateShannonEntropy(str: string): number {
  const len = str.length;
  const frequencies: Record<string, number> = {};

  for (const char of str) {
    frequencies[char] = (frequencies[char] || 0) + 1;
  }

  let entropy = 0;
  for (const count of Object.values(frequencies)) {
    const p = count / len;
    entropy -= p * Math.log2(p);
  }

  return entropy;
}

// Stage 2: Async (downstream job)
async function detectWithAI(
  text: string,
  ruleDetections: Detection[]
): Promise<Detection[]> {
  const prompt = `
Analyze this text for PII not caught by rule-based detection.
Focus on context-dependent PII (names vs variables, novel patterns).

PII Taxonomy: ${JSON.stringify(PII_TAXONOMY)}
Already detected: ${ruleDetections.map(d => d.category).join(', ')}

Text: """
${text}
"""

Return JSON array of new detections with category, position, confidence.
`;

  const response = await llm.complete(prompt, {
    model: 'claude-3-5-sonnet-20241022',
    temperature: 0.0, // Deterministic
    response_format: { type: 'json_object' }
  });

  return parseAIDetections(response);
}

// Combined pipeline
async function sanitize(text: string): Promise<DetectionResult> {
  // Stage 0: Normalization
  const normalized = normalizeText(text);

  // Stage 1: Fast rules (synchronous)
  const start = performance.now();
  const ruleDetections = detectWithRules(normalized);
  const stage1Duration = performance.now() - start;

  if (stage1Duration > 50) {
    logger.warn('Stage 1 detection exceeded budget', { duration: stage1Duration });
  }

  // Apply redactions
  const sanitized = applyRedactions(normalized, ruleDetections);

  // Create persisted detections (without original values)
  // CRITICAL: Never include 'original' field - STANDARDS section 1 compliance
  const persistedDetections: PersistedDetection[] = ruleDetections.map(d => ({
    category: d.category,
    placeholder: d.placeholder,
    confidence: d.confidence,
    position: d.position, // Positions relative to sanitized text
    detector: d.detector,
    detectorVersion: DETECTOR_VERSION // For reproducibility and rollback
    // EXPLICITLY NO 'original' field - never log or persist raw PII
  }));

  // Queue Stage 2 (async, non-blocking)
  await jobQueue.enqueue({
    type: 'ai-sanitization-validation',
    payload: {
      sanitizedText: sanitized, // Already sanitized, no raw content
      persistedDetections
    },
    priority: 'low'
  });

  return {
    sanitized,
    detections: ruleDetections, // In-memory only, discarded after hook returns
    persistedDetections, // Safe to log/persist
    stage1Confidence: calculateConfidence(ruleDetections),
    stage2Confidence: 0, // Not yet run
    method: 'rules-only'
  };
}
```

### Accuracy Targets

| Metric | Stage 1 (Rules) | Stage 2 (AI) | Combined |
|--------|----------------|--------------|----------|
| False Negative Rate | 0 known FN on regression suite (design target) | <5% (novel) | <5% |
| False Positive Rate | <1% overall | <5% | <3% |
| Latency (p95) | <50ms | <2s | <50ms sync + <2s async |
| Recall (enumerated PII) | Design target: 100% on test suite | N/A | >95% |
| Precision (utility) | 99%+ | 95%+ | 97%+ |

**Category-Specific Targets** (Stage 1):

| Category | FN Target | FP Tolerance | Rationale |
|----------|-----------|--------------|-----------|
| API Keys/Tokens/SSH Keys | <0.1% | <10% | Maximum safety, accept over-redaction |
| Database Credentials | <0.1% | <10% | Critical secrets, no leaks tolerated |
| Credit Cards | 0 known FN on test suite | <1% | PCI DSS compliance, mask last 4 for utility |
| SSNs/National IDs | <0.5% | <2% | Regulatory compliance (GDPR, HIPAA) |
| Email Addresses | <1% | <2% | Balance privacy and utility |
| IP Addresses (public) | <2% | <5% | Lower risk, preserve utility for debugging |
| Personal Names | <2% | <5% | Context-aware, preserve code variable utility |
| File Paths (user-specific) | <1% | <3% | Privacy risk moderate |

**Residual Risk Acknowledged**:
- Stage 1 design target is 0 known FN on regression suite, NOT a provable mathematical guarantee across all possible inputs
- Unknown/novel patterns (new API providers, custom auth schemes) slip through until Stage 2 runs (async gap creates exposure window)
- Stage 2 has <5% FN for novel patterns due to LLM limitations and non-deterministic behavior
- Combined system targets <5% overall FN rate with continuous improvement
- Continuous monitoring and pattern updates required when misses are found (7-day SLA for adding new patterns after discovery)
- Evaluation methodology uses labeled corpus with confidence intervals, not absolute claims

### Audit and Validation Plan

**Pre-Deployment**:
1. Canary test suite (1000+ known PII samples across all categories)
2. Property-based testing (generated variations)
3. Adversarial testing (obfuscated PII)
4. Benchmarking (latency, accuracy on labeled dataset)

**Post-Deployment**:
1. Post-ingest audits (random 1% sample daily, re-run detection on sanitized content)
   - **Key insight**: Since only sanitized content is stored, missed PII remains present in sanitized text until Stage 2 catches it
   - Seeded canaries in live traffic (synthetic innocuous markers, NOT real PII) for regression detection
   - Manual review sample: 5% of Stage 2 flagged messages (confidence 0.6-0.8)
   - Reviewer tooling shows ONLY sanitized text + context (never raw originals)
2. Canary injection (synthetic PII in test conversations)
   - Automated tests inject known-safe PII patterns
   - Validate detection within SLA (Stage 1 immediate, Stage 2 within 2s)
3. User reporting mechanism (flag missed PII)
   - In-app "Report PII" button
   - **STANDARDS-compliant remediation** (STANDARDS section 3: no "quarantined" status):
     - Option A: Soft-delete (add deleted_at timestamp, exclude from queries)
     - Option B: Disable flag (messages.disabled = true)
     - Option C: Move to separate quarantine table (not using job status enum)
   - Immediate action upon report confirmation
   - Alert security team for pattern update
4. Continuous pattern updates (monitor new API provider docs, add within 7-day SLA)
   - Weekly scan of major API provider changelogs
   - Automated alerts for new token formats
5. Kill-switch (halt uploads if audit finds PII in stored data)
   - **CRITICAL**: Trigger conditions:
     - Overall FN rate >5% on audit sample
     - Per-category FN for API_KEY/JWT/SSH_KEY >1%
     - Manual PII report confirmed
   - Action: Set global flag to disable uploads until remediation complete
6. Versioning and remediation workflow:
   - **Versioning**:
     - Log detectorVersion (semantic versioning, e.g., "1.2.3") with each sanitization
     - Log sanitized_revision when content is updated (incremental: 1, 2, 3...)
     - Enable reproducibility: re-run old detector version to verify fixes
   - **Automated remediation** (when new patterns added):
     - Re-run updated rules + AI on ALL stored sanitized content
     - Patch content in-place (UPDATE messages SET content = ..., sanitized_revision = sanitized_revision + 1)
     - Use idempotent patcher keyed by message_id + revision (handle concurrent workers)
     - "Re-sanitize until stable" approach (iterate until no new detections)
   - **Incident response** (when FN threshold exceeded):
     - Halt uploads immediately (kill-switch)
     - Notify DPO and security team
     - Root cause analysis (which patterns missed, why)
     - Emergency pattern update
     - Full corpus re-sanitization
     - Audit report to stakeholders

**Validation Metrics**:
```typescript
interface ValidationReport {
  timestamp: string;
  sampleSize: number;
  detectedPII: number;
  categoriesDetected: Record<PIICategory, number>; // Per-category counts
  categoryMetrics: Record<PIICategory, {
    falseNegatives: number; // Manual review
    falsePositives: number; // Manual review
    redactionRate: number; // % of messages with this category
  }>;
  falseNegatives: number; // Overall manual review
  falsePositives: number; // Overall manual review
  stage1Latency: { p50: number; p95: number; p99: number };
  stage2Latency: { p50: number; p95: number; p99: number };
  detectorVersion: string;
}
```

**Alert Thresholds**:
- Overall FN Rate >5%: CRITICAL alert, halt uploads, incident response
- Per-category FN for API_KEY/JWT/SSH_KEY >1%: CRITICAL, immediate pattern update
- Overall FP Rate >10%: WARNING, review detection rules
- PERSON_NAME redaction surge >20% p95: WARNING, possible prompt drift
- Stage 1 latency >100ms p95: WARNING, optimize patterns/add regex timeouts
- Stage 2 latency >5s p95: WARNING, review LLM usage/add circuit breaker

**Manual Review Process** (borderline detections):
- Stage 2 flags detections with confidence 0.6-0.8 for human review
- Reviewer tooling shows ONLY sanitized text + context (never raw originals)
- Review SLA: 48 hours for borderline cases
- Sampling rate: 5% of Stage 2 flagged messages

## Alternatives Considered

### Alternative 1: Rules-Only (No AI)

**Pros**:
- Fast, deterministic, cheap
- No LLM dependency
- Predictable behavior

**Cons**:
- High false negative rate for contextual PII (names, novel patterns)
- Requires constant pattern updates
- Can't distinguish "John" (name) from "john" (variable)

**Why not chosen**: Unacceptable privacy risk. Novel patterns and contextual PII would leak.

### Alternative 2: AI-Only (No Rules)

**Pros**:
- High accuracy for contextual PII
- Adapts to novel patterns
- No pattern maintenance

**Cons**:
- Too slow for synchronous hook (<100ms budget)
- Expensive (LLM call per message)
- Non-deterministic (LLM variance)
- False positives reduce utility

**Why not chosen**: Latency and cost prohibitive for real-time sanitization.

### Alternative 3: ML NER Models (spaCy, Presidio)

**Pros**:
- Fast (<100ms)
- Good accuracy for trained entities
- Deterministic

**Cons**:
- Requires labeled training data
- Limited to trained entity types
- Misses novel patterns (API keys, JWTs)
- Model maintenance overhead

**Why not chosen**: Complements hybrid approach but doesn't replace it. Could add post-MVP.

### Alternative 4: Field-Level Encryption (No Redaction)

**Pros**:
- Reversible
- Full data preserved
- Simple implementation

**Cons**:
- Key management complexity
- Doesn't minimize attack surface
- Encrypted PII may still violate regulations
- Key leak exposes all data

**Why not chosen**: Doesn't eliminate PII, only obscures it. Fails zero-trust principle.

### Alternative 5: Differential Privacy (Statistical Noise)

**Pros**:
- Mathematically proven privacy bounds
- Preserves statistical utility
- No PII detection needed

**Cons**:
- Not applicable to conversational data (need exact text)
- Noise makes conversations unreadable
- Complex implementation
- Doesn't prevent individual PII leaks

**Why not chosen**: Incompatible with conversational learning extraction.

## Consequences

### Positive

- **Layered defense**: Multiple detection methods reduce false negatives
- **Performance optimized**: Synchronous rules meet <50ms budget, AI runs async
- **Utility preserved**: Low false positive rate (97% precision) maintains learning value
- **Comprehensive taxonomy**: Covers all known PII types
- **Audit trail**: Post-ingest validation catches mistakes
- **Adaptable**: AI stage catches novel patterns rules miss
- **Cost-effective**: Rules handle 95%+ of PII, AI only for edge cases

### Negative

- **Residual false negatives**: <5% PII may slip through (acknowledged risk)
- **Complexity**: 2-stage pipeline harder to maintain than single method
- **Async gap**: AI validation runs after persistence (Stage 1 must be perfect for known patterns)
- **LLM dependency**: Stage 2 requires Claude API (cost, latency, availability)
- **Pattern maintenance**: Rule patterns require ongoing updates

### Neutral

- **Tuning required**: Confidence thresholds need empirical validation
- **Monitoring overhead**: Continuous audits and canary tests required
- **Manual review**: Borderline cases need human adjudication

## Risks and Mitigations

### Risk: Stage 1 False Negatives (Unknown Patterns)

**Impact**: CRITICAL - Privacy violation before Stage 2 runs

**Likelihood**: Medium (new API providers, token formats)

**Mitigation**:
- Continuous pattern updates (monitor new API docs)
- Post-ingest audits detect misses within 24h
- Kill-switch halts uploads if PII found
- User reporting mechanism
- Conservative redaction (err on side of privacy)

### Risk: Stage 2 False Positives (Over-Redaction)

**Impact**: Medium - Reduced learning utility

**Likelihood**: Low (<5% target)

**Mitigation**:
- Manual review queue for borderline cases
- Feedback loop improves AI prompts
- Metrics on redaction rate by category
- User feedback on utility loss

### Risk: Latency Budget Violation

**Impact**: High - Blocks user interactions

**Likelihood**: Low (benchmarked at <50ms)

**Mitigation**:
- Compiled regex patterns (no backtracking)
- Performance monitoring with alerts
- Fallback to minimal rules if timeout

### Risk: Novel PII Types

**Impact**: High - Privacy violation

**Likelihood**: Medium (evolving ecosystem)

**Mitigation**:
- Stage 2 AI catches unknowns
- Bug bounty for finding missed PII
- Regular security audits
- Conservative AI prompts

## Implementation

### Regex Safety and Performance (CRITICAL)

**Before any pattern implementation**:
- **Precompile all patterns**: Use compiled regex objects, not string patterns
- **Avoid catastrophic backtracking**: Use atomic groups `(?>...)`, possessive quantifiers where available
- **Bound repetitions**: Replace `.*` with `.{0,200}` or specific character classes
- **Specific character classes**: Use `[A-Za-z0-9]` instead of `.` where possible
- **Test worst-case inputs**: Maintain fuzz corpus for regex DoS testing
- **Add timeouts**: Cap regex execution time per pattern (e.g., 10ms max)
- **Limit input length**: Chunk long messages (>10KB) or apply patterns to windows
- **No nested quantifiers**: Avoid patterns like `(a+)+` or `(.*)*`

**Performance monitoring**:
- Track Stage 1 latency per pattern category
- Alert if any single pattern exceeds 10ms p95
- Auto-disable slow patterns and fall back to minimal ruleset

### Phase 0: Pattern Library

- [ ] Define regex patterns for all Tier 1 PII (72 categories)
- [ ] Implement regex safety checks (atomic groups, bounded repetitions)
- [ ] Benchmark latency on 10k messages with worst-case inputs
- [ ] Validate 0 known FN on regression suite (1000+ samples)
- [ ] Optimize for <50ms p95 total (all patterns combined)
- [ ] Add entropy detector with tuned thresholds (>4.5 bits/char)
- [ ] Implement structured JSON/YAML secret scanner

### Phase 1: Stage 1 Integration

- [ ] Implement detectWithRules() in hook
- [ ] Add performance monitoring
- [ ] Integrate with sanitization log
- [ ] Deploy to production

### Phase 2: Stage 2 Validation

- [ ] Implement AI validation job
- [ ] Define LLM prompt with taxonomy
- [ ] Queue async jobs from hook
- [ ] Reconcile AI detections with stored data

### Phase 3: Audit Infrastructure

- [ ] Post-ingest random sampling
- [ ] Canary injection tests
- [ ] Kill-switch implementation
- [ ] Monitoring dashboards

## Related Documents

### Standards
- [STANDARDS.md Section 9: Sanitization Standard](../STANDARDS.md#9-sanitization-standard)

### Decisions
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md) - Storage policy
- [ADR-001: Use Claude Hooks](./decision-use-claude-hooks-2025-01-16.md) - Hook integration

### Architecture
- [Sanitization Pipeline Architecture](../architecture/architecture-sanitization-pipeline-2025-01-16.md) - Full pipeline design

### Reference
- [PII Detection Patterns](../reference/reference-pii-patterns-2025-01-16.md) - Complete pattern library

### Reviews
- [GPT-5 Holistic Review](../reviews/gpt5-holistic-review-2025-01-16.txt) - Recommendation to split ADR
- [GPT-5 ADR-010 Review](../reviews/gpt5-adr-010-pii-detection-review-2025-01-16.txt) - Detailed feedback incorporated

---

## Review History

**GPT-5 Review (2025-01-16)**: Comprehensive review provided the following improvements incorporated into this revision:

1. **Expanded PII Taxonomy** (11 → 72+ categories):
   - Added cloud providers: Google (GCP service accounts, OAuth), Azure (SAS tokens, storage keys), Cloudflare, Supabase, Sentry, Vercel, Notion, Discord, Telegram
   - Added database credentials: PostgreSQL, MySQL, MSSQL, MongoDB, Redis, AMQP, JDBC connection strings with embedded passwords
   - Added authentication mechanisms: Authorization headers (Bearer, Basic, Api-Key, X-API-Key), Basic auth in URLs, cookies (Set-Cookie, sessionid, csrftoken, __Secure-*, __Host-*)
   - Added environment variables: Sensitive key patterns (SECRET, TOKEN, PASSWORD, PRIVATE, CLIENT_SECRET, API_KEY, AUTH, ACCESS_TOKEN, REFRESH_TOKEN)
   - Added high-entropy string detection: Shannon entropy >4.5 bits/char, length ≥20, with context keywords (secret, token, key, bearer, auth, x-amz-, x-goog-)
   - Added international identifiers: Passport numbers, driver's licenses, national IDs (UK NINO, CA SIN, IN Aadhaar, EU), IBANs, bank account/routing numbers
   - Added healthcare identifiers (HIPAA PHI): Medical Record Numbers (MRN), health plan numbers
   - Added device identifiers: IMEI (Luhn), IMSI, IDFA, Android Advertising ID, Apple UDID
   - Added geolocation: Latitude/longitude pairs with realistic ranges
   - Added multilingual PII support (Tier 3)
   - Added structured key-value detection for JSON/YAML secrets

2. **Normalization Step** (Stage 0, before detection):
   - Unicode NFKC normalization (canonical + compatibility composition)
   - Zero-width character stripping (anti-obfuscation)
   - Whitespace normalization (collapse multiple spaces)
   - HTML entity decoding (&lt;, &gt;, &amp;, &quot;, &apos;)
   - URL decoding for query parameters
   - Base64/Base64URL inspection with context heuristics

3. **Fixed "0% FN" Absolute Claim**:
   - Changed to "0 known FN on regression suite (design target, not provable guarantee)"
   - Acknowledged not mathematically provable across all inputs
   - Added evaluation methodology using labeled corpus with confidence intervals
   - Added 7-day SLA for adding new patterns after discovery

4. **Category-Specific Thresholds**:
   - API Keys/Tokens/SSH Keys: <0.1% FN, <10% FP (maximum safety)
   - Database Credentials: <0.1% FN, <10% FP (critical secrets)
   - Credit Cards: 0 known FN on test suite, <1% FP, mask all but last 4 (PCI DSS)
   - SSNs/National IDs: <0.5% FN, <2% FP (regulatory compliance)
   - Email Addresses: <1% FN, <2% FP (balance privacy/utility)
   - IP Addresses (public): <2% FN, <5% FP (lower risk, preserve debugging utility)
   - Personal Names: <2% FN, <5% FP (context-aware, preserve code variables)
   - File Paths: <1% FN, <3% FP (moderate privacy risk)

5. **PersistedDetection Type** (without original field):
   - Explicitly removed 'original' field from logged/persisted detections
   - Added detectorVersion for reproducibility and rollback
   - Positions relative to sanitized text (not original)
   - Clear comments: "EXPLICITLY NO 'original' field - never log or persist raw PII"

6. **Replaced "Quarantined" Status** (STANDARDS section 3 compliance):
   - Option A: Soft-delete (deleted_at timestamp)
   - Option B: Disable flag (messages.disabled = true)
   - Option C: Separate quarantine table (not using job status enum)
   - No use of forbidden "quarantined" job status

7. **Pseudonymization Mapping - In-Memory Only**:
   - CRITICAL: Never persist mapping to disk (violates STANDARDS section 1)
   - Store exclusively in process memory or ephemeral Redis with TTL
   - If persistence required, needs exception ADR with KMS encryption, <24h TTL, DPO sign-off
   - Auto-destroy mapping on session end or 24h TTL

8. **Structured Key-Value Secret Detection**:
   - JSON/YAML parser with recursive key scanning
   - Sensitive key patterns: password, secret, token, api_key, client_secret, private_key, access_token, refresh_token, auth, authorization
   - Faster and more precise than regex on unstructured text
   - Confidence 0.9 for structured context

9. **Enhanced Audit and Remediation**:
   - Added versioning: detectorVersion (semantic), sanitized_revision (incremental)
   - Automated remediation workflow: re-run rules + AI, patch in-place, idempotent patcher
   - "Re-sanitize until stable" approach (iterate until no new detections)
   - Incident response: kill-switch triggers, root cause analysis, emergency pattern updates, full corpus re-sanitization
   - Manual review process: 5% of Stage 2 flagged messages (confidence 0.6-0.8), reviewer tooling shows only sanitized text
   - Per-category metrics and alerts

10. **IP Address and Credit Card Policy Clarifications**:
    - IPs: Public only (exclude RFC1918 private 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, localhost 127.0.0.1, 0.0.0.0, RFC5737 documentation ranges)
    - IPv6: Public only (exclude fd00::/8, fe80::/10, ::1)
    - Credit Cards: Mask all but last 4 digits for utility while meeting PCI DSS

11. **Regex Safety and Performance**:
    - Added comprehensive regex safety guidelines (precompile, avoid catastrophic backtracking, atomic groups, bounded repetitions, timeouts)
    - Per-pattern latency monitoring
    - Fuzz corpus for regex DoS testing
    - Auto-disable slow patterns with fallback

**Status**: All critical GPT-5 recommendations incorporated. Taxonomy expanded from 11 to 72+ categories. Full STANDARDS compliance achieved.

---

*This ADR defines the detection strategy; ADR-004 defines the storage policy (never persist raw). Together they form the complete privacy architecture.*
````

## File: decisions/decision-sanitize-before-storage-2025-01-16.md
````markdown
---
title: ADR-004: Sanitize Before Storage (Privacy-First Architecture)
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [privacy, security, pii, sanitization, zero-trust]
---

# ADR-004: Sanitize Before Storage (Privacy-First Architecture)

## Status

Accepted

Date: 2025-01-16

## Context

The Global Context Network captures user conversations with AI agents, which inevitably contain:
- **API keys and secrets** - Authentication tokens, passwords
- **Personal information** - Names, emails, phone numbers
- **File paths** - Absolute paths with usernames
- **IP addresses** - Network information
- **Organization-specific data** - Company names, project names
- **URLs with tokens** - Authentication in query params

This data will be:
1. Stored locally in SQLite database
2. Extracted for learning generation
3. Potentially shared globally via IPFS/blockchain
4. Queried by other AI agents via MCP

**The Risk**: If PII is stored unsanitized, it can leak through:
- Database breaches
- Accidental sharing
- Query results
- Learning extractions
- Global network uploads
- Developer debugging
- Log files

**Data Minimization Principle**: The safest PII is PII we never store. Once stored, it can spread through the system unpredictably.

**Zero-Trust Privacy**: We cannot trust that all downstream components will properly handle PII. The only safe approach is to never let PII enter storage.

## Decision

Sanitize ALL data BEFORE database insertion. Never store unsanitized conversation data.

**Architecture**:
```
Event Capture → Sanitization Pipeline → Database
                        ↑
                  (PII never passes this gate)
```

**Sanitization happens**:
- BEFORE any database write
- In the event queue processing worker
- Using hybrid detection (rules + AI)
- With audit trail of all redactions

**Sanitization methods**:
1. **Rule-based detector** - Fast regex for known patterns
2. **AI-powered detector** - Context-aware LLM analysis
3. **Hybrid validator** - Combines both approaches
4. **Audit logger** - Tracks what was redacted

**Redaction format**:
- Irreversible redaction by default
- Placeholder tokens: `<EMAIL_1>`, `<API_KEY_1>`, `<PERSON_1>`
- Optional per-session pseudonymization for within-session linking
- Separate encrypted mapping (if pseudonymization enabled)

## Consequences

### Positive

- **Zero-trust PII handling** - Database inherently safe
- **Safe default sharing** - No risk of accidental PII in learnings
- **Breach impact minimized** - No PII to steal
- **Compliance friendly** - Easier GDPR/CCPA alignment
- **Developer safety** - Devs can access database without PII exposure
- **Query safety** - MCP queries can't return PII
- **Audit trail** - Full log of what was redacted

### Negative

- **Irreversible** - Can't recover original data if over-redaction occurs
- **Async delay** - 1-2s sanitization delay before storage
- **False negatives risk** - May miss novel PII patterns
- **Utility loss** - Over-redaction reduces learning value
- **Complexity** - Hybrid pipeline is more complex than simple storage

### Neutral

- **Processing overhead** - Sanitization adds computational cost
- **Confidence thresholds** - Must tune detection sensitivity
- **Review workflow** - Borderline cases need manual review
- **Detector maintenance** - Must update PII patterns over time

## Alternatives Considered

### Alternative 1: Sanitize on Query

**Description**: Store raw data, sanitize when querying.

**Pros**:
- Can recover original data if needed
- Simpler storage path
- Faster writes

**Cons**:
- **Database contains PII** - Breach exposes everything
- **Too late** - PII already persisted and spread
- **Query bugs leak PII** - One bug exposes all data
- **Compliance risk** - Storing PII requires strict controls
- **Multiple sanitization points** - Must sanitize every query path

**Why not chosen**: Violates zero-trust principle. Database breach or query bug exposes all PII.

### Alternative 2: Sanitize on Upload Only

**Description**: Store raw locally, sanitize only for global sharing.

**Pros**:
- Local utility preserved
- Only sanitize what's shared
- Can debug with full data

**Cons**:
- **Local database contains PII** - User machine breach exposes PII
- **Accidental sharing risk** - One bug uploads raw data
- **Developer access risk** - Devs see PII during debugging
- **Log leakage** - Logs may contain PII
- **MCP queries return PII** - Agents see raw data

**Why not chosen**: Too many opportunities for PII leakage. Doesn't minimize data surface area.

### Alternative 3: Trust Users to Redact

**Description**: Provide UI for users to review and redact before storage.

**Pros**:
- User control
- High precision (users know what's sensitive)
- No false positives

**Cons**:
- **Users make mistakes** - Will forget to redact
- **Poor UX** - Friction on every interaction
- **Incomplete coverage** - Users miss subtle PII
- **Not scalable** - Can't review every conversation

**Why not chosen**: Users are not reliable. Automated approach required.

### Alternative 4: Encryption Only (No Redaction)

**Description**: Encrypt sensitive data, don't redact.

**Pros**:
- Reversible
- Data preserved
- Simple implementation

**Cons**:
- **Key management complexity** - Where to store keys?
- **Still have access to raw data** - Can decrypt when needed
- **Doesn't minimize surface** - PII still in system
- **Compliance unclear** - Encrypted PII may still be PII
- **Key leak exposes all** - Single point of failure

**Why not chosen**: Doesn't eliminate PII, just obscures it. Key management introduces new risks.

### Alternative 5: Layered Detection (Rules + ML NER + LLM)

**Description**: Use multiple detection layers: regex, ML NER models, LLM adjudicator.

**Pros**:
- Higher accuracy than single method
- Catches different PII types
- Reduces false negatives

**Cons**:
- More complex pipeline
- Higher latency
- More expensive (ML model + LLM calls)
- More maintenance

**Why not chosen**: **ACTUALLY CHOSEN** - This is the hybrid approach we're implementing (rules + LLM). Could add ML NER post-MVP for even better accuracy.

## Implementation

### PII Taxonomy

Define what we detect:

```typescript
enum PIICategory {
  API_KEY = "API_KEY",           // API keys, tokens, passwords
  EMAIL = "EMAIL",               // Email addresses
  PHONE = "PHONE",               // Phone numbers (all formats)
  SSN = "SSN",                   // Social Security Numbers
  CREDIT_CARD = "CREDIT_CARD",   // Credit card numbers
  IP_ADDRESS = "IP_ADDRESS",     // IPv4/IPv6 addresses
  MAC_ADDRESS = "MAC_ADDRESS",   // Hardware addresses
  PERSON_NAME = "PERSON_NAME",   // Human names (not code names)
  FILE_PATH = "FILE_PATH",       // Absolute paths with usernames
  URL_WITH_TOKEN = "URL_WITH_TOKEN", // URLs with auth params
  AWS_KEY = "AWS_KEY",           // AWS access keys
  PRIVATE_KEY = "PRIVATE_KEY",   // SSH/TLS private keys
  JWT = "JWT",                   // JSON Web Tokens
  ORGANIZATION = "ORGANIZATION"  // Company/org names
}
```

### Detection Patterns

```typescript
const piiPatterns = {
  EMAIL: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g,
  PHONE: /\b(\+\d{1,3}[-.]?)?\(?\d{3}\)?[-.]?\d{3}[-.]?\d{4}\b/g,
  API_KEY: /\b(sk_live_|pk_live_|api_key_|apikey=)[A-Za-z0-9_-]{20,}\b/gi,
  AWS_KEY: /\b(AKIA[0-9A-Z]{16})\b/g,
  FILE_PATH: /\/Users\/[^\/\s]+\/.*|\/home\/[^\/\s]+\/.*/g,
  URL_WITH_TOKEN: /https?:\/\/[^\s]+[?&](token|key|auth|api_key)=[^\s&]+/gi,
  // ... more patterns
};
```

### Layered Detection Pipeline

```typescript
interface SanitizationResult {
  sanitized: string;
  detections: Detection[];
  confidence: number; // 0-1
  method: "rules" | "ai" | "hybrid";
}

interface Detection {
  category: PIICategory;
  original: string;
  placeholder: string;
  confidence: number;
  position: { start: number; end: number };
  detector: "rules" | "ai";
}

async function sanitize(text: string): Promise<SanitizationResult> {
  // Layer 1: Fast rule-based detection (< 10ms)
  const ruleDetections = detectWithRules(text);

  // Layer 2: AI-powered context-aware detection (< 2s)
  const aiDetections = await detectWithAI(text, ruleDetections);

  // Layer 3: Hybrid validator combines results
  const allDetections = mergeDetections(ruleDetections, aiDetections);

  // Layer 4: Apply redactions
  const sanitized = applyRedactions(text, allDetections);

  // Layer 5: Audit log
  await logSanitization(text, sanitized, allDetections);

  return {
    sanitized,
    detections: allDetections,
    confidence: calculateConfidence(allDetections),
    method: "hybrid"
  };
}
```

### Reversible Pseudonymization (Optional)

For within-session linking:

```typescript
interface PseudonymizationMapping {
  sessionId: string;
  mappings: Map<string, string>; // original → placeholder
  encrypted: boolean;
  ttl: number; // Auto-delete after N seconds
}

// Per-session mapping stored separately with envelope encryption
const sessionMapping = {
  sessionId: "conv-123",
  mappings: new Map([
    ["user@example.com", "<EMAIL_1>"],
    ["John Doe", "<PERSON_1>"]
  ]),
  encrypted: true,
  ttl: 86400 // 24 hours
};
```

### Confidence Thresholds

```typescript
interface SanitizationPolicy {
  minConfidence: number; // 0.8 = require 80% confidence
  reviewThreshold: number; // 0.6 = manual review if 60-80%
  blockThreshold: number; // 0.0 = block if any PII detected

  async handleBorderline(result: SanitizationResult): Promise<Action> {
    if (result.confidence < this.minConfidence) {
      if (result.confidence >= this.reviewThreshold) {
        return "QUARANTINE_FOR_REVIEW";
      } else {
        return "BLOCK_STORAGE";
      }
    }
    return "ALLOW_STORAGE";
  }
}
```

### Redaction Format with Entity Tags

```typescript
function applyRedactions(text: string, detections: Detection[]): string {
  let sanitized = text;
  const entityCounts = new Map<PIICategory, number>();

  // Sort by position (reverse) to maintain indices
  detections.sort((a, b) => b.position.start - a.position.start);

  for (const detection of detections) {
    // Increment counter for this entity type
    const count = (entityCounts.get(detection.category) || 0) + 1;
    entityCounts.set(detection.category, count);

    // Generate placeholder
    const placeholder = `<${detection.category}_${count}>`;

    // Replace
    sanitized =
      sanitized.slice(0, detection.position.start) +
      placeholder +
      sanitized.slice(detection.position.end);
  }

  return sanitized;
}
```

### Streaming Sanitization

Avoid buffering raw content:

```typescript
async function* streamingSanitize(
  eventStream: AsyncIterable<string>
): AsyncGenerator<string> {
  let buffer = "";

  for await (const chunk of eventStream) {
    buffer += chunk;

    // Process complete sentences
    const sentences = buffer.split(/[.!?]\s+/);
    buffer = sentences.pop() || ""; // Keep incomplete sentence

    for (const sentence of sentences) {
      const { sanitized } = await sanitize(sentence);
      yield sanitized + ". ";
    }
  }

  // Process remaining buffer
  if (buffer) {
    const { sanitized } = await sanitize(buffer);
    yield sanitized;
  }
}
```

### Evidence of Sanitization

Store metadata with every record:

```typescript
interface SanitizationEvidence {
  recordId: string;
  timestamp: string;
  detectorVersion: string; // "rules-v1.2 + ai-v2.0"
  detectionsCount: number;
  categoriesDetected: PIICategory[];
  confidence: number;
  reviewStatus: "auto" | "reviewed" | "quarantined";
  auditor: string; // "automated" | "human-reviewer-id"
}
```

### Right to Delete Process

Even for sanitized data:

```typescript
async function handleDeletionRequest(userId: string, conversationId: string) {
  // 1. Delete sanitized conversation
  await db.conversations.delete({ id: conversationId });

  // 2. Delete pseudonymization mappings
  await db.pseudonymMappings.delete({ conversationId });

  // 3. Delete derived learnings
  await db.learnings.delete({ sourceConversationId: conversationId });

  // 4. Add to revocation list for global network
  await db.revocations.insert({
    conversationId,
    timestamp: new Date(),
    reason: "user-requested-deletion"
  });

  // 5. If already uploaded, publish revocation
  const upload = await db.uploads.findOne({ conversationId });
  if (upload) {
    await publishRevocation(upload.ipfsCid);
  }
}
```

### Post-Ingest Audits

Continuous validation:

```typescript
async function runSanitizationAudit() {
  // Randomly sample stored conversations
  const samples = await db.conversations.sample(100);

  for (const conv of samples) {
    // Re-run detection on stored data
    const { detections } = await sanitize(conv.sanitizedContent);

    if (detections.length > 0) {
      // Found PII in supposedly sanitized data!
      await alert({
        severity: "CRITICAL",
        message: `PII found in stored conversation ${conv.id}`,
        detections
      });

      // Quarantine
      await db.conversations.update(conv.id, {
        status: "QUARANTINED",
        quarantineReason: "post-ingest-pii-detection"
      });
    }
  }
}

// Run nightly
schedule("0 2 * * *", runSanitizationAudit);
```

### Canary Scans

Inject known PII to verify detection:

```typescript
const canaryTests = [
  "My email is canary-test-001@example.com",
  "API key: sk_test_canary_12345",
  "My SSN is 123-45-6789",
  // ... more canaries
];

async function runCanaryTest() {
  for (const canary of canaryTests) {
    const { detections } = await sanitize(canary);

    if (detections.length === 0) {
      await alert({
        severity: "CRITICAL",
        message: "Sanitization canary test failed",
        canary
      });
    }
  }
}

// Run on every deployment
```

## Risks and Mitigations

### Risk: False Negatives (Missed PII)

**Impact**: Critical - Privacy violation

**Mitigation**:
- Layered detection (rules + AI)
- Post-ingest audits (random sampling)
- Canary tests for known patterns
- User reporting mechanism
- Kill-switch for publishing if issues found
- Continuous pattern updates

### Risk: Over-Redaction (Utility Loss)

**Impact**: Medium - Reduced learning value

**Mitigation**:
- Confidence thresholds (tune to balance precision/recall)
- Context-aware AI detection (distinguish names from code)
- Manual review queue for borderline cases
- Feedback loop to improve detection
- Metrics on redaction rate

### Risk: Novel PII Patterns

**Impact**: High - New PII types not detected

**Mitigation**:
- Continuous pattern updates
- AI detection catches unknowns
- User feedback
- Regular security audits
- Bug bounty for finding missed PII

## Related Documents

### Architecture
- [Sanitization Pipeline Architecture](../architecture/architecture-sanitization-pipeline-2025-01-16.md)
- [Global Context Network](../architecture/architecture-global-context-network-2025-01-16.md)

### Decisions
- [ADR-001: Use Claude Hooks](./decision-use-claude-hooks-2025-01-16.md)
- [ADR-005: Use SQLite](./decision-use-sqlite-2025-01-16.md)

### Plans
- [Phase 2: Sanitization Pipeline](../plans/plan-phase-2-sanitization-2025-01-16.md)

### Reference
- [PII Detection Patterns](../reference/reference-pii-patterns-2025-01-16.md)
````

## File: decisions/decision-security-provenance-2025-01-16.md
````markdown
---
title: ADR-011: Signing, Attestations, and Provenance for Published Learnings
category: decision
date: 2025-01-16
status: draft
deciders: Claude + Dennison
tags: [security, signing, provenance, attestations, trust, integrity]
---

# ADR-011: Signing, Attestations, and Provenance for Published Learnings

## Status

Accepted (Revised after GPT-5 Review)

Date: 2025-01-16
Review Date: 2025-01-16

## Context

The Global Context Network enables users to publish sanitized learnings to a distributed network (IPFS + blockchain) for global consumption. These learnings will be:

1. **Consumed by unknown parties** - Downloaded and used by AI agents worldwide
2. **Trusted as knowledge** - Integrated into agent context and decision-making
3. **Potentially high-impact** - Could influence critical workflows or decisions
4. **Subject to tampering** - IPFS content is immutable, but metadata/indexing can be manipulated
5. **Attributed to publishers** - Users need reputation and accountability

**The Trust Problem**: Without signing and provenance, consumers have no way to:
- Verify learnings came from the claimed publisher
- Detect tampering during transmission or storage
- Validate sanitization was performed correctly
- Trust the tooling versions used for extraction
- Build reputation systems or trust networks
- Audit the chain of custody from capture → sanitization → extraction → upload

**Real-World Attack Scenarios**:
- **Malicious injection**: Attacker uploads poisoned learnings claiming to be from trusted publisher
- **Metadata manipulation**: Blockchain metadata altered to point to malicious IPFS content
- **Sanitization bypass**: Learning uploaded without proper PII removal
- **Version mismatch**: Learning extracted with buggy/vulnerable extractor version
- **Replay attacks**: Old, outdated learnings re-uploaded as new
- **Reputation hijacking**: Attacker impersonates high-reputation publisher

**Current State**: MVP lacks any signing or attestation mechanism. Uploads are anonymous and unverifiable.

**External Review Recommendation**: GPT-5 holistic review (2025-01-16) specifically recommended an ADR for signing, attestations, and provenance to establish trust in the global network.

## Decision

Implement cryptographic signing and attestation system for all published learnings with per-device key management.

### Core Components

#### 1. Per-Device Signing Keys

**Key Generation**:
- Each device/installation generates an Ed25519 keypair on first run
- Private key stored securely in OS-native keychain
- Public key becomes the device's publishing identity
- Optional: User can link multiple devices to same identity (future)

**Key Storage**:
```typescript
interface KeyStorage {
  // OS-native secure storage
  platform: "macos-keychain" | "windows-dpapi" | "linux-keyring";

  // Keychain attributes
  service: "global-context-network";
  account: string; // deviceId or user-chosen identity

  // Private key encrypted by OS
  privateKey: Uint8Array; // Ed25519 private key (32 bytes)
}
```

**Platform-Specific Implementation**:
- **macOS**: Keychain Services API (`security add-generic-password`)
- **Windows**: DPAPI (Data Protection API) via node `dpapi` module
- **Linux**: Secret Service API (Freedesktop.org) via `libsecret` or `gnome-keyring`

**Fallback**: If OS keychain unavailable, use encrypted file with user-provided password (warn about security implications).

#### 2. Signature Format

Every published learning includes:

```typescript
interface SignedLearning {
  // Core content
  learning: {
    id: string;              // ULID
    content: string;         // Sanitized markdown
    tags: string[];
    category: string;
    extractedAt: string;     // ISO-8601
  };

  // Provenance metadata (sourceConversationId EXCLUDED for privacy)
  provenance: {
    sanitizerVersion: string;      // "rules-v1.2+ai-v2.0"
    extractorVersion: string;      // "learning-extractor-v1.0"
    claudeCodeVersion: string;     // "1.2.3"
    timestamp: string;             // ISO-8601 signing time (SINGLE SOURCE OF TRUTH)
    toolchainCommitSha: string;    // Git commit of sanitizer/extractor
    lockfileHash: string;          // Hash of package-lock.json for reproducibility
    nodeVersion: string;           // Node.js runtime version
  };

  // Consent and licensing (required per STANDARDS)
  consent: {
    license: string;               // "CC-BY-4.0" or "ODC-By"
    consentType: string;           // "explicit" | "implicit"
  };

  // Attestations
  attestations: {
    sanitizationEvidence: {
      rulesVersion: string;        // "v1.2"
      aiModelVersion: string;      // "claude-3.5-sonnet-20250115"
      detectionsByCategory: Record<string, number>; // { EMAIL: 2, API_KEY: 1 }
      totalDetections: number;     // Total PII items redacted
      confidence: number;          // 0-1 sanitization confidence
      auditPassed: boolean;        // Post-ingest audit passed
      sanitizationLogMerkleRoot: string; // Merkle root of sanitization log entries
    };

    extractionEvidence: {
      modelVersion: string;        // "claude-3.5-sonnet-20250115"
      promptVersion: string;       // "learning-extractor-v1.0"
      temperature: number;         // 0.0-1.0
      reviewStatus: string;        // "auto" | "reviewed" | "approved"
    };

    // Integrity checks (EXCLUDED from metadataHash computation to avoid circularity)
    integrityChecks: {
      contentHash: string;         // SHA-256 of learning.content
      metadataHash: string;        // SHA-256 of RFC 8785 canonical JSON (provenance + consent + attestations WITHOUT integrityChecks)
    };
  };

  // Cryptographic signature
  signature: {
    algorithm: "ed25519";          // Fixed for MVP (enables algorithm agility)
    hashAlgorithm: "sha256";       // Hash function used (SHA-256)
    canonicalization: "rfc8785";   // RFC 8785 canonical JSON (prevents malleability)
    publicKey: string;             // Ed25519 public key (32 bytes, base64url encoded)
    signatureValue: string;        // Ed25519 signature (64 bytes, base64url encoded)
    keyId: string;                 // Stable key ID: "ed25519:base64url(sha256(publicKey))"
    schemaVersion: "1";            // Schema version for migrations
  };
}

/**
 * CRITICAL DESIGN NOTES (per GPT-5 review):
 *
 * 1. Metadata Hash Circularity Prevention:
 *    - metadataHash is computed EXCLUDING integrityChecks
 *    - metadataBody = { provenance, consent, attestations WITHOUT integrityChecks }
 *
 * 2. Single Timestamp Source of Truth:
 *    - ONE timestamp (provenance.timestamp) used in both signing and verification
 *    - Avoid creating separate timestamps in signingPayload
 *
 * 3. Canonical JSON Throughout:
 *    - ALL hashing and signing uses RFC 8785 canonicalize()
 *    - NEVER use JSON.stringify() for cryptographic operations
 *
 * 4. UTF-8 Encoding:
 *    - Sign UTF-8 bytes of canonical JSON, not strings
 *    - Use TextEncoder().encode(canonicalJSON(payload))
 *
 * 5. Privacy Protection:
 *    - sourceConversationId is EXCLUDED from published artifact
 *    - deviceId is EXCLUDED or made clearly pseudonymous
 */
```

**Signing Process** (Fixed per GPT-5 Review):
```typescript
import canonicalize from 'canonicalize'; // RFC 8785 canonical JSON
import * as ed from '@noble/ed25519';
import { sha256 } from '@noble/hashes/sha256';

async function signLearning(learning: Learning): Promise<SignedLearning> {
  // 1. Load private key from OS keychain (using keytar, NOT CLI)
  const privateKey = await loadPrivateKey();
  const publicKey = await ed.getPublicKey(privateKey);

  // 2. Build provenance metadata with SINGLE timestamp (SOURCE OF TRUTH)
  const timestamp = new Date().toISOString(); // ISO-8601
  const provenance = {
    sanitizerVersion: SANITIZATION_VERSION,
    extractorVersion: LEARNING_EXTRACTOR_VERSION,
    claudeCodeVersion: CLAUDE_CODE_VERSION,
    timestamp, // CRITICAL: This timestamp used everywhere (signing, verification)
    toolchainCommitSha: await getGitCommitSha(),
    lockfileHash: await getLockfileHash(),
    nodeVersion: process.version
  };

  // 3. Build attestations WITHOUT integrityChecks (avoid circularity)
  const attestations = await buildAttestations(learning);

  // 4. Build consent object (required per STANDARDS.md)
  const consent = {
    license: "CC-BY-4.0",
    consentType: "explicit" as const
  };

  // 5. Compute content hash (SHA-256 of UTF-8 bytes)
  const contentHash = bytesToHex(sha256(new TextEncoder().encode(learning.content)));

  // 6. Build metadata body (EXCLUDING integrityChecks to avoid circularity)
  const metadataBody = {
    provenance,
    consent,
    attestations: {
      sanitizationEvidence: attestations.sanitizationEvidence,
      extractionEvidence: attestations.extractionEvidence
      // CRITICAL: integrityChecks NOT included - would create circular dependency
    }
  };

  // 7. Compute metadata hash using RFC 8785 canonical JSON
  const canonicalMetadata = canonicalize(metadataBody);
  if (!canonicalMetadata) throw new Error('Failed to canonicalize metadata');

  const metadataHash = bytesToHex(
    sha256(new TextEncoder().encode(canonicalMetadata))
  );

  // 8. Derive key ID (stable, derivable identifier)
  const keyId = "ed25519:" + bytesToBase64Url(sha256(publicKey));

  // 9. Create canonical signing payload with domain separation
  const signingPayload = {
    context: "gcn.signedlearning.v1", // Domain separation (prevents cross-protocol replay)
    schemaVersion: "1",                // Schema versioning for migrations
    hashAlgorithm: "sha256",           // Explicitly document hash algorithm
    canonicalization: "rfc8785",       // Explicitly document canonicalization
    learningId: learning.id,           // ULID
    contentHash,                       // SHA-256 hex
    metadataHash,                      // SHA-256 hex
    timestamp: provenance.timestamp,   // CRITICAL: Use SAME timestamp (not new Date())
    keyId                              // Stable key identifier
  };

  // 10. Sign the UTF-8 bytes of RFC 8785 canonical JSON
  const canonicalPayload = canonicalize(signingPayload);
  if (!canonicalPayload) throw new Error('Failed to canonicalize signing payload');

  const payloadBytes = new TextEncoder().encode(canonicalPayload);
  const signatureValue = await ed.sign(payloadBytes, privateKey);

  // 11. Construct signed learning (integrityChecks added AFTER hashing)
  return {
    learning,
    provenance,
    consent,
    attestations: {
      sanitizationEvidence: attestations.sanitizationEvidence,
      extractionEvidence: attestations.extractionEvidence,
      integrityChecks: { contentHash, metadataHash } // Added AFTER metadataHash computation
    },
    signature: {
      algorithm: "ed25519",
      hashAlgorithm: "sha256",
      canonicalization: "rfc8785",
      publicKey: bytesToBase64Url(publicKey),           // 32 bytes base64url
      signatureValue: bytesToBase64Url(signatureValue), // 64 bytes base64url
      keyId,
      schemaVersion: "1"
    }
  };
}

// Helper functions
function bytesToHex(bytes: Uint8Array): string {
  return Array.from(bytes).map(b => b.toString(16).padStart(2, '0')).join('');
}

function bytesToBase64Url(bytes: Uint8Array): string {
  return Buffer.from(bytes)
    .toString('base64')
    .replace(/\+/g, '-')
    .replace(/\//g, '_')
    .replace(/=/g, '');
}

function base64UrlToBytes(str: string): Uint8Array {
  const base64 = str.replace(/-/g, '+').replace(/_/g, '/');
  return new Uint8Array(Buffer.from(base64, 'base64'));
}
```

**Verification Process** (Fixed per GPT-5 Review):
```typescript
async function verifyLearning(signed: SignedLearning): Promise<VerificationResult> {
  // 1. Recompute content hash (SHA-256 of UTF-8 bytes)
  const contentHash = bytesToHex(
    sha256(new TextEncoder().encode(signed.learning.content))
  );

  // 2. Verify content hash matches (detect content tampering)
  if (contentHash !== signed.attestations.integrityChecks.contentHash) {
    return { valid: false, reason: "Content hash mismatch - content tampered" };
  }

  // 3. Rebuild metadata body (EXCLUDING integrityChecks - same as signing)
  const metadataBody = {
    provenance: signed.provenance,
    consent: signed.consent,
    attestations: {
      sanitizationEvidence: signed.attestations.sanitizationEvidence,
      extractionEvidence: signed.attestations.extractionEvidence
      // CRITICAL: integrityChecks NOT included - matches signing logic
    }
  };

  // 4. Recompute metadata hash using RFC 8785 canonical JSON
  const canonicalMetadata = canonicalize(metadataBody);
  if (!canonicalMetadata) {
    return { valid: false, reason: "Failed to canonicalize metadata for verification" };
  }

  const metadataHash = bytesToHex(
    sha256(new TextEncoder().encode(canonicalMetadata))
  );

  // 5. Verify metadata hash matches (detect metadata tampering)
  if (metadataHash !== signed.attestations.integrityChecks.metadataHash) {
    return { valid: false, reason: "Metadata hash mismatch - metadata tampered" };
  }

  // 6. Reconstruct signing payload (EXACTLY as in signing process)
  const signingPayload = {
    context: "gcn.signedlearning.v1",              // Domain separation
    schemaVersion: signed.signature.schemaVersion, // Schema version
    hashAlgorithm: signed.signature.hashAlgorithm, // "sha256"
    canonicalization: signed.signature.canonicalization, // "rfc8785"
    learningId: signed.learning.id,                // ULID
    contentHash,                                   // Recomputed
    metadataHash,                                  // Recomputed
    timestamp: signed.provenance.timestamp,        // CRITICAL: Use SAME timestamp from provenance
    keyId: signed.signature.keyId                  // Key identifier
  };

  // 7. Verify Ed25519 signature on UTF-8 bytes of RFC 8785 canonical JSON
  const canonicalPayload = canonicalize(signingPayload);
  if (!canonicalPayload) {
    return { valid: false, reason: "Failed to canonicalize signing payload for verification" };
  }

  const payloadBytes = new TextEncoder().encode(canonicalPayload);
  const publicKeyBytes = base64UrlToBytes(signed.signature.publicKey);
  const signatureBytes = base64UrlToBytes(signed.signature.signatureValue);

  const signatureValid = await ed.verify(signatureBytes, payloadBytes, publicKeyBytes);

  if (!signatureValid) {
    return { valid: false, reason: "Invalid Ed25519 signature - not signed by claimed key" };
  }

  // 8. Verify key ID matches public key (detect public key substitution)
  const derivedKeyId = "ed25519:" + bytesToBase64Url(sha256(publicKeyBytes));
  if (derivedKeyId !== signed.signature.keyId) {
    return { valid: false, reason: "Key ID mismatch - public key altered" };
  }

  // 9. Verify timestamp freshness (optional policy - configurable)
  const signedAt = new Date(signed.provenance.timestamp);
  const age = Date.now() - signedAt.getTime();
  const MAX_AGE_MS = 30 * 24 * 60 * 60 * 1000; // 30 days
  if (age > MAX_AGE_MS) {
    return { valid: true, warning: "Signature older than 30 days" };
  }

  // 10. Verify consent/license is present (required per STANDARDS.md)
  if (!signed.consent || !signed.consent.license) {
    return { valid: true, warning: "Missing license information (STANDARDS violation)" };
  }

  // 11. Verify algorithm support (reject unknown algorithms)
  if (signed.signature.algorithm !== "ed25519") {
    return { valid: false, reason: `Unsupported signature algorithm: ${signed.signature.algorithm}` };
  }

  if (signed.signature.hashAlgorithm !== "sha256") {
    return { valid: false, reason: `Unsupported hash algorithm: ${signed.signature.hashAlgorithm}` };
  }

  return { valid: true };
}
```

#### 3. On-Chain Provenance Registry

**Blockchain Smart Contract** (Fixed per GPT-5 Review):
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.20;

/**
 * @title LearningRegistry
 * @notice On-chain registry for signed learnings with Ed25519 public keys
 * @dev Ed25519 signature verification happens OFF-CHAIN (no EVM precompile)
 *      Contract stores hashes and metadata for integrity verification
 *      Publisher uses separate EVM key for transactions
 */
contract LearningRegistry {
  struct LearningRecord {
    bytes32 contentHash;      // SHA-256 of learning content
    bytes32 metadataHash;     // SHA-256 of metadata (provenance + attestations)
    address publisher;        // EVM address (tx signer, NOT derived from Ed25519)
    bytes ipfsCid;            // IPFS CID (multihash encoded, or use string for readability)
    uint256 timestamp;        // block.timestamp (authoritative)
    bytes publicKey;          // Ed25519 public key (32 bytes) for off-chain verification
    bytes32 keyId;            // sha256(publicKey) for key rotation tracking
    uint8 schemaVersion;      // Schema version (1 for MVP)
  }

  // ULID stored as bytes16 (128 bits)
  mapping(bytes16 => LearningRecord) public learnings;

  // Publisher reputation (EVM address -> score)
  mapping(address => uint256) public reputationScores;

  // Key ID to publisher mapping (for key rotation)
  mapping(bytes32 => address) public keyIdToPublisher;

  // Per-publisher nonce for replay protection
  mapping(address => uint256) public publisherNonces;

  event LearningPublished(
    bytes16 indexed learningId,
    address indexed publisher,
    bytes32 indexed keyId,
    bytes ipfsCid,
    bytes32 contentHash,
    bytes32 metadataHash,
    uint256 nonce
  );

  event PublisherRegistered(
    address indexed publisher,
    bytes32 indexed keyId,
    bytes publicKey
  );

  /**
   * @notice Publish a signed learning to the registry
   * @param learningId ULID as bytes16 (128 bits)
   * @param contentHash SHA-256 hash of learning content
   * @param metadataHash SHA-256 hash of metadata
   * @param ipfsCid IPFS content identifier (bytes or string)
   * @param publicKey Ed25519 public key (32 bytes)
   * @dev Signature verification happens OFF-CHAIN by consumers
   */
  function publishLearning(
    bytes16 learningId,
    bytes32 contentHash,
    bytes32 metadataHash,
    bytes calldata ipfsCid,
    bytes calldata publicKey
  ) external {
    require(publicKey.length == 32, "Invalid Ed25519 public key length");
    require(learnings[learningId].timestamp == 0, "Already published");

    // Derive key ID
    bytes32 keyId = sha256(publicKey);

    // Register publisher-key association (first time)
    if (keyIdToPublisher[keyId] == address(0)) {
      keyIdToPublisher[keyId] = msg.sender;
      emit PublisherRegistered(msg.sender, keyId, publicKey);
    } else {
      require(keyIdToPublisher[keyId] == msg.sender, "Key ID already claimed");
    }

    // Get publisher nonce
    uint256 nonce = publisherNonces[msg.sender];
    publisherNonces[msg.sender]++;

    // Store learning record
    learnings[learningId] = LearningRecord({
      contentHash: contentHash,
      metadataHash: metadataHash,
      publisher: msg.sender,
      ipfsCid: ipfsCid,
      timestamp: block.timestamp,
      publicKey: publicKey,
      keyId: keyId,
      schemaVersion: 1
    });

    emit LearningPublished(
      learningId,
      msg.sender,
      keyId,
      ipfsCid,
      contentHash,
      metadataHash,
      nonce
    );
  }

  /**
   * @notice Verify learning hashes match on-chain record
   * @param learningId ULID as bytes16
   * @param providedContentHash Content hash to verify
   * @param providedMetadataHash Metadata hash to verify
   * @return True if hashes match and record exists
   */
  function verifyLearning(
    bytes16 learningId,
    bytes32 providedContentHash,
    bytes32 providedMetadataHash
  ) external view returns (bool) {
    LearningRecord memory record = learnings[learningId];
    return
      record.timestamp > 0 &&
      record.contentHash == providedContentHash &&
      record.metadataHash == providedMetadataHash;
  }

  /**
   * @notice Get full learning record
   * @param learningId ULID as bytes16
   */
  function getLearning(bytes16 learningId)
    external
    view
    returns (LearningRecord memory)
  {
    return learnings[learningId];
  }
}
```

**Key Design Decisions** (Hybrid Approach per GPT-5 Review):
- **Separate keys**: Ed25519 for content signing (off-chain), separate secp256k1/EVM key for transactions
  - CRITICAL: Cannot derive EVM address from Ed25519 key (different elliptic curves)
  - Publisher uses their EVM wallet (MetaMask, etc.) to submit transactions
  - Ed25519 public key stored on-chain for verification reference
- **Off-chain signature verification**: Consumers verify Ed25519 signatures using IPFS content, NOT on-chain
  - No Ed25519 EVM precompile exists (would be too expensive anyway)
  - Contract stores hashes and public key for integrity verification
- **On-chain integrity anchoring**: Contract stores contentHash, metadataHash, and Ed25519 public key
- **Key-to-publisher binding**: First EVM address to use a keyId owns it (prevents impersonation)
- **Nonce for replay protection**: Per-publisher nonce prevents replay attacks
- **ULID encoding**: bytes16 (128 bits) for gas efficiency

**Integration with Upload Flow** (Fixed per GPT-5 Review):
```typescript
async function uploadSignedLearning(signed: SignedLearning) {
  // 1. Upload to IPFS (full signed learning JSON)
  const ipfsCid = await ipfs.add(JSON.stringify(signed));

  // 2. Get user's EVM wallet (SEPARATE from Ed25519 signing key)
  // User must connect wallet (MetaMask, WalletConnect, etc.)
  const evmWallet = await getConnectedEVMWallet(); // secp256k1 key
  const publisherAddress = await evmWallet.getAddress();

  // 3. Convert ULID to bytes16 for Solidity
  const learningIdBytes16 = ulidToBytes16(signed.learning.id);

  // 4. Convert hashes to bytes32
  const contentHashBytes32 = hexToBytes32(signed.attestations.integrityChecks.contentHash);
  const metadataHashBytes32 = hexToBytes32(signed.attestations.integrityChecks.metadataHash);

  // 5. Convert IPFS CID to bytes (multihash encoding)
  const ipfsCidBytes = cidToBytes(ipfsCid);

  // 6. Decode Ed25519 public key from base64url
  const publicKeyBytes = base64UrlToBytes(signed.signature.publicKey); // 32 bytes

  // 7. Register on blockchain (signed by EVM key, stores Ed25519 pubkey)
  const tx = await learningRegistry.publishLearning(
    learningIdBytes16,
    contentHashBytes32,
    metadataHashBytes32,
    ipfsCidBytes,
    publicKeyBytes
  );

  await tx.wait();

  // 8. Store upload record
  await db.uploads.insert({
    learningId: signed.learning.id,
    ipfsCid,
    blockchainTxHash: tx.hash,
    evmPublisher: publisherAddress,           // EVM address (tx signer)
    ed25519PublicKey: signed.signature.publicKey, // Ed25519 pubkey (content signer)
    uploadedAt: new Date().toISOString()
  });
}
```

#### 4. Attestation Generation

**Sanitization Evidence**:
```typescript
async function buildSanitizationEvidence(
  conversationId: string
): Promise<SanitizationEvidence> {
  // Query sanitization log for this conversation
  const sanitizationRecords = await db.sanitizationLog.findMany({
    conversationId
  });

  // Aggregate evidence
  const totalDetections = sanitizationRecords.reduce(
    (sum, r) => sum + r.detectionsCount,
    0
  );

  const avgConfidence = sanitizationRecords.reduce(
    (sum, r) => sum + r.confidence,
    0
  ) / sanitizationRecords.length;

  // Check if post-ingest audit passed
  const auditPassed = await checkAuditStatus(conversationId);

  return {
    rulesVersion: SANITIZATION_RULES_VERSION,
    aiModelVersion: SANITIZATION_AI_MODEL,
    detectionsCount: totalDetections,
    confidence: avgConfidence,
    auditPassed
  };
}
```

**Extraction Evidence**:
```typescript
async function buildExtractionEvidence(
  learningId: string
): Promise<ExtractionEvidence> {
  // Query job queue for extraction job
  const job = await db.jobQueue.findOne({
    type: 'extract_learning',
    targetId: learningId
  });

  return {
    modelVersion: job.modelVersion || LEARNING_EXTRACTOR_MODEL,
    promptVersion: LEARNING_EXTRACTOR_PROMPT_VERSION,
    temperature: job.temperature || 0.0,
    reviewStatus: job.reviewStatus || 'auto'
  };
}
```

#### 5. Trust and Reputation (Future)

**Publisher Reputation System** (post-MVP):
- Track download counts, citations, user feedback
- Compute reputation score based on:
  - Number of published learnings
  - Quality ratings from consumers
  - Citation count (how often learnings are referenced)
  - Longevity (how long publisher has been active)
  - Verification history (zero violations = boost)

**Web of Trust** (post-MVP):
- Users can endorse/vouch for publishers
- Transitive trust: "I trust publishers that Alice trusts"
- Dispute resolution for reported malicious content

## GPT-5 Review Summary

**Review Date**: 2025-01-16
**Model**: gpt-5
**Verdict**: Strong direction and threat model with critical corrections required
**Review File**: `/docs/reviews/gpt5-adr-011-review-2025-01-16.txt`

### Critical Issues Fixed

#### 1. **Metadata Hash Circularity** ✅ FIXED
**Issue**: metadataHash included integrityChecks which contained metadataHash (circular dependency)
**Impact**: Verification would always fail - cannot compute hash of object containing itself
**Fix**:
- Exclude integrityChecks from metadataBody when computing metadataHash
- Build metadataBody = { provenance, consent, attestations WITHOUT integrityChecks }
- Add integrityChecks AFTER computing metadataHash
- Document explicitly in code comments

**Code Location**: `signLearning()` step 6, `verifyLearning()` step 3

#### 2. **Timestamp Mismatch** ✅ FIXED
**Issue**: signLearning created separate timestamps for provenance and signingPayload, causing signature verification to fail
**Impact**: Signatures would not verify due to timestamp mismatch
**Fix**:
- Create ONE timestamp (provenance.timestamp) as single source of truth
- Use provenance.timestamp in signingPayload (not new Date())
- Document "SINGLE SOURCE OF TRUTH" in comments

**Code Location**: `signLearning()` step 2 and 9, `verifyLearning()` step 6

#### 3. **Canonicalization Inconsistency** ✅ FIXED
**Issue**: Used JSON.stringify() instead of RFC 8785 canonical JSON for hashing
**Impact**: Same object could produce different hashes (key ordering, whitespace)
**Fix**:
- Use canonicalize() library (RFC 8785) for ALL cryptographic operations
- NEVER use JSON.stringify() for hashing or signing
- Add null checks for canonicalization failures

**Code Location**: All hashing and signing operations

#### 4. **CLI Secret Exposure** ✅ FIXED
**Issue**: macOS `security -w "secret"` exposed private key via process args (visible in ps, shell history)
**Impact**: Critical security vulnerability - private keys leaked to all system users
**Fix**:
- Replace CLI with keytar library (native Keychain Services API)
- No CLI invocation, no process args, no shell history leakage
- Document security issue and solution in code comments

**Code Location**: Appendix, macOS Keychain section

#### 5. **EVM Identity Mismatch** ✅ FIXED
**Issue**: Cannot derive EVM address from Ed25519 public key (different elliptic curves)
**Impact**: Upload flow would fail - cannot sign transactions with Ed25519 key
**Fix**:
- Hybrid approach: Ed25519 for content signing (off-chain), separate secp256k1/EVM key for transactions
- Publisher uses EVM wallet (MetaMask) to submit transactions
- Store Ed25519 public key on-chain for verification reference
- Verification happens OFF-CHAIN (no EVM precompile for Ed25519)

**Code Location**: Smart contract design, upload flow integration

#### 6. **Missing Provenance Fields** ✅ FIXED
**Issue**: Lacked reproducibility metadata (toolchain commit SHA, lockfile hash, Node version)
**Impact**: Cannot reproduce builds or verify supply chain integrity
**Fix**:
- Added toolchainCommitSha: Git commit of sanitizer/extractor code
- Added lockfileHash: SHA-256 of package-lock.json
- Added nodeVersion: Node.js runtime version
- Enables SLSA/in-toto attestations in future

**Code Location**: Provenance interface, signLearning() step 2

#### 7. **Privacy Concerns** ✅ FIXED
**Issue**: sourceConversationId and deviceId create linkability between learnings
**Impact**: Can deanonymize publishers by correlating published learnings
**Fix**:
- EXCLUDE sourceConversationId from published artifact (keep local-only)
- EXCLUDE deviceId or make it clearly pseudonymous
- Document privacy protection in interface comments

**Code Location**: SignedLearning interface, provenance metadata

#### 8. **Missing Consent/License** ✅ FIXED
**Issue**: No license or consent fields (required per STANDARDS.md)
**Impact**: Published learnings lack licensing information (compliance violation)
**Fix**:
- Added consent object with license and consentType
- Default to "CC-BY-4.0" or "ODC-By"
- Verify consent/license present during verification
- Document requirement per STANDARDS.md

**Code Location**: SignedLearning interface, signLearning() step 4, verifyLearning() step 10

### Strengths Highlighted by GPT-5

- Clear articulation of trust problem and threat scenarios
- Sensible choice of Ed25519 and RFC 8785 canonicalization
- Good testing strategy with property-based tests
- Phased implementation allows iterative hardening
- On-chain anchoring of both content and metadata hashes

### Additional Recommendations Implemented

#### Cryptographic Best Practices
- **Domain separation tag**: "gcn.signedlearning.v1" prevents cross-protocol replay attacks
- **Schema versioning**: schemaVersion field enables future migrations
- **Algorithm agility**: algorithm, hashAlgorithm, canonicalization fields explicit
- **Key ID derivation**: keyId = "ed25519:base64url(sha256(publicKey))" for stable identification
- **Consistent encoding**: base64url for all binary data (publicKey, signature)
- **UTF-8 byte signing**: Sign UTF-8 bytes of canonical JSON, not strings

#### Attestation Improvements
- **Merkle root**: sanitizationLogMerkleRoot for tamper-proof log anchoring
- **Redaction summary**: counts by category (EMAIL, API_KEY, etc.)
- **Toolchain provenance**: Git commit SHA, lockfile hash, Node version

#### Blockchain Optimizations
- **Per-publisher nonce**: Prevents replay attacks and enables freshness checks
- **ULID as bytes16**: 128-bit encoding for gas efficiency (not string or bytes32)
- **bytes32 for hashes**: Native Solidity type for SHA-256 hashes
- **bytes for publicKey**: 32-byte Ed25519 public key (not string)

#### Verification Policies
- **Algorithm validation**: Reject unsupported algorithms during verification
- **Timestamp freshness**: Optional policy (30-day default, configurable)
- **Consent validation**: Verify license/consent fields present (STANDARDS compliance)
- **Hash matching**: Verify both contentHash and metadataHash

### Future Enhancements Suggested by GPT-5

#### Standards-Based Envelopes (Post-MVP)
- Consider DSSE (Dead Simple Signing Envelope) or COSE_Sign1
- Reduces custom envelope risks
- Standards-based migration path

#### Decentralized Identity (Post-MVP)
- DIDs + Verifiable Credentials for organizational publishers
- No KYC/OIDC requirement
- Supports Web3 ethos

#### Supply Chain Security (Future)
- SLSA attestations for toolchain integrity
- in-toto framework for build provenance
- Signed releases and reproducible builds

#### Additional Risks to Document
- Canonicalization drift across languages/platforms (add conformance tests)
- IPFS pinning/liveness strategy (multiple providers, retention policy)
- Supply chain compromise (signed releases, provenance attestations)

## Consequences

### Positive

- **Verifiable authenticity** - Consumers can verify learnings from claimed publisher
- **Tamper detection** - Any modification invalidates signature
- **Accountability** - Publishers responsible for their content
- **Reputation building** - Good publishers build trust over time
- **Audit trail** - Full provenance from capture to upload
- **Tool version transparency** - Consumers know what versions created the learning
- **Sanitization confidence** - Attestations prove PII removal
- **Attack resistance** - Cryptographic signatures prevent impersonation
- **Compliance support** - Chain of custody for data lineage requirements
- **Trust network foundation** - Enables future reputation/endorsement systems

### Negative

- **Implementation complexity** - Signing, key management, verification logic
- **Key management burden** - Users must protect private keys (though OS handles this)
- **Platform dependencies** - Different keychains per OS
- **Storage overhead** - Signatures and attestations increase payload size (~500 bytes)
- **Verification cost** - Consumers must verify signatures (though fast with Ed25519)
- **Key rotation complexity** - Changing keys requires migration strategy
- **Privacy tradeoff** - Public keys create persistent identities (but pseudonymous)
- **Blockchain cost** - Gas fees for on-chain registration (mitigated by L2s)

### Neutral

- **Device-level identity** - Each device has separate identity (could be feature or bug)
- **No central authority** - Decentralized trust model (aligns with Web3 philosophy)
- **Optional verification** - Consumers can choose to skip verification (not recommended)
- **Signature algorithm locked** - Ed25519 chosen for MVP (could support others later)

## Alternatives Considered

### Alternative 1: No Signing (Status Quo)

**Description**: Publish learnings without signatures or attestations.

**Pros**:
- Simple implementation
- No key management
- Zero overhead

**Cons**:
- **Zero trust** - No way to verify authenticity
- **Impersonation attacks** - Anyone can claim to be anyone
- **No accountability** - Publishers can deny their uploads
- **No reputation** - Can't build trust networks
- **Malicious content** - Poisoned learnings indistinguishable from legitimate

**Why not chosen**: Unacceptable for a trustworthy knowledge network. External review specifically called this out.

### Alternative 2: Centralized PKI (Certificate Authority)

**Description**: Use traditional PKI with a central Certificate Authority.

**Pros**:
- Well-established standards (X.509)
- Revocation via CRLs/OCSP
- Strong identity binding
- Trusted by enterprises

**Cons**:
- **Centralization** - Single point of failure/control
- **Contradicts Web3 ethos** - Defeats purpose of decentralized network
- **Requires KYC** - CA must verify identities
- **Privacy violation** - Real-world identities tied to uploads
- **Cost** - Certificate issuance fees
- **Complexity** - Full X.509 infrastructure

**Why not chosen**: Contradicts decentralized architecture. Introduces centralized trust dependency.

### Alternative 3: Keyless Signing (Sigstore/Fulcio Model)

**Description**: Use ephemeral keys with OIDC identity binding (like Sigstore).

**Pros**:
- No long-term key management
- Identity tied to existing OIDC providers (Google, GitHub)
- Transparency log for auditability
- Modern approach

**Cons**:
- **Requires OIDC provider** - Not always available/desired
- **Privacy issues** - Real identity leaked via OIDC claims
- **Complexity** - Fulcio CA, Rekor transparency log, etc.
- **Dependency** - Relies on external services
- **Not fully decentralized** - OIDC providers are centralized

**Why not chosen**: Too complex for MVP. Privacy concerns with OIDC identity. **Future consideration**: Could add as optional identity binding mechanism post-MVP.

### Alternative 4: Blockchain-Native Signing (Ethereum Keys)

**Description**: Use Ethereum keypairs (secp256k1) instead of Ed25519.

**Pros**:
- Native blockchain compatibility
- Existing wallet support (MetaMask)
- Single key for signing and transactions
- Standard in Web3 ecosystem

**Cons**:
- **Larger signatures** - secp256k1 signatures ~65 bytes vs Ed25519 ~64 bytes (marginal)
- **Slower verification** - secp256k1 slower than Ed25519
- **Key security** - Users must manage Ethereum private keys (risky)
- **Wallet dependency** - Requires wallet software

**Why not chosen**: **ACTUALLY A STRONG CONTENDER**. Could simplify architecture by using single Ethereum key. Revisit during implementation.

### Alternative 5: Multi-Signature Threshold Signing

**Description**: Require K-of-N signatures to publish (e.g., 2-of-3).

**Pros**:
- Higher security threshold
- Prevents single key compromise
- Supports organizational publishing

**Cons**:
- **Too complex for MVP** - Individual users don't need this
- **UX burden** - Coordinate multiple signers
- **Not applicable** - Single-user publishing doesn't need multi-sig

**Why not chosen**: Overkill for individual publisher model. **Future consideration**: Could support for organizational/team accounts.

### Alternative 6: Hardware Security Module (HSM) Storage

**Description**: Store private keys in hardware devices (YubiKey, TPM).

**Pros**:
- Maximum key security
- Hardware-backed operations
- Tamper resistance

**Cons**:
- **Requires hardware** - Not all users have HSMs
- **Complexity** - Platform-specific integrations
- **Cost** - Hardware purchase required
- **Portability** - Keys locked to specific device

**Why not chosen**: Too high barrier to entry. OS keychain provides adequate security for MVP. **Future consideration**: Optional HSM support for high-security users.

## Implementation Plan

### Phase 1: Key Management (MVP)

1. Generate Ed25519 keypair on first run
2. Store in OS keychain (platform-specific)
3. Provide key export/import for backup
4. Add key rotation capability

**Acceptance Criteria**:
- Keys generated and stored securely
- Private keys never exposed in logs/UI
- Cross-platform support (macOS, Windows, Linux)
- Key backup mechanism exists

### Phase 2: Signing & Verification (MVP)

1. Implement `signLearning()` function with RFC 8785 canonicalization
2. Implement `verifyLearning()` function with same canonicalization
3. Add signature to upload payload
4. Verify signatures on download/query
5. Implement helper functions (bytesToHex, bytesToBase64Url, etc.)
6. Add domain separation and schema versioning
7. Implement keyId derivation

**Acceptance Criteria**:
- All uploaded learnings have valid signatures
- Verification rejects tampered content
- Verification rejects invalid signatures
- Verification rejects timestamp mismatches
- Metadata hash excludes integrityChecks (no circularity)
- Single timestamp source of truth (provenance.timestamp)
- RFC 8785 canonical JSON used throughout
- Performance <100ms for signing, <50ms for verification
- Cross-implementation test vectors pass

### Phase 3: Attestations (MVP)

1. Collect sanitization evidence from `sanitization_log`
2. Collect extraction evidence from `job_queue`
3. Add toolchain provenance (commit SHA, lockfile hash, Node version)
4. Include consent object with license and consentType
5. Compute Merkle root of sanitization log entries
6. Include attestations in signed payload
7. Display attestations in MCP query results

**Acceptance Criteria**:
- Attestations include all required fields (per GPT-5 review)
- Sanitization confidence accurately reflects detection quality
- Tool versions match actual deployed versions
- Toolchain provenance (commitSha, lockfileHash, nodeVersion) present
- Consent object present with license (CC-BY-4.0 or ODC-By)
- Merkle root computed for sanitization log
- sourceConversationId EXCLUDED from published artifact (privacy)
- Attestations visible to consumers

### Phase 4: Blockchain Integration (MVP)

1. Deploy `LearningRegistry` smart contract to L2
2. Implement EVM wallet integration (MetaMask, WalletConnect)
3. Implement ULID to bytes16 conversion
4. Implement hash to bytes32 conversion
5. Implement CID to bytes encoding
6. Integrate contract calls into upload flow
7. Store on-chain transaction hashes
8. Verify on-chain records match IPFS content

**Acceptance Criteria**:
- Learnings registered on-chain with correct hashes (bytes32)
- IPFS CID matches on-chain record
- Ed25519 public key stored on-chain (bytes, 32 bytes)
- EVM address (publisher) recorded correctly
- ULID encoded as bytes16 (gas efficiency)
- Hybrid key approach working (Ed25519 + EVM wallet)
- Per-publisher nonce prevents replay
- Verification possible via blockchain query
- Off-chain Ed25519 signature verification working

### Phase 5: Reputation System (Post-MVP)

1. Track download counts
2. Implement rating/feedback mechanism
3. Compute reputation scores
4. Display publisher reputation in MCP results

**Acceptance Criteria**:
- Publishers have visible reputation scores
- High-reputation publishers ranked higher
- Feedback mechanism prevents spam/abuse
- Zero-violation publishers get trust boost

## Risks and Mitigations

### Risk: Private Key Loss

**Impact**: Critical - User loses publishing identity and reputation

**Mitigation**:
- Mandatory key backup during setup
- Export/import functionality
- Optional multi-device key sharing (future)
- Reputation can be rebuilt with new key
- **No key recovery** - Explicitly document this

### Risk: Private Key Compromise

**Impact**: High - Attacker can impersonate publisher

**Mitigation**:
- OS keychain protection (requires user auth)
- Key rotation mechanism
- Revocation list on blockchain (future)
- Reputation hit for suspicious activity
- Users can publish key revocation notice

### Risk: Clock Skew / Timestamp Manipulation

**Impact**: Medium - Old learnings replayed as new

**Mitigation**:
- Blockchain timestamp is authoritative (block time)
- Local timestamp only for UX
- Consumers check blockchain timestamp
- Reject learnings with future timestamps

### Risk: Signature Algorithm Weakness (Future)

**Impact**: Low (Ed25519 currently secure) - Algorithm broken

**Mitigation**:
- Support multiple algorithms in schema
- Version field in signature structure
- Migration plan to new algorithm
- Monitor cryptography research

### Risk: Attestation Falsification

**Impact**: Medium - Publisher lies about sanitization

**Mitigation**:
- Consumers can re-run sanitization checks
- Reputation system penalizes false attestations
- Random audit sampling by network validators (future)
- Report mechanism for disputed learnings

### Risk: Blockchain Reorg / Finality Issues

**Impact**: Low - On-chain record disappears

**Mitigation**:
- Wait for sufficient confirmations (L2s have fast finality)
- Store IPFS CID as backup identifier
- Re-publish if reorg detected
- Use L2s with economic finality

## Testing Strategy

### Unit Tests

```typescript
describe('Signing and Verification', () => {
  it('should sign and verify a learning', async () => {
    const learning = createTestLearning();
    const signed = await signLearning(learning);
    const result = await verifyLearning(signed);

    expect(result.valid).toBe(true);
  });

  it('should reject tampered content', async () => {
    const signed = await signLearning(createTestLearning());
    signed.learning.content += ' TAMPERED';

    const result = await verifyLearning(signed);
    expect(result.valid).toBe(false);
    expect(result.reason).toContain('Content hash mismatch');
  });

  it('should reject tampered metadata', async () => {
    const signed = await signLearning(createTestLearning());
    signed.provenance.sanitizerVersion = 'v999.0'; // Tamper

    const result = await verifyLearning(signed);
    expect(result.valid).toBe(false);
    expect(result.reason).toContain('Metadata hash mismatch');
  });

  it('should reject invalid signature', async () => {
    const signed = await signLearning(createTestLearning());
    signed.signature.signatureValue = 'invalid';

    const result = await verifyLearning(signed);
    expect(result.valid).toBe(false);
    expect(result.reason).toContain('Invalid Ed25519 signature');
  });

  it('should reject public key substitution', async () => {
    const signed = await signLearning(createTestLearning());
    const fakeKey = ed.utils.randomPrivateKey();
    const fakePublicKey = await ed.getPublicKey(fakeKey);
    signed.signature.publicKey = bytesToBase64Url(fakePublicKey);

    const result = await verifyLearning(signed);
    expect(result.valid).toBe(false);
    expect(result.reason).toContain('Key ID mismatch');
  });

  // GPT-5 Fix: Metadata hash circularity prevention
  it('should exclude integrityChecks from metadataHash', async () => {
    const learning = createTestLearning();
    const signed = await signLearning(learning);

    // Verify integrityChecks is NOT in the metadata body used for hashing
    const metadataBody = {
      provenance: signed.provenance,
      consent: signed.consent,
      attestations: {
        sanitizationEvidence: signed.attestations.sanitizationEvidence,
        extractionEvidence: signed.attestations.extractionEvidence
        // integrityChecks NOT included
      }
    };

    const recomputedHash = bytesToHex(
      sha256(new TextEncoder().encode(canonicalize(metadataBody) || ''))
    );

    expect(recomputedHash).toBe(signed.attestations.integrityChecks.metadataHash);
  });

  // GPT-5 Fix: Single timestamp source of truth
  it('should use same timestamp in provenance and signingPayload', async () => {
    const learning = createTestLearning();
    const signed = await signLearning(learning);

    // Rebuild signing payload with same timestamp
    const signingPayload = {
      context: "gcn.signedlearning.v1",
      schemaVersion: signed.signature.schemaVersion,
      hashAlgorithm: signed.signature.hashAlgorithm,
      canonicalization: signed.signature.canonicalization,
      learningId: signed.learning.id,
      contentHash: signed.attestations.integrityChecks.contentHash,
      metadataHash: signed.attestations.integrityChecks.metadataHash,
      timestamp: signed.provenance.timestamp, // SAME timestamp
      keyId: signed.signature.keyId
    };

    // Verify signature with this payload
    const payloadBytes = new TextEncoder().encode(canonicalize(signingPayload) || '');
    const publicKeyBytes = base64UrlToBytes(signed.signature.publicKey);
    const signatureBytes = base64UrlToBytes(signed.signature.signatureValue);

    const valid = await ed.verify(signatureBytes, payloadBytes, publicKeyBytes);
    expect(valid).toBe(true);
  });

  // GPT-5 Fix: RFC 8785 canonicalization
  it('should produce same hash regardless of key order', () => {
    const obj1 = { b: 2, a: 1, c: 3 };
    const obj2 = { a: 1, c: 3, b: 2 };

    const canonical1 = canonicalize(obj1);
    const canonical2 = canonicalize(obj2);

    expect(canonical1).toBe(canonical2);
    expect(canonical1).toBe('{"a":1,"b":2,"c":3}'); // RFC 8785 format
  });

  // GPT-5 Fix: Consent/license validation
  it('should include consent and license', async () => {
    const learning = createTestLearning();
    const signed = await signLearning(learning);

    expect(signed.consent).toBeDefined();
    expect(signed.consent.license).toBe('CC-BY-4.0');
    expect(signed.consent.consentType).toBe('explicit');
  });

  it('should warn if license missing', async () => {
    const learning = createTestLearning();
    const signed = await signLearning(learning);
    delete signed.consent.license; // Remove license

    const result = await verifyLearning(signed);
    expect(result.valid).toBe(true); // Still valid signature
    expect(result.warning).toContain('Missing license information');
  });
});
```

### Integration Tests

```typescript
describe('End-to-End Signing Flow', () => {
  it('should sign, upload, download, and verify', async () => {
    // Sign
    const learning = await extractLearning(conversationId);
    const signed = await signLearning(learning);

    // Upload to IPFS + blockchain
    const { ipfsCid, txHash } = await uploadSignedLearning(signed);

    // Download from IPFS
    const downloaded = await ipfs.get(ipfsCid);

    // Verify signature
    const result = await verifyLearning(downloaded);
    expect(result.valid).toBe(true);

    // Verify blockchain record
    const onChainRecord = await learningRegistry.learnings(learning.id);
    expect(onChainRecord.ipfsCid).toBe(ipfsCid);
    expect(onChainRecord.contentHash).toBe(signed.attestations.integrityChecks.contentHash);
  });
});
```

### Property-Based Tests

```typescript
import fc from 'fast-check';

describe('Signing Properties', () => {
  it('signing is deterministic with same key', () => {
    fc.assert(
      fc.property(
        fc.string({ minLength: 100 }), // Learning content
        (content) => {
          const learning = { ...testLearning, content };
          const sig1 = signLearningSync(learning);
          const sig2 = signLearningSync(learning);

          // Same content + same key = same signature
          return sig1.signature.signatureValue === sig2.signature.signatureValue;
        }
      )
    );
  });

  it('any modification invalidates signature', () => {
    fc.assert(
      fc.property(
        fc.string({ minLength: 100 }),
        fc.nat(),
        (content, tamperIndex) => {
          const learning = { ...testLearning, content };
          const signed = signLearningSync(learning);

          // Tamper with one character
          const tampered = {
            ...signed,
            learning: {
              ...signed.learning,
              content: content.substring(0, tamperIndex % content.length) +
                       'X' +
                       content.substring((tamperIndex % content.length) + 1)
            }
          };

          const result = verifyLearningSync(tampered);
          return !result.valid;
        }
      )
    );
  });
});
```

### Performance Tests

```typescript
describe('Signing Performance', () => {
  it('should sign learning in <100ms', async () => {
    const learning = createTestLearning();

    const start = performance.now();
    await signLearning(learning);
    const duration = performance.now() - start;

    expect(duration).toBeLessThan(100);
  });

  it('should verify learning in <50ms', async () => {
    const signed = await signLearning(createTestLearning());

    const start = performance.now();
    await verifyLearning(signed);
    const duration = performance.now() - start;

    expect(duration).toBeLessThan(50);
  });
});
```

## Security Audit Checklist

Before production deployment:

### Cryptographic Implementation
- [ ] Ed25519 implementation reviewed (use audited library like `@noble/ed25519`)
- [ ] RFC 8785 canonical JSON used for ALL hashing and signing (NEVER JSON.stringify)
- [ ] Metadata hash excludes integrityChecks (no circular dependency)
- [ ] Single timestamp source of truth (provenance.timestamp)
- [ ] Domain separation tag ("gcn.signedlearning.v1") present
- [ ] Schema versioning fields (schemaVersion, algorithm, hashAlgorithm) present
- [ ] UTF-8 byte encoding used for signing (not strings)
- [ ] Base64url encoding consistent (publicKey, signatureValue)
- [ ] Key ID derivation correct (sha256 of public key)
- [ ] Hash algorithm (SHA-256) is collision-resistant
- [ ] Signature verification cannot be bypassed
- [ ] Cross-implementation test vectors pass

### Key Management
- [ ] Key storage tested on all platforms (macOS, Windows, Linux)
- [ ] keytar library used (NO CLI commands like `security -w`)
- [ ] Private keys never logged or exposed in UI
- [ ] Private keys never appear in process args or shell history
- [ ] Key export encrypted with Argon2id + AEAD
- [ ] Fallback file storage has chmod 0600 permissions
- [ ] Key rotation mechanism implemented
- [ ] Key backup mechanism exists

### Attestations & Provenance
- [ ] Attestation fields cannot be omitted
- [ ] Toolchain provenance included (commitSha, lockfileHash, nodeVersion)
- [ ] Consent object present with license and consentType
- [ ] sourceConversationId EXCLUDED from published artifact (privacy)
- [ ] deviceId EXCLUDED or clearly pseudonymous (privacy)
- [ ] Merkle root of sanitization log computed

### Blockchain Integration
- [ ] Hybrid key approach working (Ed25519 + EVM wallet)
- [ ] ULID encoded as bytes16 (gas efficiency)
- [ ] Hashes encoded as bytes32 (native Solidity type)
- [ ] Ed25519 public key stored as bytes (32 bytes, not string)
- [ ] Per-publisher nonce prevents replay
- [ ] Blockchain integration tested on testnet
- [ ] Off-chain Ed25519 verification working

### Testing & Performance
- [ ] Timestamp validation prevents replay attacks
- [ ] Property-based tests cover edge cases
- [ ] Performance budgets met (<100ms signing, <50ms verification)
- [ ] Metadata hash circularity tests pass
- [ ] Timestamp consistency tests pass
- [ ] Canonicalization consistency tests pass

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Upload & Distribution Architecture](../architecture/architecture-upload-distribution-2025-01-16.md) (to be created)

### Decisions
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md)
- [ADR-007: Consent and Licensing](./decision-consent-licensing-2025-01-16.md) (to be created)
- [ADR-010: Blockchain Selection](./decision-blockchain-selection-2025-01-16.md) (to be created)

### Plans
- [Phase 4: Global Distribution](../plans/plan-phase-4-global-distribution-2025-01-16.md) (to be created)

### Reference
- [Database Schema Reference](../reference/reference-database-schema-2025-01-16.md)
- [Cryptography Standards](../reference/reference-cryptography-standards-2025-01-16.md) (to be created)

### Reviews
- [GPT-5 Holistic Review](../reviews/gpt5-holistic-review-2025-01-16.txt)
- [GPT-5 ADR-011 Review](../reviews/gpt5-adr-011-review-2025-01-16.txt)

---

## Revision History

### 2025-01-16 - Post GPT-5 Review (CURRENT)
**Status**: Accepted (Revised)
**Changes**: Fixed 8 critical issues identified in GPT-5 security review

**Critical Fixes Applied**:
1. ✅ Metadata hash circularity - Exclude integrityChecks from metadataBody
2. ✅ Timestamp mismatch - Single timestamp source of truth (provenance.timestamp)
3. ✅ Canonicalization - RFC 8785 canonical JSON throughout
4. ✅ CLI secret exposure - keytar library (no CLI args leakage)
5. ✅ EVM identity mismatch - Hybrid approach (Ed25519 + EVM wallet)
6. ✅ Missing provenance - Added commitSha, lockfileHash, nodeVersion
7. ✅ Privacy concerns - Excluded sourceConversationId from published artifact
8. ✅ Missing consent - Added consent object with license/consentType

**Review File**: `/docs/reviews/gpt5-adr-011-review-2025-01-16.txt`

**Verdict**: "Strong direction and threat model with critical corrections required" - All corrections implemented.

### 2025-01-16 - Initial Draft
**Status**: Draft (Superseded)
**Changes**: Initial ADR based on GPT-5 holistic review recommendation

---

## Appendix: Key Management Details

### Platform-Specific Key Storage Examples

#### macOS Keychain (Fixed - No CLI Leakage)
```typescript
import keytar from 'keytar';

/**
 * CRITICAL: Use keytar library instead of CLI to avoid exposing secrets in process args
 *
 * SECURITY ISSUE (FIXED):
 * Previous approach used: execSync(`security add-generic-password -w "${secret}"`)
 * This leaked the private key via:
 * - Process arguments visible in `ps` output to all users
 * - Shell history files (~/.bash_history, ~/.zsh_history)
 *
 * SOLUTION:
 * keytar uses native Keychain Services API (Security.framework) via Node.js bindings
 * No CLI invocation, no process args, no shell history leakage
 */

async function storePrivateKey(privateKey: Uint8Array, deviceId: string) {
  const base64Key = Buffer.from(privateKey).toString('base64');

  // keytar uses native Keychain APIs directly (no CLI)
  await keytar.setPassword('global-context-network', deviceId, base64Key);
}

async function loadPrivateKey(deviceId: string): Promise<Uint8Array> {
  const base64Key = await keytar.getPassword('global-context-network', deviceId);

  if (!base64Key) {
    throw new Error('Private key not found in keychain');
  }

  return new Uint8Array(Buffer.from(base64Key, 'base64'));
}

async function deletePrivateKey(deviceId: string): Promise<boolean> {
  return await keytar.deletePassword('global-context-network', deviceId);
}
```

#### Windows DPAPI (with Secure Permissions)
```typescript
import { protectData, unprotectData } from 'dpapi-addon';
import fs from 'fs/promises';
import path from 'path';
import { execSync } from 'child_process';

async function storePrivateKey(privateKey: Uint8Array, deviceId: string) {
  const encrypted = protectData(Buffer.from(privateKey), null, 'CurrentUser');
  const keyPath = path.join(process.env.APPDATA!, 'GlobalContextNetwork', `${deviceId}.key`);

  // Create directory with restrictive permissions
  await fs.mkdir(path.dirname(keyPath), { recursive: true });

  // Write encrypted key
  await fs.writeFile(keyPath, encrypted);

  // CRITICAL: Restrict file access to current user only (Windows ACLs)
  // Remove inherited permissions and grant access only to current user
  try {
    execSync(`icacls "${keyPath}" /inheritance:r /grant:r "%USERNAME%:F"`);
  } catch (error) {
    console.warn('Failed to set restrictive permissions on key file:', error);
    // Continue - DPAPI CurrentUser scope provides some protection
  }
}

async function loadPrivateKey(deviceId: string): Promise<Uint8Array> {
  const keyPath = path.join(process.env.APPDATA!, 'GlobalContextNetwork', `${deviceId}.key`);
  const encrypted = await fs.readFile(keyPath);
  const decrypted = unprotectData(encrypted, null, 'CurrentUser');

  return new Uint8Array(decrypted);
}
```

#### Linux Secret Service
```typescript
import * as keytar from 'keytar';

async function storePrivateKey(privateKey: Uint8Array, deviceId: string) {
  const base64Key = Buffer.from(privateKey).toString('base64');
  await keytar.setPassword('global-context-network', deviceId, base64Key);
}

async function loadPrivateKey(deviceId: string): Promise<Uint8Array> {
  const base64Key = await keytar.getPassword('global-context-network', deviceId);
  if (!base64Key) throw new Error('Private key not found');

  return new Uint8Array(Buffer.from(base64Key, 'base64'));
}
```

### Canonical JSON Implementation

```typescript
import canonicalize from 'canonicalize';

function canonicalJSON(obj: any): string {
  // RFC 8785 compliant canonical JSON
  return canonicalize(obj) || '';
}

// Example
const payload = {
  learningId: 'abc123',
  contentHash: 'hash1',
  metadataHash: 'hash2',
  timestamp: '2025-01-16T12:00:00.000Z'
};

const canonical = canonicalJSON(payload);
// Always produces same string regardless of key order
```

### Ed25519 Library Selection

**Recommended**: `@noble/ed25519`

**Why**:
- Audited implementation
- No dependencies
- Fast (WASM when available)
- TypeScript native
- Active maintenance

**Example**:
```typescript
import * as ed from '@noble/ed25519';

// Generate keypair
const privateKey = ed.utils.randomPrivateKey();
const publicKey = await ed.getPublicKey(privateKey);

// Sign
const message = new TextEncoder().encode('Hello World');
const signature = await ed.sign(message, privateKey);

// Verify
const isValid = await ed.verify(signature, message, publicKey);
```
````

## File: decisions/decision-subagent-driven-development-2025-01-16.md
````markdown
---
title: ADR-002: Subagent-Driven Development
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [subagents, claude-agent-sdk, development-workflow, architecture]
---

# ADR-002: Subagent-Driven Development

## Status

Accepted

Date: 2025-01-16

## Context

The Global Context Network is a complex system with multiple specialized components:
- Event capture with strict latency constraints
- Privacy-critical sanitization pipeline
- Async job processing with reliability guarantees
- Learning extraction requiring domain expertise
- MCP server implementation
- Blockchain/IPFS integration

Each component requires:
- **Specialized expertise** in its domain
- **Comprehensive testing** with high coverage
- **Quality validation** at every step
- **Consistent implementation** across components
- **Parallel development** where possible

Traditional development approaches struggle with:
1. **Context limits** - Single agent can't hold all system knowledge
2. **Specialization** - One agent can't be expert in all domains
3. **Quality variance** - No built-in validation of implementation quality
4. **Sequential bottlenecks** - Can't parallelize independent tasks
5. **Test generation gaps** - Manual test writing is slow and incomplete

The system naturally decomposes into a DAG of asynchronous enrichment tasks (capture → sanitize → store → extract → publish) where independent agents map well to pipeline stages.

## Decision

Use specialized subagents (via Claude Agent SDK) for ALL implementation and testing.

**Core Principle**: Never implement directly - always delegate to specialized subagents.

**Subagent Types**:
1. **Implementation subagents** - Build features (hooks, sanitization, MCP, etc.)
2. **Test generation subagents** - Create comprehensive test suites
3. **Test validation subagents** - Validate test quality and coverage
4. **Quality gate subagents** - Enforce code standards and security
5. **Integration subagents** - Verify component interactions

**Orchestration**:
- Main agent coordinates subagent execution
- DAG orchestration with explicit inputs/outputs
- Idempotency contracts per subagent
- Parallel execution of independent subagents
- Sequential execution for dependencies

## Consequences

### Positive

- **Parallel execution** - Independent components built simultaneously
- **Specialized expertise** - Each subagent expert in its domain
- **Built-in validation** - Quality gates at every step
- **Comprehensive testing** - Automated test generation and validation
- **Consistent patterns** - Same approach across all components
- **Clear separation** - Focused responsibility per subagent
- **Auditability** - Complete trace of decisions and artifacts
- **Maintainability** - Easy to update individual subagents

### Negative

- **Orchestration complexity** - Managing subagent dependencies and DAG
- **Vendor dependency** - Requires Claude Agent SDK and API access
- **Cost** - API token usage for subagent-to-subagent communication
- **Non-determinism** - LLM outputs can vary between runs
- **Debugging complexity** - Harder to trace issues across subagents
- **Learning curve** - Team must understand subagent orchestration

### Neutral

- **Requires Claude Agent SDK** - Adds dependency on SDK
- **Prompt engineering** - Quality depends on subagent prompt design
- **Artifact storage** - Need to store subagent outputs and prompts
- **Versioning** - Must version subagent configurations

## Alternatives Considered

### Alternative 1: Human-Written Code with Automated Checks

**Description**: Developers write code manually, CI runs automated linting/tests.

**Pros**:
- Full control over implementation
- No LLM dependency
- Deterministic results
- Standard industry practice

**Cons**:
- Slower development (no parallel execution)
- Inconsistent quality across developers
- Manual test writing is time-consuming
- No automated test quality validation
- Harder to maintain consistency across components

**Why not chosen**: Too slow for MVP timeline, lacks automated test generation, no built-in quality validation.

### Alternative 2: Monolithic Agent (Single Large Prompt)

**Description**: One agent with a large prompt containing all system knowledge.

**Pros**:
- Simpler orchestration
- No coordination overhead
- Single context window

**Cons**:
- Context limits (can't hold entire system)
- No specialization
- Can't parallelize
- Lower quality (jack-of-all-trades)
- Loses focus across components

**Why not chosen**: Doesn't scale, can't parallelize, quality suffers from lack of specialization.

### Alternative 3: Orchestrator Frameworks (Temporal, Prefect, Dagster)

**Description**: Use workflow orchestration framework for task DAG.

**Pros**:
- Battle-tested orchestration
- Built-in retry and error handling
- Good observability
- Scales to production

**Cons**:
- Still need to write task code manually
- Doesn't provide specialized expertise
- No automated test generation
- Operational overhead (deploy orchestrator)
- Overkill for MVP

**Why not chosen**: Solves orchestration but not the core problem (specialized implementation and test generation). Could be adopted post-MVP.

### Alternative 4: Non-Claude Agent Ecosystems (LangChain, LlamaIndex, OpenAI Assistants)

**Description**: Use alternative agent frameworks.

**Pros**:
- Reduces vendor lock-in
- More ecosystem options
- Potentially lower cost

**Cons**:
- Less integrated with Claude Code
- Different quality characteristics
- Need to learn new frameworks
- May lack specialization features
- Migration cost if switching

**Why not chosen**: Claude Agent SDK integrates best with Claude Code, proven quality. Can evaluate alternatives post-MVP.

### Alternative 5: Reduced Surface (Few Composable Tools)

**Description**: One orchestrator agent with a few composable tools instead of many subagents.

**Pros**:
- Simpler architecture
- Easier to reason about
- Lower cost

**Cons**:
- Less specialization
- Can't parallelize as effectively
- Lower quality per component
- Manual test generation still needed

**Why not chosen**: Doesn't leverage specialization benefits, sacrifices quality for simplicity.

## Implementation

### DAG Orchestration

```typescript
interface SubagentConfig {
  name: string;
  description: string;
  prompt: string;
  tools: string[];
  model: "sonnet" | "opus" | "haiku";
  temperature: number; // Near-deterministic for CI
  inputs: string[]; // Required input artifacts
  outputs: string[]; // Produced artifacts
  idempotencyKey: (inputs: any) => string;
}

interface SubagentResult {
  success: boolean;
  artifacts: Record<string, any>;
  metrics: {
    latency: number;
    tokenSpend: number;
    retries: number;
  };
}
```

### Subagent Execution

```typescript
async function executeSubagent(
  config: SubagentConfig,
  inputs: any
): Promise<SubagentResult> {
  // Check idempotency
  const key = config.idempotencyKey(inputs);
  if (cache.has(key)) {
    return cache.get(key);
  }

  // Execute with SDK
  const result = await query({
    prompt: config.prompt,
    options: {
      model: config.model,
      temperature: config.temperature,
      agents: { [config.name]: config }
    }
  });

  // Track metrics
  const metrics = {
    latency: Date.now() - start,
    tokenSpend: result.tokenUsage,
    retries: retryCount
  };

  // Cache result
  cache.set(key, { success: true, artifacts: result, metrics });

  return { success: true, artifacts: result, metrics };
}
```

### Prompt Template Versioning

Store prompts with version control:

```typescript
// subagents/v1/hook-developer.ts
export const hookDeveloperPrompt = {
  version: "1.0",
  template: `You are a Claude Code hooks expert.

  Implement:
  - UserPromptSubmit hook (< 100ms)
  - Stop hook (< 100ms)
  - Event serialization
  - Error handling (never block user)

  Follow TDD: write tests first.

  Inputs: {{requirements}}

  Success criteria:
  - All tests pass
  - Hook latency < 100ms p95
  - No user blocking
  - Proper error handling`,
  temperature: 0.3 // Near-deterministic
};
```

### Cost and SLO Budgets

Per-subagent budget tracking:

```typescript
interface SubagentBudget {
  maxTokensPerRun: number;
  maxLatencyMs: number;
  maxRetries: number;
  alertThreshold: number; // % of budget
}

const budgets: Record<string, SubagentBudget> = {
  "hook-developer": {
    maxTokensPerRun: 10000,
    maxLatencyMs: 30000,
    maxRetries: 3,
    alertThreshold: 0.8
  }
};
```

### Guardrails and Acceptance Criteria

Define quality thresholds per subagent:

```typescript
interface SubagentAcceptanceCriteria {
  minTestCoverage: number;
  maxPIILeakageScore: number;
  maxSecurityIssues: number;
  requiredChecks: string[];
}

const criteria = {
  "sanitization-pipeline": {
    minTestCoverage: 0.95, // Critical component
    maxPIILeakageScore: 0.0, // Zero tolerance
    maxSecurityIssues: 0,
    requiredChecks: ["pii-detection", "redaction-audit"]
  }
};
```

### Observability Per Subagent

Track metrics for each subagent:

```typescript
interface SubagentMetrics {
  invocationCount: number;
  successRate: number;
  averageLatency: number;
  p95Latency: number;
  totalTokenSpend: number;
  errorRate: number;
  retryRate: number;
}
```

### Determinism and Reproducibility

For CI and debugging:

1. **Set low temperature** (0.1-0.3) for near-deterministic outputs
2. **Use fixed seeds** when possible
3. **Mock external dependencies** for test subagents
4. **Store prompt versions** with git hash
5. **Capture full inputs/outputs** for replay

### Traceability

Each subagent execution produces:

```typescript
interface SubagentTrace {
  subagentName: string;
  version: string;
  executionId: string;
  timestamp: string;
  inputs: any;
  outputs: any;
  promptUsed: string;
  metrics: SubagentMetrics;
  artifacts: string[]; // Paths to generated files
}
```

## Risks and Mitigations

### Risk: Cost Explosion from Agent Chatter

**Impact**: High - Could exceed budget

**Mitigation**:
- Set token budgets per subagent
- Monitor spend in real-time
- Alert at 80% of budget
- Use Haiku for simple tasks
- Cache subagent results aggressively

### Risk: Non-Determinism Breaks CI

**Impact**: Medium - Flaky builds

**Mitigation**:
- Low temperature (0.1-0.3) for production subagents
- Retry with same seed
- Quarantine flaky results
- Human review for critical paths

### Risk: Debugging Difficulty

**Impact**: Medium - Hard to trace issues

**Mitigation**:
- Full trace logging per subagent
- Artifact preservation
- Replay capability
- Clear error messages with context

### Risk: Vendor Lock-In

**Impact**: Medium - Hard to migrate

**Mitigation**:
- Abstract subagent interface
- Store prompts as configuration
- Document alternative frameworks
- Plan migration path for post-MVP

## Related Documents

### Architecture
- [Subagent System Architecture](../architecture/architecture-subagent-system-2025-01-16.md)
- [Global Context Network](../architecture/architecture-global-context-network-2025-01-16.md)

### Decisions
- [ADR-003: Claude Testing Harness](./decision-claude-testing-harness-2025-01-16.md)

### Guides
- [Using Subagents Guide](../guides/guide-using-subagents-2025-01-16.md)
- [TDD Workflow with Subagents](../guides/guide-tdd-workflow-2025-01-16.md)

### Reference
- [Subagent Types Reference](../reference/reference-subagent-types-2025-01-16.md)
- [Claude Agent SDK API](../reference/reference-claude-agent-sdk-api-2025-01-16.md)
````

## File: decisions/decision-token-rewards-sybil-resistance-2025-01-16.md
````markdown
---
title: ADR-009: Token Rewards and Sybil Resistance Strategy
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [tokenomics, sybil-resistance, incentives, validation, mvp-scope]
---

# ADR-009: Token Rewards and Sybil Resistance Strategy

## Status

Accepted

Date: 2025-01-16

## Context

The user's original vision includes a "mining through learning" model where contributors earn token rewards for providing quality learnings to the global context network:

> "By providing context to the pool you are effectively mining... validators... would decide whether or not this is a valid addition to the context pool... if it's valid then we distribute tokens out to the agent that provided it."

### The Challenge

Without proper incentive design and Sybil resistance, token rewards create significant risks:

1. **Spam/Junk Submissions**: Users can flood the network with low-quality or AI-generated junk to farm tokens
2. **Sybil Attacks**: Single actor creates multiple identities to multiply rewards
3. **Gaming the System**: Validators colluding, fake submissions validated by confederates
4. **Quality Degradation**: Quantity-over-quality incentives degrade network value
5. **Legal/Regulatory Risk**: Token issuance triggers securities regulations, KYC/AML requirements
6. **Premature Tokenomics**: Without usage data, we can't design fair reward distribution

### Why This Matters for MVP

The GPT-5 holistic review identified token rewards as a **MEDIUM priority** ADR, noting:
- Complexity of tokenomics design
- Risk of sybil attacks and spam
- Need for validator network infrastructure
- Regulatory uncertainty
- Should defer to avoid blocking MVP launch

### Constraints

**Technical**:
- MVP must ship without blockchain infrastructure
- No validator network in Phase 1-3
- Quality scoring undefined
- No proven usage patterns yet

**Legal**:
- Token issuance may trigger securities laws
- KYC/AML requirements unclear
- International compliance complexity
- Legal review required before launch

**User Experience**:
- Token rewards shouldn't be required for core functionality
- Learnings should have intrinsic value
- Community participation driven by value, not speculation

## Decision

**DEFER token issuance and distribution to Phase 2 (post-MVP).**

**For MVP (Phase 1)**:
- Track **Non-Transferable Contribution Credits (NTCCs)** off-chain in local database
- **NTCCs are reputation points with no monetary value**
- Record submission quality scores
- Log validator consensus (when implemented)
- Build foundation for future token distribution
- No actual token minting or distribution

**Legal Clarity**: NTCCs have no monetary value and are non-transferable. Any future token program, if any, would be subject to separate approval, legal review, regulatory compliance, and may or may not convert NTCCs. Accumulated NTCCs may be adjusted or revoked for abuse, plagiarism, or quality violations.

**Future Implementation (Phase 2+)**:
- Issue tokens based on accumulated contribution credits
- Implement Sybil resistance measures
- Deploy validator network
- Quality-based reward weighting
- Anti-gaming mechanisms

### Non-Transferable Contribution Credits (NTCCs) System

**What are NTCCs?**
- Reputation points tracking quality contributions
- Non-transferable and non-fungible
- No monetary value
- Subject to adjustment or revocation for abuse
- May or may not be eligible for future token conversion

```typescript
interface NTCC {
  id: string;
  userId: string;              // User identifier (pseudonymous)
  conversationId: string;       // Source conversation
  learningId: string;           // Extracted learning
  timestamp: string;
  qualityScore: number;         // 0-100 computed score
  noveltyScore: number;         // 0-100 global uniqueness
  identityAttestationHash?: string; // Hash of identity proof (not PII)
  identityWeight: number;       // 0-1 based on attestations
  validatorConsensus?: number;  // Future: validator agreement %
  creditAmount: number;         // NTCCs earned (not tokens)
  status: "pending" | "approved" | "rejected" | "quarantined" | "revoked";
  provenanceHash: string;       // CID or hash for plagiarism detection
  submittedAt: string;          // Commit timestamp
}
```

### Token Distribution (Phase 2+)

When ready to launch token rewards:

1. **Retroactive Distribution**: Convert accumulated NTCCs → tokens (with trimming/normalization)
2. **Quality Weighting**: Higher quality earns more tokens
3. **Identity-Weighted Rewards**: Attestation-based multipliers (not PII storage)
4. **Per-Identity Caps**: Hard limits per epoch to prevent sybil splitting
5. **Concave Reward Curves**: Diminishing returns per identity (sqrt or log)
6. **Novelty/Deduplication**: Global uniqueness checks across all identities
7. **Validator Staking**: Validators stake tokens, slashed for bad votes

## Consequences

### Positive

- **Faster MVP Launch**: No blockchain integration, smart contracts, or token launch delays
- **Reduced Legal Risk**: Avoid securities/KYC complexity during MVP phase
- **Learn First**: Gather usage data to inform tokenomics design
- **Quality Focus**: Build intrinsic value before adding extrinsic incentives
- **Retroactive Fairness**: Early contributors credited when tokens launch
- **Simpler Testing**: Test core functionality without financial incentives
- **Regulatory Clarity**: Time to assess legal landscape before token launch
- **Avoid Premature Optimization**: Design rewards based on actual behavior patterns

### Negative

- **Lower Initial Participation**: Without token rewards, fewer early contributors
- **Delayed Gratification**: Early adopters don't get immediate financial incentive
- **Competitive Disadvantage**: Other networks offering tokens may attract more users
- **Implementation Complexity Later**: Retroactive distribution requires careful record-keeping
- **Expectation Management**: Must clearly communicate token plans to avoid disappointment

### Neutral

- **NTCCs as Reputation**: NTCCs track contributions but have no monetary value; future conversion (if any) is contingent and not guaranteed
- **Validator Network Still Needed**: Quality control required regardless of tokens
- **Design Flexibility**: More time to design optimal tokenomics
- **Market Timing**: Can launch tokens when market/regulatory conditions favorable

## Alternatives Considered

### Alternative 1: Immediate Token Launch

**Description**: Launch ERC-20 token with MVP, distribute rewards immediately.

**Pros**:
- Strong immediate incentive for participation
- Aligns with original vision
- Early adopter rewards
- Network effects from speculation

**Cons**:
- **Securities risk**: Likely classified as security, requires legal review/compliance
- **Premature tokenomics**: No data to design fair distribution
- **Development delay**: Smart contract audit, security, blockchain integration
- **KYC/AML burden**: May require identity verification
- **Market risk**: Token price volatility distracts from utility
- **Gaming risk**: Sybil attacks, spam, quality degradation

**Why not chosen**: Legal and technical complexity blocks MVP launch. Risk of poor tokenomics design without usage data.

### Alternative 2: No Credits, No Tokens (Pure Utility)

**Description**: No token rewards ever. Network participation driven purely by utility value.

**Pros**:
- Zero legal/regulatory risk
- Simplest implementation
- Pure focus on quality and utility
- No gaming or sybil concerns

**Cons**:
- **Violates user vision**: "Mining through learning" is core concept
- **Lower participation**: Reduced incentive to contribute
- **No validator incentive**: How to compensate validators?
- **Missed opportunity**: Tokens can align incentives effectively
- **Competitive disadvantage**: Other networks offer rewards

**Why not chosen**: Contradicts user's core vision of incentivized learning network.

### Alternative 3: Flat Rewards (Equal Distribution)

**Description**: Launch tokens but distribute equally regardless of quality.

**Pros**:
- Simple implementation
- Fair baseline
- No complex quality scoring

**Cons**:
- **No quality incentive**: Encourages spam and low-effort submissions
- **Sybil multiplication**: Creating multiple identities maximizes rewards
- **Race to bottom**: Network flooded with junk
- **Unfair to quality contributors**: High-effort work gets same as spam

**Why not chosen**: Perverse incentives destroy network quality.

### Alternative 4: Staking Requirement for Submission

**Description**: Require users to stake tokens to submit learnings (slash if spam).

**Pros**:
- Strong sybil deterrent (expensive to create multiple identities)
- Spam prevention (risk of losing stake)
- Self-funding moderation

**Cons**:
- **Barrier to entry**: New users can't participate without buying tokens
- **Plutocracy risk**: Wealthy users dominate
- **Bootstrapping problem**: Where do initial stakes come from?
- **Complexity**: Requires smart contracts, staking logic, slashing rules

**Why not chosen**: Too complex for MVP. Barrier to entry contradicts accessibility goals.

### Alternative 5: Gradual Token Unlock Schedule

**Description**: Issue tokens immediately but unlock over time based on continued quality.

**Pros**:
- Immediate incentive
- Long-term alignment (vesting)
- Reduces dump risk

**Cons**:
- Still requires token launch (legal/technical complexity)
- Complex vesting logic
- Doesn't solve initial tokenomics design problem
- More smart contract complexity

**Why not chosen**: Doesn't address core issue of launching tokens prematurely.

## Implementation

### Phase 1 (MVP): Contribution Credit Tracking

**Database Schema**:

```sql
CREATE TABLE contribution_credits (
  id TEXT PRIMARY KEY,
  user_id TEXT NOT NULL,
  conversation_id TEXT NOT NULL,
  learning_id TEXT NOT NULL,
  timestamp TEXT NOT NULL,
  quality_score REAL NOT NULL,        -- 0-100
  validator_consensus REAL,           -- NULL until validators exist
  credit_amount REAL NOT NULL,        -- Computed from quality
  status TEXT CHECK(status IN ('pending', 'approved', 'rejected', 'quarantined')),
  metadata TEXT,                      -- JSON: {detailsAboutSubmission}
  created_at TEXT DEFAULT CURRENT_TIMESTAMP,
  updated_at TEXT DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_credits_user ON contribution_credits(user_id);
CREATE INDEX idx_credits_status ON contribution_credits(status);
CREATE INDEX idx_credits_quality ON contribution_credits(quality_score);
```

**Credit Calculation (Quality-Based)**:

```typescript
interface QualityMetrics {
  novelty: number;        // 0-1: Is this new information?
  accuracy: number;       // 0-1: Is this correct?
  clarity: number;        // 0-1: Is this well-explained?
  applicability: number;  // 0-1: How broadly useful?
  completeness: number;   // 0-1: Is it thorough?
}

function computeQualityScore(metrics: QualityMetrics): number {
  const weights = {
    novelty: 0.25,
    accuracy: 0.30,
    clarity: 0.15,
    applicability: 0.20,
    completeness: 0.10
  };

  return Object.entries(metrics).reduce((score, [key, value]) => {
    return score + (value * weights[key as keyof QualityMetrics]);
  }, 0) * 100; // Scale to 0-100
}

function computeCreditAmount(qualityScore: number, learningSize: number): number {
  // Base credits from quality
  const baseCredits = qualityScore / 10; // 0-10 range

  // Size multiplier (diminishing returns)
  const sizeMultiplier = Math.log10(learningSize + 1);

  // Combine
  return baseCredits * (1 + sizeMultiplier * 0.5);
}
```

**Provenance and Plagiarism Prevention (MVP)**:

```typescript
interface ProvenanceTracking {
  commitHash: string;        // Hash of learning content (commit phase)
  revealTimestamp: string;   // When content revealed
  contentHash: string;       // CID or SHA-256 of actual content
  similarityScore: number;   // 0-1 similarity to existing learnings
  priorArtMatches: string[]; // IDs of similar prior submissions
}

async function submitLearningWithProvenance(
  userId: string,
  learning: Learning
): Promise<NTCC> {
  // Step 1: Commit phase - store hash first
  const contentHash = await hashContent(learning.content);
  const commitRecord = await db.commits.insert({
    userId,
    commitHash: contentHash,
    timestamp: new Date().toISOString()
  });

  // Step 2: Reveal phase - store actual content after commit
  await delay(config.minCommitRevealDelay); // e.g., 5 minutes

  // Step 3: Check for plagiarism/prior art
  const similarity = await checkGlobalSimilarity(learning);
  if (similarity.maxScore > 0.85) {
    // Too similar to existing learning
    return rejectSubmission(userId, "PLAGIARISM_DETECTED", similarity);
  }

  // Step 4: Award NTCCs with provenance metadata
  return createNTCC({
    userId,
    learning,
    provenanceHash: contentHash,
    noveltyScore: (1 - similarity.maxScore) * 100,
    priorSubmissions: similarity.matches
  });
}
```

**Anti-Spam Measures (MVP)**:

```typescript
interface SpamDetection {
  // Rate limiting
  maxSubmissionsPerHour: number;   // e.g., 10
  maxSubmissionsPerDay: number;    // e.g., 50
  maxSubmissionsPerEpoch: number;  // e.g., 200 per week

  // Quality floor
  minQualityThreshold: number;     // e.g., 40/100
  minNoveltyThreshold: number;     // e.g., 30/100

  // Similarity detection
  maxSimilarSubmissions: number;   // e.g., 3 similar learnings
  globalDedupThreshold: number;    // e.g., 0.85 similarity = duplicate

  // Cooldown after rejection
  rejectionCooldownMinutes: number; // e.g., 60

  // Identity requirements
  minIdentityWeight: number;       // e.g., 0.3 (requires some attestation)
}

async function checkSpamRisk(userId: string, learning: Learning): Promise<SpamRisk> {
  // Check rate limits
  const recentSubmissions = await countRecentSubmissions(userId, "1 hour");
  if (recentSubmissions >= config.maxSubmissionsPerHour) {
    return { isSpam: true, reason: "RATE_LIMIT_EXCEEDED" };
  }

  // Check quality floor
  const quality = await evaluateLearningQuality(learning);
  if (quality < config.minQualityThreshold) {
    return { isSpam: true, reason: "QUALITY_TOO_LOW" };
  }

  // Check for duplicates/near-duplicates
  const similar = await findSimilarLearnings(userId, learning);
  if (similar.length >= config.maxSimilarSubmissions) {
    return { isSpam: true, reason: "TOO_MANY_SIMILAR" };
  }

  return { isSpam: false };
}
```

### Phase 2+: Token Launch & Distribution

When ready to launch tokens (post-MVP):

#### 1. Sybil Resistance Mechanisms

**Identity Verification**:
- GitHub/GitLab account linking (activity history, reputation)
- Email verification with domain reputation
- Optional: Decentralized identity (DID) integration
- Optional: BrightID or other social graph proof

**Behavioral Analysis**:
- Submission patterns (timing, frequency)
- Interaction diversity (query vs submit ratio)
- Network participation (helping others)
- Historical quality scores

**Economic Deterrents**:
- Identity attestation requirements (Gitcoin Passport, BrightID, DIDs/VCs)
- Per-identity concave rewards WITH hard caps per epoch
- Cross-identity novelty/dedup checks
- Validator staking requirements
- Reputation-weighted rewards

**IMPORTANT**: Naive quadratic rewards (sqrt per total) WITHOUT identity proof incentivize splitting into sybils. We combine multiple defenses:

**Example: Identity-Weighted Concave Rewards**:

```typescript
// WRONG (sybil-vulnerable):
// 100 sybils × 1 submission each = 100 tokens (linear per identity)
// 1 user × 100 submissions = sqrt(100) = 10 tokens
// Result: Sybils get 10× more!

// CORRECT (sybil-resistant):
interface IdentityReward {
  qualityAdjustedCredits: number;  // Sum of quality-weighted submissions
  identityWeight: number;          // 0-1 from attestations (Passport score, etc.)
  epochCap: number;                // Hard limit per identity per epoch
}

function computeIdentityReward(identity: IdentityReward): number {
  // Step 1: Apply concave curve to reduce splitting incentive
  const concaveScore = Math.sqrt(identity.qualityAdjustedCredits);

  // Step 2: Weight by identity attestations (low weight = low reward)
  const weightedScore = concaveScore * identity.identityWeight;

  // Step 3: Hard cap per epoch (prevents farming even with many identities)
  return Math.min(weightedScore, identity.epochCap);
}

// With identity attestations:
// 100 sybils × 1 submission × 0.1 weight = 10 × sqrt(1) = 10 tokens
// 1 user × 100 submissions × 1.0 weight = sqrt(100) = 10 tokens (capped)
// Sybil advantage eliminated + massive setup cost for attestations
```

**Privacy-Preserving Identity**:
- DO NOT store emails, GitHub accounts, or any PII
- Store only hashed attestation proofs or verifiable credentials
- Users hold identity attestations (Gitcoin Passport, BrightID, Sismo, DIDs)
- System verifies proofs without learning identity

#### 2. Validator Network

**Validator Selection**:
- Stake minimum tokens to become validator
- Reputation score based on past accuracy
- Random assignment to prevent collusion
- Minimum quorum (e.g., 5 validators)

**Consensus Mechanism**:
- Each validator scores submission (0-100)
- Median score used (robust to outliers)
- Validators rewarded for consensus
- Outlier validators slashed if consistently wrong

**Example**:

```typescript
interface ValidationRound {
  learningId: string;
  validators: string[];          // Selected validator IDs
  scores: Map<string, number>;   // Validator → score
  consensusScore: number;        // Median of scores
  rewardPool: number;            // Tokens for validators
  slashPool: number;             // Tokens from slashed validators
}

function computeConsensus(scores: number[]): number {
  scores.sort((a, b) => a - b);
  const mid = Math.floor(scores.length / 2);
  return scores.length % 2 === 0
    ? (scores[mid - 1] + scores[mid]) / 2
    : scores[mid];
}

function distributeValidatorRewards(round: ValidationRound) {
  const consensus = round.consensusScore;

  for (const [validatorId, score] of round.scores.entries()) {
    const deviation = Math.abs(score - consensus);

    if (deviation < 10) {
      // Close to consensus: reward
      const reward = round.rewardPool / round.validators.length;
      creditValidator(validatorId, reward);
    } else if (deviation > 30) {
      // Far from consensus: slash
      const slash = round.slashPool / round.validators.length;
      slashValidator(validatorId, slash);
    }
    // 10-30 deviation: no reward, no slash
  }
}
```

#### 3. Retroactive Token Distribution

Convert accumulated credits to tokens:

```typescript
interface RetroactiveDistribution {
  snapshotDate: string;           // When credits locked
  conversionRate: number;         // Credits → Tokens ratio
  vestingSchedule: VestingTier[]; // Time-based unlock
}

interface VestingTier {
  unlockDate: string;
  percentage: number; // % of tokens unlocked
}

// Example: 40% immediate, 60% over 12 months
const vesting: VestingTier[] = [
  { unlockDate: "2025-03-01", percentage: 40 },
  { unlockDate: "2025-06-01", percentage: 20 },
  { unlockDate: "2025-09-01", percentage: 20 },
  { unlockDate: "2025-12-01", percentage: 20 }
];

async function distributeRetroactiveTokens() {
  const snapshot = await db.contribution_credits.findAll({
    status: "approved",
    created_at: { $lte: config.snapshotDate }
  });

  for (const credit of snapshot) {
    const tokenAmount = credit.credit_amount * config.conversionRate;

    await issueVestedTokens(
      credit.user_id,
      tokenAmount,
      vesting
    );
  }
}
```

#### 4. Smart Contract Architecture (Future)

**Contracts Needed**:
- **Token Contract**: ERC-20 for rewards
- **Staking Contract**: Validator stakes
- **Distribution Contract**: Credit → token conversion
- **Governance Contract**: Parameter updates

**Security Requirements**:
- Formal verification
- Multi-sig admin controls
- Upgrade path with timelock
- Emergency pause mechanism
- External security audit

## Phase 2 Readiness Checklist

Before launching token distribution, ALL of the following gates must be satisfied:

### Legal & Compliance
- [ ] Legal opinion obtained for target jurisdictions (US/EU minimum)
- [ ] Token classification determined (utility vs security)
- [ ] Compliance strategy for MiCA (EU) and SEC/CFTC (US)
- [ ] KYC/AML requirements defined (if any)
- [ ] Terms of service and user agreements finalized
- [ ] Licensing requirements assessed (money transmitter laws)
- [ ] Tax implications documented for users

### Technical Readiness
- [ ] Quality scoring stable (false positive/negative on dedup < 3%)
- [ ] Scoring test-retest correlation > 0.85
- [ ] Anti-sybil efficacy proven (> 95% precision on abuse flags)
- [ ] Smart contracts developed and unit tested
- [ ] External security audit completed (reputable firm)
- [ ] Bug bounty program launched (3+ months before TGE)
- [ ] Testnet deployment successful (3+ months validation)
- [ ] Incident response and emergency pause tested
- [ ] Multi-sig governance setup and tested
- [ ] Snapshot and distribution tooling validated

### Data & Governance
- [ ] Minimum 6 months of MVP usage data collected
- [ ] NTCC distribution analyzed (Gini coefficient, outliers, abuse patterns)
- [ ] Token supply model finalized (emission, caps, allocations)
- [ ] Utility framework defined (staking, governance, fee discounts)
- [ ] Validator network operational (minimum 10 diverse validators)
- [ ] Governance mechanism deployed (DAO or multi-sig → DAO path)
- [ ] Parameter adjustment process defined
- [ ] Dispute resolution and appeals process documented

### User Communication
- [ ] Public token roadmap published
- [ ] NTCC → token conversion formula disclosed
- [ ] Outlier trimming policy communicated
- [ ] Vesting schedule announced
- [ ] User education materials created
- [ ] Migration guide for NTCC holders prepared
- [ ] Community feedback collected and addressed

### Metrics Thresholds (MVP Success)
- [ ] < 5% spam rate sustained for 3+ months
- [ ] Mean quality score > 70/100
- [ ] > 70% user retention month-over-month
- [ ] NTCCs distributed across > 100 unique users
- [ ] < 2% identified sybil accounts
- [ ] Validator consensus agreement > 80%
- [ ] No critical privacy violations in 6 months

**Decision Authority**: Requires unanimous approval from legal counsel, technical lead, and community governance vote (if DAO established).

## Risks and Mitigations

### Risk: Credit Inflation Before Token Launch

**Impact**: High - Accumulated credits become worthless if too many issued

**Mitigation**:
- Conservative credit issuance in MVP
- Monitor credit distribution rates
- Reserve right to adjust conversion rate
- Transparent communication about credit value
- Quality threshold prevents spam credits

### Risk: Expectation Mismatch

**Impact**: Medium - Users expect immediate tokens, feel misled

**Mitigation**:
- **Clear communication**: "Credits now, tokens later"
- Document token roadmap publicly
- Regular updates on token launch progress
- Honor all accumulated credits (build trust)
- Explain rationale (legal, quality, fairness)

### Risk: Sybil Attacks on NTCCs

**Impact**: Medium - Users farm credits with multiple identities

**Mitigation**:
- Implement anti-spam measures immediately (rate limits, quality floors)
- Identity attestation system (Gitcoin Passport, BrightID, hashed proofs only)
- Identity-weighted concave rewards with hard per-epoch caps
- Global novelty/dedup checks across all identities
- Behavioral analysis (submission patterns, timing, diversity)
- Manual review of suspicious patterns
- Retroactive NTCC adjustment or revocation if abuse detected
- Commit-reveal mechanism to prevent plagiarism

### Risk: Privacy Violation Through Identity Tracking

**Impact**: Critical - Storing PII contradicts privacy-first principle

**Mitigation**:
- **NEVER store emails, GitHub accounts, or any PII**
- Accept only hashed attestation proofs or verifiable credentials
- Users hold identity attestations (Passport, BrightID, Sismo, DIDs)
- System verifies proofs without learning identity
- Zero-knowledge proof integration for future enhancements
- Regular privacy audits of identity data handling
- Explicit documentation: "No PII storage for sybil resistance"

### Risk: Plagiarism and Front-Running

**Impact**: High - Users copy others' learnings, claim credit first

**Mitigation**:
- Commit-reveal mechanism for submissions (commit hash first, reveal later)
- Provenance hashing (CID or content hash) stored at submission time
- Timestamp-based priority for identical submissions
- Cross-submission similarity detection and dedup
- Penalty for discovered plagiarism (NTCC revocation, user ban)
- Attestation of originality required
- Community reporting mechanism

### Risk: Usage Gaming (If Usage-Based Rewards Added)

**Impact**: Medium - Users artificially inflate query metrics

**Mitigation**:
- Anti-wash-trading heuristics (self-queries, circular patterns)
- Cap impact of usage metrics on rewards (max 20% of score)
- Require diverse queriers (not just submission author)
- Temporal decay on usage stats (older learnings valued less)
- Behavioral analysis to detect coordinated gaming

### Risk: Validator Centralization

**Impact**: Medium - Few validators control consensus

**Mitigation**:
- Low staking barrier for validators
- Geographic/identity diversity requirements
- Random validator selection
- Reputation-based rotation
- Governance to adjust parameters

### Risk: Regulatory Changes

**Impact**: High - Token classification changes, requires pivot

**Mitigation**:
- Legal monitoring and compliance updates
- Flexible smart contract design (upgradeable)
- Multiple jurisdiction strategy
- Utility-focused token design (not security)
- DAO governance for decentralization

### Risk: Poor Tokenomics Design

**Impact**: High - Bad incentives destroy network value

**Mitigation**:
- Learn from MVP usage patterns first
- Tokenomics simulations and modeling
- Gradual rollout with parameter adjustments
- Governance mechanism for updates
- Expert consultation (tokenomics advisors)

### Risk: Smart Contract Bugs and Exploits

**Impact**: Critical - Loss of user funds, network compromise

**Mitigation**:
- Formal verification of core contracts
- External security audit by reputable firm (minimum 2)
- Bug bounty program (3+ months before mainnet)
- Testnet deployment and validation (3+ months)
- Multi-sig admin controls with timelock
- Emergency pause mechanism
- Incident response plan and runbook
- Insurance or reserve fund for potential exploits

### Risk: Dispute Resolution and Appeals

**Impact**: Medium - Users contest NTCC revocation or sybil flagging

**Mitigation**:
- Clear dispute resolution process documented
- Appeal window (e.g., 30 days from revocation)
- Evidence submission requirements
- Human review board for appeals
- Transparent decision criteria
- Escalation path to governance vote for edge cases
- Clawback grace period before finalization

### Risk: Tax and Legal Liability for Users

**Impact**: Medium - Users face unexpected tax obligations

**Mitigation**:
- Clear documentation: "Consult tax advisor"
- Country-specific guidance where possible
- KYC thresholds if required by jurisdiction
- Reporting tools for users (transaction history, valuations)
- Partnership with crypto tax software providers
- Terms of service disclaimer on tax responsibility

## Success Metrics

### MVP (Phase 1) Success

- **Credits Issued**: Track total credits distributed
- **Quality Distribution**: Histogram of quality scores
- **User Retention**: Are users contributing without token rewards?
- **Spam Rate**: % of submissions flagged as spam/rejected
- **Intrinsic Value**: Are learnings useful (query frequency)?

**Targets**:
- < 5% spam rate
- Mean quality score > 70/100
- > 70% user retention month-over-month
- Credits distributed across > 100 unique users

### Token Launch (Phase 2) Success

- **Fair Distribution**: Gini coefficient < 0.5 (avoid concentration)
- **Sybil Resistance**: < 2% identified sybil accounts
- **Validator Decentralization**: No single entity > 10% validation power
- **Token Utility**: Tokens used for staking/governance, not just speculation
- **Network Quality**: Quality scores maintain or improve post-token launch

**Targets**:
- > 1000 token holders in first 6 months
- Validator consensus agreement > 80%
- < 5% token price volatility (stable utility value)

## Future Enhancements

### Advanced Sybil Resistance

**Machine Learning Detection**:
- User behavior clustering
- Anomaly detection for farming patterns
- Graph analysis for collusion networks

**Zero-Knowledge Proofs**:
- Prove unique human without revealing identity
- zk-SNARK based reputation proofs

**Cross-Chain Identity**:
- Aggregate reputation from multiple networks
- Decentralized identity standards (W3C DID)

### Dynamic Reward Adjustment

**Market-Based Pricing**:
- Supply/demand for specific learning topics
- Bonus rewards for underserved categories
- Penalty for oversupplied topics

**Contextual Quality**:
- Time-sensitive rewards (emerging tech gets bonus)
- Community voting on value
- Usage-based rewards (high-query learnings worth more)

### Governance Integration

**DAO Control**:
- Token holders vote on reward parameters
- Validator selection governance
- Quality threshold adjustments
- Emergency pause/unpause

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Validator Network Design](../architecture/architecture-validator-network-2025-01-16.md) (Future)

### Decisions
- [ADR-006: Async Processing Model](./decision-async-processing-model-2025-01-16.md) - Job queue for credit processing
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md) - Privacy in public submissions

### Plans
- [Original User Vision](../plans/plan-original-user-vision-2025-01-16.md) - Mining through learning concept
- [Implementation Roadmap](../plans/plan-implementation-roadmap-2025-01-16.md) - Phase 2+ token launch

### Reference
- [Database Schema](../reference/reference-database-schema-2025-01-16.md) - Contribution credits table
- [Quality Scoring Algorithm](../reference/reference-quality-scoring-2025-01-16.md) (Future)

---

## Appendix: Token Economics Research

### Comparables Analysis

**Existing Learning/Knowledge Networks**:
- **Ocean Protocol**: Data tokenization, staking for curation
- **The Graph**: Indexer rewards, curator bonding curves
- **Gitcoin**: Quadratic funding, anti-sybil via passport
- **Numerai**: Staking on model accuracy, tournament rewards

**Key Lessons**:
1. **Staking works**: Skin in the game reduces spam
2. **Quadratic funding reduces sybil**: Math proven effective
3. **Reputation matters**: Aggregate signals across platforms
4. **Utility > Speculation**: Networks succeed when tokens have utility beyond price

### Regulatory Landscape (2025)

**United States**:
- SEC: Utility tokens may avoid security classification if sufficiently decentralized
- CFTC: May claim jurisdiction over governance tokens
- State laws: Money transmitter licenses may apply

**European Union**:
- MiCA regulation: Crypto-asset service providers need licensing
- GDPR: Right to deletion conflicts with immutable ledgers

**Recommendations**:
- Launch in favorable jurisdiction first
- Decentralize governance early
- Emphasize utility over investment returns
- Legal review before token generation event (TGE)

---

*This ADR establishes the strategic decision to defer token rewards to post-MVP while building the foundation for fair, sybil-resistant distribution when the time is right.*
````

## File: decisions/decision-use-claude-hooks-2025-01-16.md
````markdown
---
title: ADR-001: Use Claude Code Hooks for Event Capture
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [hooks, event-capture, claude-code, privacy]
---

# ADR-001: Use Claude Code Hooks for Event Capture

## Status

Accepted

Date: 2025-01-16

## Context

The Global Context Network needs to capture user prompts and agent responses to extract learnings. This capture must:

1. **Never block user interaction** - UX constraint of p95 < 100ms
2. **Minimize data surface area pre-sanitization** - Privacy-first architecture
3. **Capture complete conversations** - User prompts, agent responses, tool calls
4. **Preserve context** - Conversation flow, timestamps, metadata
5. **Be reliable** - No missed events, persist across restarts

The capture point must happen as close to the source as possible to ensure sanitization occurs before data spreads through the system.

## Decision

Use Claude Code's native hook system (UserPromptSubmit and Stop hooks) for event capture.

**Implementation**:
- Configure hooks via `.claude/hooks.json`
- UserPromptSubmit hook captures user input
- Stop hook captures agent responses and tool calls
- Events queued to persistent storage < 50ms
- Fire-and-forget with bounded ring buffer
- Explicit user opt-in per project

**Performance SLOs**:
- Hook execution p95 < 100ms
- Event queueing < 50ms
- Timeout after 100ms to prevent blocking
- Bounded ring buffer (max 1000 events) for backpressure

## Consequences

### Positive

- **Non-blocking capture** - Hooks execute asynchronously, never block user
- **Source-level access** - Captures data before it spreads through system
- **Complete conversation context** - Access to full prompt/response pairs
- **Native integration** - Uses Claude Code's supported APIs
- **Minimal data surface area** - Sanitization can happen immediately after capture
- **Tool call visibility** - Captures all tool invocations and results
- **Privacy-preserving** - Nothing persisted until sanitization passes

### Negative

- **Vendor lock-in** - Tightly coupled to Claude Code
- **API stability risk** - Hook contract may change between versions
- **Configuration overhead** - Requires hooks.json setup
- **Limited portability** - Can't easily switch to other AI coding tools
- **Versioning complexity** - Need to handle hook schema changes

### Neutral

- **Per-project opt-in** - Users must explicitly enable per project
- **Requires configuration** - Not zero-config out of the box
- **Event schema versioning** - Must track hook versions for compatibility

## Alternatives Considered

### Alternative 1: IDE/LSP Plugin Instrumentation

**Description**: Build VS Code extension that captures prompts/responses via Language Server Protocol.

**Pros**:
- Not tied to Claude Code specifically
- Could work with multiple AI coding tools
- More portable across editors

**Cons**:
- Higher latency (>100ms) due to additional layers
- Can't capture internal state or tool calls
- More complex setup and maintenance
- Delayed capture point (data already spread)

**Why not chosen**: Higher latency violates UX constraints, and delayed capture point increases PII surface area before sanitization.

### Alternative 2: Export APIs (Periodic Batch Export)

**Description**: Export conversations periodically via Claude Code's export functionality.

**Pros**:
- Simpler integration
- No runtime performance impact
- Decoupled from real-time execution

**Cons**:
- No real-time capture
- Missing intermediate states
- Can't capture ongoing conversations
- User must remember to export
- Higher PII risk (longer time before sanitization)

**Why not chosen**: Fails to meet "never lose data" requirement and delays sanitization dangerously.

### Alternative 3: HTTP/SOCKS Proxy with MITM

**Description**: Intercept HTTP traffic between Claude Code and Anthropic API.

**Pros**:
- Tool-agnostic
- Complete capture of API traffic

**Cons**:
- Not viable with certificate pinning
- Breaks E2E encryption
- Complex setup
- Security risk
- Can't capture local tool calls

**Why not chosen**: Security concerns and incompatible with Claude Code's implementation.

### Alternative 4: Manual Logging

**Description**: Users manually trigger capture via commands.

**Pros**:
- Simple implementation
- Full user control

**Cons**:
- Error-prone (users forget)
- Incomplete coverage
- Poor UX
- Not suitable for automated learning extraction

**Why not chosen**: Unreliable, defeats purpose of automated learning extraction.

### Alternative 5: File System Watchers

**Description**: Watch Claude Code's conversation storage files.

**Pros**:
- No Claude Code integration needed
- Simple implementation

**Cons**:
- Too late (data already persisted)
- Can't capture thinking/tool calls
- Depends on undocumented file formats
- Brittle to Claude Code updates
- Misses pre-sanitization window

**Why not chosen**: Violates privacy-first principle (capture happens after persistence).

## Implementation

### Hook Configuration

```json
{
  "hooks": {
    "UserPromptSubmit": {
      "command": "node",
      "args": [".claude/hooks/capture-prompt.js"],
      "timeout": 100
    },
    "Stop": {
      "command": "node",
      "args": [".claude/hooks/capture-response.js"],
      "timeout": 100
    }
  }
}
```

### Event Schema

Define canonical event model versioned for compatibility:

```typescript
interface CapturedEvent {
  version: "1.0";
  timestamp: string; // ISO8601
  type: "UserPromptSubmit" | "Stop";
  conversationId: string;
  clientVersion: string;

  // UserPromptSubmit fields
  prompt?: string;

  // Stop fields
  response?: string;
  toolCalls?: ToolCall[];

  // Metadata
  redactionStatus: "pending" | "sanitized" | "failed";
  hookVersion: string;
}
```

### Consent and Scope

- **Explicit opt-in** - Per-project .claude/hooks.json must be created
- **Guard rails** - Nothing persisted until sanitization passes
- **User control** - Users can disable hooks at any time
- **Transparency** - Clear documentation of what's captured

### Performance Safeguards

- **Timeout enforcement** - 100ms hard limit
- **Ring buffer** - Bounded queue (1000 events max)
- **Backpressure handling** - Drop oldest events if overwhelmed
- **Offline mode** - Queue persists, flushes when online
- **Sampling** - Can reduce capture rate if system overloaded

### Observability

Privacy-safe counters (no PII logged):

```typescript
{
  eventsCaptures: number;
  eventsDropped: number;
  eventsSanitized: number;
  eventsFailed: number;
  averageHookLatency: number;
  p95HookLatency: number;
}
```

### Compatibility and Versioning

- **Hook contract documentation** - Events, fields, versions tracked
- **Version detection** - Check Claude Code version on startup
- **Fallback strategy** - Graceful degradation if hooks unavailable
- **Migration plan** - Handle schema changes across versions

### What We Do NOT Capture

**CRITICAL: Chain-of-thought prohibition**

- **Do NOT capture hidden chain-of-thought** - Violates provider policies
- **Only capture observable outputs** - User-visible responses only
- **Only capture tool calls** - Structured, user-approved actions
- **Only capture explicit rationales** - If model emits them publicly

This aligns with Anthropic's usage policies and prevents compliance risks.

## Risks and Mitigations

### Risk: Hook API Changes

**Impact**: High - Could break capture entirely

**Mitigation**:
- Version detection on startup
- Graceful degradation if unsupported
- Test against multiple Claude Code versions
- Monitor Claude Code release notes

### Risk: Performance Degradation

**Impact**: High - Could block users

**Mitigation**:
- Hard 100ms timeout
- Ring buffer prevents unbounded growth
- Monitoring of p95 latency
- Circuit breaker if latency spikes

### Risk: Incomplete Capture

**Impact**: Medium - Missing events reduces learning quality

**Mitigation**:
- Persistent queue survives restarts
- Retry failed events
- Alert on high drop rate
- Audit trail of dropped events

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Event Capture System](../architecture/architecture-hooks-event-capture-2025-01-16.md)

### Decisions
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md)
- [ADR-006: Async Processing Model](./decision-async-processing-model-2025-01-16.md)

### Plans
- [Phase 1: Event Capture](../plans/plan-phase-1-event-capture-2025-01-16.md)

### Reference
- [Event Schema Reference](../reference/reference-event-schema-2025-01-16.md)
````

## File: decisions/decision-use-sqlite-2025-01-16.md
````markdown
---
title: ADR-005: Use SQLite for MVP Storage
category: decision
date: 2025-01-16
status: accepted
deciders: Claude + Dennison
tags: [database, sqlite, storage, local-first]
---

# ADR-005: Use SQLite for MVP Storage

## Status

Accepted

Date: 2025-01-16

## Context

The Global Context Network MVP needs persistent storage for:

1. **Sanitized conversations** - User prompts and agent responses
2. **Extracted learnings** - Insights and patterns
3. **Job queue** - Async processing tasks
4. **Upload tracking** - Network synchronization state
5. **Sanitization audit log** - PII redaction history

**Requirements**:
- ACID compliance (data integrity critical)
- Fast queries (< 100ms p95 for MCP server)
- Persistent across restarts
- Local-first (no network dependency)
- Simple setup (zero-config ideal)
- Migration support (schema evolution)
- Indexing for performance
- Transaction support for atomicity

**Constraints**:
- MVP is single-user, local-only
- Development machine environment
- No distributed access needed (yet)
- Budget-conscious (avoid operational overhead)

## Decision

Use SQLite with migrations for MVP storage.

**Configuration**:
- Write-Ahead Logging (WAL) mode for concurrency
- `synchronous=NORMAL` for performance/safety balance
- `busy_timeout=5000` for write contention
- Versioned migrations (forward-only)
- Indexes on all query paths

**Migration path to PostgreSQL** defined for post-MVP scaling.

## Consequences

### Positive

- **Zero-configuration** - Embedded database, no setup
- **ACID compliance** - Full transactional guarantees
- **Fast queries** - Indexed queries < 100ms easily achieved
- **Easy backup** - Single file copy
- **No operational overhead** - No server to maintain
- **Proven technology** - Battle-tested, stable
- **Great tooling** - SQLite CLI, many GUIs
- **Small footprint** - Minimal resource usage
- **Clear migration path** - Can upgrade to PostgreSQL later

### Negative

- **Single writer** - Only one write at a time (fine for MVP)
- **Not distributed** - Can't share across machines
- **File-based limits** - Performance degrades at very large sizes
- **No built-in replication** - Must implement separately
- **Limited concurrency** - Not ideal for high-concurrency workloads

### Neutral

- **File permissions** - Must manage OS-level access
- **Backup strategy** - Need to implement file copying
- **Corruption recovery** - Rare but need plan
- **Size monitoring** - Watch for growth

## Alternatives Considered

### Alternative 1: PostgreSQL

**Description**: Use PostgreSQL from the start.

**Pros**:
- Multi-writer concurrency
- Excellent performance at scale
- Robust replication
- Advanced features (JSONB, full-text search)
- Industry standard

**Cons**:
- **Overkill for MVP** - Single user doesn't need multi-writer
- **Setup complexity** - Install, configure, manage server
- **Operational overhead** - Monitor, backup, upgrade server
- **Resource usage** - Heavier than SQLite
- **Network dependency** - Even for local use

**Why not chosen**: Too much overhead for single-user MVP. Can migrate later when multi-user is needed.

### Alternative 2: MongoDB

**Description**: Use document database.

**Pros**:
- Flexible schema
- Good for unstructured data
- Horizontal scaling
- (Note: MongoDB does have ACID transactions since v4.x in replica sets)

**Cons**:
- **Operational overhead** - Server management
- **Document model not needed** - Our data is structured
- **Replica set needed for ACID** - Adds complexity
- **Resource usage** - Heavier than SQLite
- **Migration complexity** - Harder to move to SQL later

**Why not chosen**: Document model and schema flexibility not needed. Operational overhead too high for MVP.

### Alternative 3: JSON Files

**Description**: Store data as JSON files on disk.

**Pros**:
- Very simple
- Human-readable
- Easy to inspect/debug
- No dependencies

**Cons**:
- **No query performance** - Must load entire files
- **No transactions** - Race conditions possible
- **No ACID guarantees** - Data corruption risk
- **Indexing impossible** - Linear scans only
- **Concurrency issues** - File locking problems

**Why not chosen**: Unacceptable performance and reliability for production system.

### Alternative 4: In-Memory Database (Redis, Memcached)

**Description**: Keep all data in memory.

**Pros**:
- Extremely fast
- Simple data structures
- Good for caching

**Cons**:
- **Data loss on restart** - Not persistent (or requires snapshots)
- **Memory constraints** - Limited by RAM
- **No complex queries** - Limited query capabilities
- **Operational overhead** - Server management

**Why not chosen**: "Never lose data" requirement demands persistence. Not suitable for primary storage.

### Alternative 5: DuckDB

**Description**: Use DuckDB (analytical database) for local storage.

**Pros**:
- Embedded like SQLite
- Excellent analytical queries
- Columnar storage
- Very fast aggregations

**Cons**:
- **OLAP not OLTP** - Optimized for analytics, not transactions
- **Overkill for MVP** - We don't have complex analytical needs yet
- **Less mature** - Newer than SQLite
- **Write performance** - Optimized for reads, not writes

**Why not chosen**: Wrong optimization target. We need OLTP (transactional), not OLAP (analytical). Could add later for analytics.

### Alternative 6: SQLCipher (SQLite with Encryption)

**Description**: Use encrypted SQLite variant.

**Pros**:
- All SQLite benefits
- Encryption at rest
- Good for sensitive data
- Drop-in SQLite replacement

**Cons**:
- Performance overhead (encryption/decryption)
- Key management complexity
- Not needed for MVP (data is sanitized)

**Why not chosen**: Since we sanitize before storage, encryption at rest is lower priority. Can add post-MVP if needed.

### Alternative 7: LiteFS / Litestream

**Description**: SQLite with replication/streaming backup.

**Pros**:
- SQLite compatibility
- Built-in replication
- Continuous backup
- Multi-region support

**Cons**:
- Additional complexity
- Not needed for single-user MVP
- Operational overhead

**Why not chosen**: Replication not needed for MVP. Can add when multi-user support required.

## Implementation

### SQLite Configuration

```typescript
import Database from "better-sqlite3";

const db = new Database("global-context.db", {
  // WAL mode for better concurrency
  // Allows readers while writer is active
  wal: true,

  // Read-only mode (false for read-write)
  readonly: false,

  // Create if doesn't exist
  fileMustExist: false,

  // Timeout for busy database
  timeout: 5000 // 5 seconds
});

// Performance and safety tuning
db.pragma("synchronous = NORMAL"); // Balance safety and performance
db.pragma("cache_size = 10000"); // ~40MB cache
db.pragma("temp_store = memory"); // Temporary tables in memory
db.pragma("mmap_size = 30000000000"); // Memory-mapped I/O
db.pragma("journal_mode = WAL"); // Write-Ahead Logging
db.pragma("busy_timeout = 5000"); // 5 second timeout on locks
```

### Schema Versioning and Migrations

```typescript
interface Migration {
  version: number;
  name: string;
  up: string; // SQL to apply migration
  down: string; // SQL to rollback (optional for MVP)
}

const migrations: Migration[] = [
  {
    version: 1,
    name: "initial-schema",
    up: `
      CREATE TABLE conversations (
        id TEXT PRIMARY KEY,
        created_at INTEGER NOT NULL,
        updated_at INTEGER NOT NULL,
        status TEXT NOT NULL,
        message_count INTEGER DEFAULT 0
      );

      CREATE INDEX idx_conversations_created ON conversations(created_at);
      CREATE INDEX idx_conversations_status ON conversations(status);

      CREATE TABLE migrations (
        version INTEGER PRIMARY KEY,
        name TEXT NOT NULL,
        applied_at INTEGER NOT NULL
      );
    `,
    down: `
      DROP TABLE conversations;
      DROP TABLE migrations;
    `
  },
  {
    version: 2,
    name: "add-learnings-table",
    up: `
      CREATE TABLE learnings (
        id TEXT PRIMARY KEY,
        conversation_id TEXT NOT NULL,
        category TEXT NOT NULL,
        content TEXT NOT NULL,
        confidence REAL NOT NULL,
        created_at INTEGER NOT NULL,
        FOREIGN KEY (conversation_id) REFERENCES conversations(id)
      );

      CREATE INDEX idx_learnings_conversation ON learnings(conversation_id);
      CREATE INDEX idx_learnings_category ON learnings(category);
      CREATE INDEX idx_learnings_confidence ON learnings(confidence);
    `,
    down: "DROP TABLE learnings;"
  }
  // ... more migrations
];

async function runMigrations() {
  // Ensure migrations table exists
  db.exec(`
    CREATE TABLE IF NOT EXISTS migrations (
      version INTEGER PRIMARY KEY,
      name TEXT NOT NULL,
      applied_at INTEGER NOT NULL
    );
  `);

  // Get current version
  const current = db.prepare("SELECT MAX(version) as version FROM migrations").get();
  const currentVersion = current?.version || 0;

  // Apply pending migrations
  for (const migration of migrations) {
    if (migration.version > currentVersion) {
      console.log(`Applying migration ${migration.version}: ${migration.name}`);

      db.transaction(() => {
        db.exec(migration.up);
        db.prepare(`
          INSERT INTO migrations (version, name, applied_at)
          VALUES (?, ?, ?)
        `).run(migration.version, migration.name, Date.now());
      })();

      console.log(`✓ Migration ${migration.version} applied`);
    }
  }
}
```

### Index Strategy

All query paths must be indexed:

```sql
-- Conversations
CREATE INDEX idx_conversations_created ON conversations(created_at);
CREATE INDEX idx_conversations_status ON conversations(status);

-- Messages
CREATE INDEX idx_messages_conversation ON messages(conversation_id);
CREATE INDEX idx_messages_created ON messages(created_at);

-- Learnings
CREATE INDEX idx_learnings_conversation ON learnings(conversation_id);
CREATE INDEX idx_learnings_category ON learnings(category);
CREATE INDEX idx_learnings_confidence ON learnings(confidence);
CREATE INDEX idx_learnings_created ON learnings(created_at);

-- Composite index for common query
CREATE INDEX idx_learnings_category_confidence
  ON learnings(category, confidence);

-- Job queue
CREATE INDEX idx_jobs_status ON job_queue(status);
CREATE INDEX idx_jobs_priority_created ON job_queue(priority, created_at);
```

### Query Budget Enforcement

```typescript
interface QueryBudget {
  maxDuration: number; // ms
  slowQueryThreshold: number; // ms
  logSlowQueries: boolean;
}

const budget: QueryBudget = {
  maxDuration: 100, // p95 target
  slowQueryThreshold: 50, // Log if > 50ms
  logSlowQueries: true
};

function query<T>(sql: string, params: any[]): T {
  const start = Date.now();

  try {
    const result = db.prepare(sql).all(...params);
    const duration = Date.now() - start;

    if (duration > budget.slowQueryThreshold) {
      console.warn(`Slow query (${duration}ms): ${sql}`);
    }

    return result as T;
  } catch (error) {
    console.error(`Query failed: ${sql}`, error);
    throw error;
  }
}
```

### Backup Strategy

```typescript
async function backup() {
  const timestamp = new Date().toISOString().replace(/:/g, "-");
  const backupPath = `./backups/global-context-${timestamp}.db`;

  // Use SQLite backup API
  const backup = db.backup(backupPath);

  return new Promise((resolve, reject) => {
    backup.step(-1); // Copy entire database
    backup.finish();

    // Verify backup
    const backupDb = new Database(backupPath, { readonly: true });
    const integrity = backupDb.pragma("integrity_check");
    backupDb.close();

    if (integrity[0].integrity_check === "ok") {
      console.log(`✓ Backup created: ${backupPath}`);
      resolve(backupPath);
    } else {
      reject(new Error("Backup integrity check failed"));
    }
  });
}

// Schedule daily backups
schedule("0 3 * * *", backup);
```

### File Permissions

```typescript
import { chmod } from "fs/promises";

// Restrict database file to owner only
async function secureDatabaseFile() {
  await chmod("global-context.db", 0o600); // rw-------
  await chmod("global-context.db-shm", 0o600); // Shared memory file
  await chmod("global-context.db-wal", 0o600); // WAL file
}
```

### Corruption Recovery

```typescript
async function checkDatabaseIntegrity(): Promise<boolean> {
  try {
    const result = db.pragma("integrity_check");
    return result[0].integrity_check === "ok";
  } catch (error) {
    console.error("Integrity check failed:", error);
    return false;
  }
}

async function recoverFromCorruption() {
  console.error("Database corruption detected");

  // 1. Close database
  db.close();

  // 2. Try to dump to SQL
  const dumpPath = `./recovery/dump-${Date.now()}.sql`;
  exec(`sqlite3 global-context.db .dump > ${dumpPath}`);

  // 3. Create new database from dump
  const recoveredPath = `./recovery/recovered-${Date.now()}.db`;
  exec(`sqlite3 ${recoveredPath} < ${dumpPath}`);

  // 4. Verify recovered database
  const recoveredDb = new Database(recoveredPath);
  const integrity = recoveredDb.pragma("integrity_check");

  if (integrity[0].integrity_check === "ok") {
    console.log("✓ Database recovered successfully");
    // Replace original with recovered
    fs.renameSync(recoveredPath, "global-context.db");
  } else {
    console.error("Recovery failed - restore from backup");
    // Restore latest backup
    await restoreFromBackup();
  }
}
```

### Migration Threshold to PostgreSQL

Define when to migrate:

```typescript
interface MigrationTriggers {
  maxDatabaseSize: number; // bytes
  maxConcurrentWriters: number;
  requiresCrossMAchineAccess: boolean;
  requiresReplication: boolean;
}

const postgresThresholds: MigrationTriggers = {
  maxDatabaseSize: 5 * 1024 * 1024 * 1024, // 5GB
  maxConcurrentWriters: 1, // > 1 needs PostgreSQL
  requiresCrossMAchineAccess: false, // true needs PostgreSQL
  requiresReplication: false // true needs PostgreSQL
};

function shouldMigrateToPostgres(): boolean {
  const dbSize = fs.statSync("global-context.db").size;

  return (
    dbSize > postgresThresholds.maxDatabaseSize ||
    // Future checks:
    // concurrentWriters > postgresThresholds.maxConcurrentWriters ||
    // requiresCrossMAchineAccess ||
    // requiresReplication
  );
}
```

## Risks and Mitigations

### Risk: Database Corruption

**Impact**: High - Data loss

**Mitigation**:
- Daily backups with integrity checks
- WAL mode reduces corruption risk
- Regular integrity checks
- Recovery procedure documented
- Keep multiple backup generations

### Risk: Performance Degradation at Scale

**Impact**: Medium - Slow queries

**Mitigation**:
- Monitor database size
- Index all query paths
- Prune old data
- Migrate to PostgreSQL if needed
- Set query budgets and alerts

### Risk: Concurrent Write Contention

**Impact**: Low - MVP is single-threaded

**Mitigation**:
- WAL mode allows concurrent reads
- Busy timeout prevents immediate failures
- Transaction batching reduces locks
- Monitor lock wait times

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Database Schema](../architecture/architecture-database-schema-2025-01-16.md)

### Decisions
- [ADR-004: Sanitize Before Storage](./decision-sanitize-before-storage-2025-01-16.md)
- [ADR-006: Async Processing Model](./decision-async-processing-model-2025-01-16.md)

### Plans
- [Phase 3: Database and Storage](../plans/plan-phase-3-database-storage-2025-01-16.md)

### Reference
- [Database Schema Reference](../reference/reference-database-schema-2025-01-16.md)
````

## File: decisions/INDEX.md
````markdown
# Architecture Decision Records (ADRs)

> Last updated: 2025-01-16

## Overview

This directory contains Architecture Decision Records for the Global Context Network MVP. Each ADR documents a significant architectural decision, the context that led to it, alternatives considered, and consequences.

## Purpose

ADRs capture the "why" behind major architectural decisions:
- Context that motivated the decision
- The decision itself
- Alternatives considered with pros/cons
- Consequences (positive, negative, neutral)
- Implementation notes
- Risks and mitigations

## Documents

### Active ADRs

| ADR | Date | Document | Decision | Status |
|-----|------|----------|----------|--------|
| 001 | 2025-01-16 | [decision-use-claude-hooks-2025-01-16.md](./decision-use-claude-hooks-2025-01-16.md) | Use Claude Code hooks for event capture | Accepted |
| 002 | 2025-01-16 | [decision-subagent-driven-development-2025-01-16.md](./decision-subagent-driven-development-2025-01-16.md) | All implementation via specialized subagents | Accepted |
| 003 | 2025-01-16 | [decision-claude-testing-harness-2025-01-16.md](./decision-claude-testing-harness-2025-01-16.md) | Claude Agent SDK for test generation and validation | Accepted |
| 004 | 2025-01-16 | [decision-sanitize-before-storage-2025-01-16.md](./decision-sanitize-before-storage-2025-01-16.md) | Sanitize ALL data BEFORE database insertion | Accepted |
| 005 | 2025-01-16 | [decision-use-sqlite-2025-01-16.md](./decision-use-sqlite-2025-01-16.md) | SQLite with migrations for MVP storage | Accepted |
| 006 | 2025-01-16 | [decision-async-processing-model-2025-01-16.md](./decision-async-processing-model-2025-01-16.md) | SQLite-based job queue with async workers | Accepted |
| **007** | **2025-01-16** | **[decision-data-licensing-consent-2025-01-16.md](./decision-data-licensing-consent-2025-01-16.md)** | **Local-first, opt-in consent and CC BY 4.0 licensing (CRITICAL)** | **Accepted** |
| 008 | 2025-01-16 | [decision-global-sharing-architecture-2025-01-16.md](./decision-global-sharing-architecture-2025-01-16.md) | IPFS + Base L2 for global sharing, Celestia deferred | Accepted |
| 009 | 2025-01-16 | [decision-token-rewards-sybil-resistance-2025-01-16.md](./decision-token-rewards-sybil-resistance-2025-01-16.md) | Defer token rewards to Phase 2; NTCCs in MVP | Accepted |
| 010 | 2025-01-16 | [decision-pii-detection-strategy-2025-01-16.md](./decision-pii-detection-strategy-2025-01-16.md) | 2-stage PII detection: fast rules + AI validation with 72+ categories | Accepted |
| 011 | 2025-01-16 | [decision-security-provenance-2025-01-16.md](./decision-security-provenance-2025-01-16.md) | Signing, attestations, and provenance for published learnings | Accepted |
| 012 | 2025-01-16 | [decision-data-model-schema-versioning-2025-01-16.md](./decision-data-model-schema-versioning-2025-01-16.md) | Semantic versioning with JSON Schema validation and Atlas migrations | Accepted |
| 013 | 2025-01-16 | [decision-observability-cost-slo-governance-2025-01-16.md](./decision-observability-cost-slo-governance-2025-01-16.md) | Observability, cost management, and SLO governance for Claude API usage | Accepted |

## Decision Summary

### Core Principles

The ADRs embody these key principles:

1. **Privacy First** (ADR-004) - Sanitize before storage, zero-trust PII handling
2. **Never Block User** (ADR-001, ADR-006) - Async everything, hooks < 100ms
3. **Quality Gates** (ADR-002, ADR-003) - Testing harness, subagent validation
4. **Local-First MVP** (ADR-005, ADR-006) - SQLite, no external dependencies
5. **Migration Path** - All decisions support future scaling

### Technology Stack Decisions

| Aspect | Decision | ADR | Rationale |
|--------|----------|-----|-----------|
| Event Capture | Claude Code Hooks | 001 | Source-level capture, < 100ms, complete context |
| Development Model | Subagent-Driven | 002 | Parallel execution, specialized expertise, quality gates |
| Testing Strategy | Claude Testing Harness | 003 | Comprehensive coverage, automated quality scoring |
| Privacy Architecture | Sanitize Before Storage | 004 | Zero-trust PII, database inherently safe |
| Database | SQLite with WAL | 005 | Zero-config, ACID, fast queries, easy backup |
| Async Processing | SQLite Job Queue | 006 | Persistent, no external deps, offline tolerant |

### Key Trade-offs

**Chosen**:
- Simple setup (SQLite, embedded queue)
- Privacy first (sanitize before storage)
- Quality over speed (comprehensive testing)
- Local-first (no cloud dependencies for MVP)

**Deferred**:
- Distributed processing (Temporal, PostgreSQL, Redis)
- Advanced analytics (DuckDB)
- Blockchain/token rewards (simplified in MVP)
- Multi-user support

## ADR Writing Guidelines

### ADR Template Structure

```markdown
---
title: ADR-NNN: [Decision Title]
category: decision
date: YYYY-MM-DD
status: [proposed|accepted|deprecated|superseded]
deciders: [Names]
---

# ADR-NNN: [Decision Title]

## Status
[Status and date]

## Context
[What problem are we solving? What constraints?]

## Decision
[What did we decide to do?]

## Consequences
### Positive
### Negative
### Neutral

## Alternatives Considered
### Alternative 1: [Name]
**Description**: [What]
**Pros**: [List]
**Cons**: [List]
**Why not chosen**: [Reason]

## Implementation
[How will this be implemented?]

## Risks and Mitigations
### Risk: [Name]
**Impact**: [High/Medium/Low]
**Mitigation**: [How we address it]

## Related Documents
[Links to architecture, plans, guides]
```

### When to Create an ADR

Create an ADR when:
- Making a significant architectural decision
- Choosing between multiple viable approaches
- Decision will be hard to reverse
- Decision affects multiple components
- Team needs alignment on approach
- Future developers will ask "why did we do this?"

Don't create an ADR for:
- Trivial implementation details
- Temporary/experimental approaches
- Decisions easily reversed
- Pure coding style preferences

### ADR Status Lifecycle

- **Proposed** - Under discussion, not yet decided
- **Accepted** - Decision made and being implemented
- **Deprecated** - No longer recommended but may exist in code
- **Superseded** - Replaced by another ADR (link to it)

## Future ADRs (Identified by GPT-5 Review)

### Critical for Post-MVP

| Priority | Topic | Rationale | Status |
|----------|-------|-----------|--------|
| ~~HIGH~~ | ~~Global Sharing Architecture~~ | ~~IPFS vs blockchain, what to share, pinning strategy~~ | **COMPLETED (ADR-008)** |
| ~~HIGH~~ | ~~Consent and Licensing~~ | ~~Opt-in model, data licensing, right-to-delete, age gating~~ | **COMPLETED (ADR-007) - Addresses GPT-5 & Gemini critical blocker** |
| ~~HIGH~~ | ~~PII Detection Strategy~~ | ~~Layered detection, taxonomy, confidence thresholds~~ | **COMPLETED (ADR-010)** |
| ~~HIGH~~ | ~~Security and Provenance~~ | ~~Signing artifacts, key management, attestations~~ | **COMPLETED (ADR-011)** |
| ~~MEDIUM~~ | ~~Token Rewards Model~~ | ~~Defer vs implement, sybil resistance~~ | **COMPLETED (ADR-009)** |
| ~~MEDIUM~~ | ~~Observability and Cost SLOs~~ | ~~Telemetry, budgets, alerting~~ | **COMPLETED (ADR-013)** |
| ~~MEDIUM~~ | ~~Data Model Versioning~~ | ~~Schema evolution, cross-client compatibility~~ | **COMPLETED (ADR-012)** |
| LOW | Chain-of-Thought Capture Policy | What to capture vs avoid (Addressed in ADR-001 & STANDARDS.md) |
| LOW | Right-to-Delete vs Immutability | GDPR/CCPA alignment strategy (Addressed in ADR-007) |

## GPT-5 Review Feedback

### What We Did Well
- Concise, principle-driven
- Honest about trade-offs
- Oriented to MVP constraints
- Clear migration paths

### What We Improved
- Corrected MongoDB ACID claim (ADR-005)
- Added more alternatives per ADR
- Expanded implementation notes
- Added risk/mitigation sections
- Removed "capture thinking" language (provider policy)
- Added consent and scope considerations (ADR-001)
- Improved "why" explanations

### Areas for Future Work
- Add missing ADRs for global sharing layer
- Document consent and licensing model
- Detail PII detection taxonomy
- Define security and provenance strategy

## Related Categories

- [Architecture](../architecture/INDEX.md) - System design documents
- [Plans](../plans/INDEX.md) - Implementation plans
- [Reference](../reference/INDEX.md) - Technical specifications

## Cross-References

### By Theme

**Privacy & Security**:
- ADR-004: Sanitize Before Storage
- ADR-007: Data Licensing and Consent Model (CRITICAL - addresses GPT-5 & Gemini blocker)
- ADR-010: PII Detection Strategy (72+ categories, 2-stage detection)
- ADR-011: Security and Provenance (signing, attestations, key management)
- ADR-013: Observability and Cost SLO Governance (privacy-safe telemetry)
- ADR-001: Claude Hooks (includes consent)

**Development Workflow**:
- ADR-002: Subagent-Driven Development
- ADR-003: Claude Testing Harness

**Technical Infrastructure**:
- ADR-005: SQLite Database
- ADR-006: Async Processing Model
- ADR-012: Data Model Schema Versioning
- ADR-008: Global Sharing Architecture

**User Experience**:
- ADR-001: Claude Hooks (< 100ms requirement)
- ADR-006: Async Processing (never block user)

## Quick Tips

- All ADRs follow the standard template
- Read ADR-001, ADR-004, ADR-002 for core principles
- Check "Alternatives Considered" to understand trade-offs
- Review "Implementation" sections for technical details
- "Related Documents" link to architecture and plans
````

## File: guides/guide-claude-agent-sdk-integration-2025-01-16.md
````markdown
# Claude Agent SDK Integration Guide

> Complete guide to integrating Claude Agent SDK for subagent orchestration

---
title: Claude Agent SDK Integration Guide
category: guide
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [guide, claude-agent-sdk, subagents, integration, orchestration]
---

## Overview

This guide shows you how to integrate the Claude Agent SDK into your TypeScript project to orchestrate specialized subagents. You'll learn the core concepts, installation, configuration, and advanced patterns for building multi-agent systems.

**Time to complete**: 60-90 minutes

## What You'll Learn

- Install and configure Claude Agent SDK
- Create your first subagent configuration
- Invoke subagents and handle streaming responses
- Integrate MCP servers for tools
- Handle errors and implement retry logic
- Run subagents in parallel and sequence

## Prerequisites

- **Phase 0 Setup Complete**: TypeScript project with Vitest
- **Node.js 18+**: `node --version`
- **Anthropic API Key**: Get from https://console.anthropic.com
- **Understanding of async/await**: Basic Promise knowledge

## Core Concepts

### What is the Claude Agent SDK?

The Claude Agent SDK enables you to:
- Define specialized "subagents" with focused prompts and tools
- Delegate tasks to these subagents programmatically
- Stream responses asynchronously
- Integrate external tools via Model Context Protocol (MCP)

### The `query()` API

The primary API for invoking agents:

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';

const response = query({
  prompt: "Your task description",
  options: {
    model: "claude-sonnet-4-5",
    agents: { /* subagent definitions */ },
    mcpServers: { /* MCP server connections */ }
  }
});
```

### Subagent Architecture

```
Main Orchestrator
    │
    ├─► Subagent A (specialized task)
    ├─► Subagent B (specialized task)
    └─► Subagent C (validation)
```

Each subagent has:
- **Description**: What it does
- **Prompt**: Specialized instructions
- **Tools**: Available capabilities (Read, Write, Bash, MCP tools)
- **Model**: Which Claude model to use

## Step 1: Install Claude Agent SDK

### 1.1 Install Package

**IMPORTANT NOTE**: As of January 2025, the Claude Agent SDK package name may be different from what's shown here. Check the official Anthropic documentation for the correct package name and installation instructions.

For this guide, we'll use the placeholder package name:

```bash
# Install Claude Agent SDK (VERIFY ACTUAL PACKAGE NAME)
npm install @anthropic-ai/claude-agent-sdk

# Install types if separate package
npm install -D @types/anthropic-ai__claude-agent-sdk
```

**Official Documentation**: https://docs.anthropic.com/en/docs/agents

### 1.2 Set Up Environment Variables

Add to `.env`:

```bash
# Anthropic API Key (REQUIRED)
ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxx

# Optional: Override default model
DEFAULT_MODEL=claude-sonnet-4-5

# Optional: Enable debug logging
AGENT_SDK_DEBUG=true
```

### 1.3 Load Environment Variables

Update `src/index.ts` (or create if missing):

```typescript
import { config } from 'dotenv';

// Load environment variables
config();

// Validate required environment variables
if (!process.env.ANTHROPIC_API_KEY) {
  throw new Error('ANTHROPIC_API_KEY is required. Set it in .env file.');
}

console.log('✓ Environment variables loaded');
```

### Verification Step 1

```bash
# Test environment loading
node -r ts-node/register src/index.ts
```

**Expected output**:
```
✓ Environment variables loaded
```

## Step 2: Create Your First Subagent

### 2.1 Create Agents Directory

```bash
mkdir -p src/agents
```

### 2.2 Define a Simple Subagent

Create `src/agents/hello-agent.ts`:

```typescript
import { AgentDefinition } from '@anthropic-ai/claude-agent-sdk';

/**
 * Simple "hello world" subagent that greets users
 */
export const helloAgent: AgentDefinition = {
  description: 'Greets users in a friendly manner',
  prompt: `You are a friendly greeter.
  When given a name, respond with a warm, personalized greeting.
  Be enthusiastic but professional.`,
  tools: [], // No tools needed for simple greeting
  model: 'claude-sonnet-4-5',
};

/**
 * All subagent definitions for easy import
 */
export const agents = {
  'hello-agent': helloAgent,
};
```

### 2.3 Create Orchestrator

Create `src/agents/orchestrator.ts`:

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';
import { agents } from './hello-agent';

/**
 * Invokes a subagent and returns the complete response
 */
export async function invokeSubagent(
  agentName: string,
  prompt: string
): Promise<string> {
  console.log(`Invoking subagent: ${agentName}`);
  console.log(`Prompt: ${prompt}`);

  const response = query({
    prompt,
    options: {
      model: 'claude-sonnet-4-5',
      agents,
      apiKey: process.env.ANTHROPIC_API_KEY,
    },
  });

  let fullResponse = '';
  let messageCount = 0;

  for await (const message of response) {
    messageCount++;

    // Log message type for debugging
    if (process.env.AGENT_SDK_DEBUG === 'true') {
      console.log('Message:', message.type, message.subtype || '');
    }

    // Handle different message types
    if (message.type === 'system' && message.subtype === 'subagent_start') {
      console.log(`  → Subagent started: ${message.agent_name}`);
    }

    if (message.type === 'system' && message.subtype === 'subagent_end') {
      console.log(`  ✓ Subagent completed: ${message.agent_name}`);
    }

    if (message.type === 'text') {
      fullResponse += message.text;
    }
  }

  console.log(`Received ${messageCount} messages`);
  return fullResponse;
}
```

### 2.4 Create Test Runner

Create `src/agents/run-hello.ts`:

```typescript
import { config } from 'dotenv';
import { invokeSubagent } from './orchestrator';

config();

async function main() {
  try {
    console.log('=== Hello Agent Test ===\n');

    const response = await invokeSubagent('hello-agent', 'My name is Alice');

    console.log('\n=== Response ===');
    console.log(response);
  } catch (error) {
    console.error('Error:', error);
    process.exit(1);
  }
}

main();
```

### Verification Step 2

```bash
# Run the hello agent
ts-node src/agents/run-hello.ts
```

**Expected output**:
```
=== Hello Agent Test ===

Invoking subagent: hello-agent
Prompt: My name is Alice
  → Subagent started: hello-agent
  ✓ Subagent completed: hello-agent
Received 5 messages

=== Response ===
Hello Alice! It's wonderful to meet you. I hope you're having a great day!
```

## Step 3: Advanced Subagent Patterns

### 3.1 Subagent with File Access

Create `src/agents/implementation-agents.ts`:

```typescript
import { AgentDefinition } from '@anthropic-ai/claude-agent-sdk';

/**
 * Implementation subagent that can write code
 */
export const codeWriterAgent: AgentDefinition = {
  description: 'Writes TypeScript code following best practices',
  prompt: `You are an expert TypeScript developer.
  When asked to implement a function:
  1. Write clean, type-safe code
  2. Add JSDoc comments
  3. Handle edge cases
  4. Follow functional programming principles when possible
  5. Use strict TypeScript types (no 'any')`,
  tools: ['Write', 'Read', 'Edit'], // File manipulation tools
  model: 'claude-sonnet-4-5',
};

/**
 * Test generator subagent
 */
export const testGeneratorAgent: AgentDefinition = {
  description: 'Generates comprehensive unit tests using Vitest',
  prompt: `You are a testing expert specializing in Vitest.
  When asked to generate tests:
  1. Use describe/it/expect structure
  2. Cover happy path, edge cases, and errors
  3. Use proper arrange-act-assert pattern
  4. Mock external dependencies
  5. Aim for >85% coverage
  Target: Comprehensive, maintainable tests.`,
  tools: ['Write', 'Read', 'Edit'],
  model: 'claude-sonnet-4-5',
};

export const implementationAgents = {
  'code-writer': codeWriterAgent,
  'test-generator': testGeneratorAgent,
};
```

### 3.2 Parallel Subagent Execution

Create `src/agents/parallel-orchestrator.ts`:

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';

/**
 * Runs multiple subagents in parallel
 */
export async function runParallelSubagents(
  tasks: Array<{ agentName: string; prompt: string }>,
  agents: Record<string, any>
): Promise<string[]> {
  console.log(`Running ${tasks.length} subagents in parallel`);

  const promises = tasks.map(async (task) => {
    const response = query({
      prompt: task.prompt,
      options: {
        model: 'claude-sonnet-4-5',
        agents,
        apiKey: process.env.ANTHROPIC_API_KEY,
      },
    });

    let fullResponse = '';
    for await (const message of response) {
      if (message.type === 'text') {
        fullResponse += message.text;
      }
    }
    return fullResponse;
  });

  return Promise.all(promises);
}
```

Example usage:

```typescript
const results = await runParallelSubagents(
  [
    { agentName: 'code-writer', prompt: 'Implement add(a, b) function' },
    { agentName: 'test-generator', prompt: 'Generate tests for add(a, b)' },
  ],
  implementationAgents
);

console.log('Implementation:', results[0]);
console.log('Tests:', results[1]);
```

### 3.3 Sequential with Dependencies

Create `src/agents/sequential-orchestrator.ts`:

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';

/**
 * Runs subagents sequentially, passing results between them
 */
export async function runSequentialSubagents(
  steps: Array<{
    agentName: string;
    promptTemplate: (previousResults: string[]) => string;
  }>,
  agents: Record<string, any>
): Promise<string[]> {
  const results: string[] = [];

  for (const step of steps) {
    const prompt = step.promptTemplate(results);

    const response = query({
      prompt,
      options: {
        model: 'claude-sonnet-4-5',
        agents,
        apiKey: process.env.ANTHROPIC_API_KEY,
      },
    });

    let fullResponse = '';
    for await (const message of response) {
      if (message.type === 'text') {
        fullResponse += message.text;
      }
    }

    results.push(fullResponse);
  }

  return results;
}
```

Example usage (TDD workflow):

```typescript
const tddResults = await runSequentialSubagents(
  [
    {
      agentName: 'test-generator',
      promptTemplate: () => 'Generate failing test for sanitizeApiKeys(text)',
    },
    {
      agentName: 'code-writer',
      promptTemplate: (results) =>
        `Implement minimal code to pass this test:\n\n${results[0]}`,
    },
    {
      agentName: 'code-quality-validator',
      promptTemplate: (results) =>
        `Review this implementation:\n\nTest: ${results[0]}\n\nCode: ${results[1]}`,
    },
  ],
  { ...implementationAgents, ...validationAgents }
);
```

## Step 4: Integrate MCP Tools

### 4.1 Understanding MCP Integration

MCP servers expose tools that subagents can use. For example, a test-runner MCP server might expose:
- `run_unit_tests`: Run Vitest unit tests
- `get_coverage_report`: Get coverage statistics
- `validate_test_quality`: Score test quality

### 4.2 MCP Server Configuration

Create `src/mcp/config.ts`:

```typescript
import { MCPServerConfig } from '@anthropic-ai/claude-agent-sdk';

/**
 * MCP server configurations
 */
export const mcpServers: Record<string, MCPServerConfig> = {
  'test-runner': {
    url: process.env.MCP_TEST_RUNNER_URL || 'http://localhost:3000',
    transport: 'http',
    auth: {
      type: 'none', // Or 'bearer' with token
    },
  },
};
```

### 4.3 Subagent with MCP Tools

Update `src/agents/implementation-agents.ts`:

```typescript
export const testValidatorAgent: AgentDefinition = {
  description: 'Validates test quality using MCP tools',
  prompt: `You are a test quality expert.
  Use the validate_test_quality tool to score tests.
  Require a score ≥ 0.8 to pass.
  Provide specific feedback on failing tests.`,
  tools: [
    'Read',
    'mcp__test-runner__validate_test_quality',
    'mcp__test-runner__get_coverage_report',
  ],
  model: 'claude-sonnet-4-5',
};
```

### 4.4 Invoking with MCP

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';
import { mcpServers } from '../mcp/config';

const response = query({
  prompt: 'Validate the tests in src/sanitization/sanitize.test.ts',
  options: {
    model: 'claude-sonnet-4-5',
    agents: {
      'test-validator': testValidatorAgent,
    },
    mcpServers, // Connect MCP servers
    apiKey: process.env.ANTHROPIC_API_KEY,
  },
});
```

**Expected MCP Tool Call**:
```json
{
  "type": "tool_use",
  "name": "mcp__test-runner__validate_test_quality",
  "input": {
    "testFilePath": "src/sanitization/sanitize.test.ts"
  }
}
```

**Expected MCP Response**:
```json
{
  "score": 0.92,
  "feedback": "Excellent coverage of edge cases. Clear test names.",
  "issues": []
}
```

## Step 5: Error Handling and Retry Logic

### 5.1 Error Types

Common errors when using the SDK:

- **Authentication Error**: Invalid API key
- **Rate Limit Error**: Too many requests (HTTP 429)
- **Network Error**: Connection timeout
- **Validation Error**: Invalid agent configuration

### 5.2 Retry with Exponential Backoff

Create `src/agents/retry.ts`:

```typescript
/**
 * Retries a function with exponential backoff
 */
export async function retryWithBackoff<T>(
  fn: () => Promise<T>,
  maxAttempts = 3,
  initialDelayMs = 1000
): Promise<T> {
  let lastError: Error | undefined;

  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;

      // Don't retry on authentication errors
      if (error instanceof Error && error.message.includes('authentication')) {
        throw error;
      }

      if (attempt < maxAttempts) {
        const delayMs = initialDelayMs * Math.pow(2, attempt - 1);
        console.warn(`Attempt ${attempt} failed. Retrying in ${delayMs}ms...`);
        await sleep(delayMs);
      }
    }
  }

  throw new Error(`Failed after ${maxAttempts} attempts: ${lastError?.message}`);
}

function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}
```

### 5.3 Safe Subagent Invocation

```typescript
import { retryWithBackoff } from './retry';

export async function safeInvokeSubagent(
  agentName: string,
  prompt: string,
  agents: Record<string, any>
): Promise<string> {
  return retryWithBackoff(async () => {
    return invokeSubagent(agentName, prompt, agents);
  });
}
```

### 5.4 Token Budget Guard

Create `src/agents/budget.ts`:

```typescript
/**
 * Tracks token usage to prevent runaway costs
 */
export class TokenBudget {
  private usedTokens = 0;

  constructor(private maxTokens: number) {}

  /**
   * Check if we can spend more tokens
   */
  canSpend(estimatedTokens: number): boolean {
    return this.usedTokens + estimatedTokens <= this.maxTokens;
  }

  /**
   * Record token usage
   */
  recordUsage(tokens: number): void {
    this.usedTokens += tokens;
  }

  /**
   * Get remaining budget
   */
  remaining(): number {
    return this.maxTokens - this.usedTokens;
  }

  /**
   * Reset budget
   */
  reset(): void {
    this.usedTokens = 0;
  }
}

// Usage
const budget = new TokenBudget(100000); // 100k tokens max

if (!budget.canSpend(5000)) {
  throw new Error('Token budget exceeded');
}

// After subagent invocation
budget.recordUsage(response.usage.total_tokens);
```

## Step 6: Complete Example

### 6.1 TDD Workflow with Subagents

Create `src/agents/tdd-workflow.ts`:

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';
import { implementationAgents } from './implementation-agents';
import { retryWithBackoff } from './retry';

interface TDDResult {
  test: string;
  implementation: string;
  validationScore: number;
}

/**
 * Complete TDD cycle: Red → Green → Refactor
 */
export async function runTDDCycle(
  featureDescription: string
): Promise<TDDResult> {
  console.log('=== TDD Cycle ===');
  console.log('Feature:', featureDescription);

  // RED: Generate failing test
  console.log('\n🔴 RED: Generating test...');
  const test = await retryWithBackoff(async () => {
    const response = query({
      prompt: `Generate a failing test for: ${featureDescription}`,
      options: {
        model: 'claude-sonnet-4-5',
        agents: { 'test-generator': implementationAgents['test-generator'] },
        apiKey: process.env.ANTHROPIC_API_KEY,
      },
    });

    let result = '';
    for await (const message of response) {
      if (message.type === 'text') result += message.text;
    }
    return result;
  });

  console.log('✓ Test generated');

  // GREEN: Implement minimal code
  console.log('\n🟢 GREEN: Implementing code...');
  const implementation = await retryWithBackoff(async () => {
    const response = query({
      prompt: `Implement minimal code to pass this test:\n\n${test}`,
      options: {
        model: 'claude-sonnet-4-5',
        agents: { 'code-writer': implementationAgents['code-writer'] },
        apiKey: process.env.ANTHROPIC_API_KEY,
      },
    });

    let result = '';
    for await (const message of response) {
      if (message.type === 'text') result += message.text;
    }
    return result;
  });

  console.log('✓ Implementation complete');

  // Validate
  console.log('\n✅ Validating...');
  const validationScore = 0.9; // Would use MCP tool in real implementation

  return { test, implementation, validationScore };
}
```

### 6.2 Run Complete Example

Create `src/agents/run-tdd-example.ts`:

```typescript
import { config } from 'dotenv';
import { runTDDCycle } from './tdd-workflow';

config();

async function main() {
  try {
    const result = await runTDDCycle(
      'A function sanitizeApiKeys(text) that redacts API keys and tokens'
    );

    console.log('\n=== Results ===\n');
    console.log('TEST:\n', result.test);
    console.log('\nIMPLEMENTATION:\n', result.implementation);
    console.log('\nVALIDATION SCORE:', result.validationScore);
  } catch (error) {
    console.error('Error:', error);
    process.exit(1);
  }
}

main();
```

## Verification

### Run All Examples

```bash
# Hello agent
ts-node src/agents/run-hello.ts

# TDD workflow example
ts-node src/agents/run-tdd-example.ts
```

## You're Done When...

- ✅ Claude Agent SDK installed and configured
- ✅ Environment variables set (ANTHROPIC_API_KEY)
- ✅ Simple subagent working (hello-agent)
- ✅ Streaming responses handled correctly
- ✅ Error handling with retry implemented
- ✅ Understand parallel and sequential patterns
- ✅ Ready to integrate MCP tools

## Troubleshooting

### Issue: Authentication error

```
Error: Invalid API key
```

**Solution**:
- Verify `ANTHROPIC_API_KEY` in `.env`
- Check API key is valid at https://console.anthropic.com
- Ensure `.env` is loaded: `config()` called before using SDK

### Issue: Rate limit errors

```
Error: Rate limit exceeded (HTTP 429)
```

**Solution**:
- Implement retry with backoff (see Step 5.2)
- Add delays between requests
- Use token budget guard

### Issue: Streaming not working

```
TypeError: response is not iterable
```

**Solution**:
- Use `for await (const message of response)`
- Not `response.forEach()` or `map()`
- Ensure async/await pattern is correct

### Issue: MCP tools not found

```
Error: Tool mcp__test-runner__validate_test_quality not found
```

**Solution**:
- Verify MCP server is running
- Check `mcpServers` configuration
- Ensure tool names match exactly (case-sensitive)

## Best Practices

1. **Always use environment variables** for API keys
2. **Implement retry logic** for production code
3. **Monitor token usage** to control costs
4. **Log subagent invocations** for debugging
5. **Use specific prompts** for better subagent performance
6. **Clean up resources** after streaming completes
7. **Test error paths** (missing API key, network failures)

## Next Steps

Now that you understand the Claude Agent SDK, proceed to:

1. [Using Subagents Guide](./guide-using-subagents-2025-01-16.md) - Patterns for delegation
2. [Testing Harness Usage](./guide-testing-harness-usage-2025-01-16.md) - AI-powered testing
3. [TDD Workflow Guide](./guide-tdd-workflow-2025-01-16.md) - Test-driven development with subagents

## Related Documents

- [Subagent System Architecture](../architecture/architecture-subagent-system-2025-01-16.md)
- [Testing Harness Architecture](../architecture/architecture-testing-harness-2025-01-16.md)
- [Implementation Roadmap](../plans/plan-implementation-roadmap-2025-01-16.md)
````

## File: guides/guide-database-setup-2025-01-16.md
````markdown
---
title: Database Setup Guide
category: guide
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [database, sqlite, setup, migrations, configuration]
difficulty: beginner
estimated_time: 30-45 minutes
prerequisites:
  - Node.js 18+ installed
  - npm or pnpm package manager
  - Basic SQL knowledge
  - Text editor or IDE
references:
  - docs/STANDARDS.md (Section 2: Schema Standard)
  - docs/reference/reference-database-schema-2025-01-16.md
  - docs/decisions/decision-use-sqlite-2025-01-16.md
  - docs/architecture/architecture-async-processing-2025-01-16.md
---

# Database Setup Guide

> Step-by-step guide to set up the SQLite database for the Global Context Network with proper configuration, migrations, and validation.

---

## Overview

This guide walks you through setting up the complete database infrastructure for the Global Context Network. You'll configure SQLite with optimal settings, create all six canonical tables, set up migrations, and verify everything works correctly.

**What You'll Build**:
- SQLite database with WAL mode and foreign key enforcement
- All 6 canonical tables (conversations, messages, learnings, job_queue, uploads, sanitization_log)
- Migration system using Atlas (preferred) or custom TypeScript migrations
- Performance indexes for fast queries
- Backup and recovery procedures
- Validation and testing scripts

**Time Estimate**: 30-45 minutes

---

## Prerequisites

Before starting, ensure you have:

1. **Node.js 18+** installed
   ```bash
   node --version  # Should be v18.0.0 or higher
   ```

2. **Package Manager** (npm or pnpm)
   ```bash
   npm --version   # or: pnpm --version
   ```

3. **Project Repository** cloned and dependencies installed
   ```bash
   git clone <repository-url>
   cd shared-context-app
   npm install  # or: pnpm install
   ```

4. **Basic SQL Knowledge** - Understanding of CREATE TABLE, indexes, and foreign keys

5. **Text Editor or IDE** - VS Code, Cursor, or similar

---

## Step 1: Install Dependencies

### Core Database Dependencies

Install the required packages:

```bash
npm install better-sqlite3 ulid
npm install --save-dev @types/better-sqlite3
```

**Package Purposes**:
- `better-sqlite3` - SQLite database driver (synchronous, fast)
- `ulid` - ULID ID generation (time-sortable, lexicographic)
- `@types/better-sqlite3` - TypeScript type definitions

### Migration Tool (Atlas - Recommended)

Install Atlas CLI for database migrations:

**macOS (Homebrew)**:
```bash
brew install ariga/tap/atlas
```

**Linux**:
```bash
curl -sSf https://atlasgo.sh | sh
```

**Windows**:
```bash
# Download from https://github.com/ariga/atlas/releases
# Add to PATH
```

**Verify Installation**:
```bash
atlas version
# Should show: atlas version v0.x.x
```

**Alternative: Custom Migration System**

If you prefer a custom TypeScript migration runner (see Step 5 for implementation), no additional tools are needed beyond Node.js.

---

## Step 2: Configure SQLite

### Database File Location

Create a `src/database/` directory:

```bash
mkdir -p src/database
```

**Database File Path**: `src/database/context.db`

**IMPORTANT**: Add to `.gitignore`:
```bash
echo "src/database/*.db*" >> .gitignore
echo "src/database/*.db-shm" >> .gitignore
echo "src/database/*.db-wal" >> .gitignore
```

### SQLite Pragmas (Required)

Create `src/database/config.ts`:

```typescript
import Database from 'better-sqlite3';

/**
 * Database configuration with optimal SQLite settings
 *
 * CRITICAL: These PRAGMAs MUST be set on every connection
 */
export interface DatabaseConfig {
  path: string;
  readonly?: boolean;
  verbose?: boolean;
}

export function createDatabase(config: DatabaseConfig): Database.Database {
  const { path, readonly = false, verbose = false } = config;

  // Create database connection
  const db = new Database(path, {
    readonly,
    fileMustExist: false,
    timeout: 5000,
    verbose: verbose ? console.log : undefined
  });

  // CRITICAL: Set required PRAGMAs
  configurePragmas(db);

  return db;
}

/**
 * Configure SQLite PRAGMAs for optimal performance and safety
 *
 * From ADR-005 and STANDARDS.md
 */
function configurePragmas(db: Database.Database): void {
  // 1. FOREIGN KEYS - MUST be ON for referential integrity
  db.pragma('foreign_keys = ON');

  // 2. WAL MODE - Write-Ahead Logging for concurrency
  // Allows readers while writer is active
  db.pragma('journal_mode = WAL');

  // 3. SYNCHRONOUS - Balance between safety and performance
  // NORMAL is safe with WAL mode
  db.pragma('synchronous = NORMAL');

  // 4. BUSY TIMEOUT - Wait 5s for write locks
  db.pragma('busy_timeout = 5000');

  // 5. PAGE SIZE - 8KB pages (optimal for modern systems)
  // Set before first write to database
  const pageSize = db.pragma('page_size', { simple: true });
  if (pageSize !== 8192) {
    db.pragma('page_size = 8192');
  }

  // 6. CACHE SIZE - ~40MB cache (5000 pages * 8KB)
  db.pragma('cache_size = -40000');  // Negative = KB instead of pages

  // 7. TEMP STORE - Keep temp tables in memory
  db.pragma('temp_store = MEMORY');

  // 8. MMAP SIZE - Memory-mapped I/O (30GB max)
  db.pragma('mmap_size = 30000000000');

  // 9. AUTO VACUUM - Incremental to keep database compact
  db.pragma('auto_vacuum = INCREMENTAL');
}

/**
 * Verify database configuration
 *
 * Call after creating database to ensure proper setup
 */
export function verifyConfiguration(db: Database.Database): boolean {
  const checks = [
    { name: 'foreign_keys', expected: 1 },
    { name: 'journal_mode', expected: 'wal' },
    { name: 'synchronous', expected: 1 }  // NORMAL = 1
  ];

  for (const check of checks) {
    const value = db.pragma(check.name, { simple: true });
    if (value !== check.expected) {
      console.error(`PRAGMA ${check.name} = ${value}, expected ${check.expected}`);
      return false;
    }
  }

  return true;
}

/**
 * Get database info for debugging
 */
export function getDatabaseInfo(db: Database.Database): Record<string, any> {
  return {
    foreign_keys: db.pragma('foreign_keys', { simple: true }),
    journal_mode: db.pragma('journal_mode', { simple: true }),
    synchronous: db.pragma('synchronous', { simple: true }),
    page_size: db.pragma('page_size', { simple: true }),
    cache_size: db.pragma('cache_size', { simple: true }),
    temp_store: db.pragma('temp_store', { simple: true }),
    auto_vacuum: db.pragma('auto_vacuum', { simple: true })
  };
}
```

**Test Configuration**:

```bash
# Create test script: src/database/test-config.ts
cat > src/database/test-config.ts << 'EOF'
import { createDatabase, verifyConfiguration, getDatabaseInfo } from './config';

const db = createDatabase({ path: ':memory:', verbose: true });

console.log('Database Configuration:');
console.log(JSON.stringify(getDatabaseInfo(db), null, 2));

console.log('\nVerification:', verifyConfiguration(db) ? 'PASS' : 'FAIL');

db.close();
EOF

# Run test
npx tsx src/database/test-config.ts
```

**Expected Output**:
```json
{
  "foreign_keys": 1,
  "journal_mode": "wal",
  "synchronous": 1,
  "page_size": 8192,
  "cache_size": -40000,
  "temp_store": 2,
  "auto_vacuum": 2
}

Verification: PASS
```

---

## Step 3: Create Database Schema

### Option A: Using Atlas Migrations (Recommended)

**1. Create Atlas Configuration**

Create `atlas.hcl` in project root:

```hcl
# Atlas configuration for Global Context Network
# See: https://atlasgo.io/atlas-schema/sql

env "local" {
  src = "file://src/database/schema.sql"

  dev = "sqlite://file?mode=memory&_fk=1"

  url = "sqlite://src/database/context.db"

  migration {
    dir = "file://src/database/migrations"
  }
}
```

**2. Create Schema File**

Create `src/database/schema.sql` with the canonical schema:

```sql
-- Global Context Network Database Schema
-- Version: 1.0.0
-- Date: 2025-01-16
--
-- CRITICAL: This is the CANONICAL schema from STANDARDS.md
-- All 6 production tables, NO events/event_queue tables

-- ============================================================================
-- TABLE: conversations
-- Stores sanitized conversation metadata
-- ============================================================================

CREATE TABLE IF NOT EXISTS conversations (
  -- Identity
  id TEXT PRIMARY KEY,                    -- ULID
  session_id TEXT NOT NULL,               -- Claude Code session ID
  correlation_id TEXT NOT NULL UNIQUE,    -- Tracking ID

  -- Timestamps (ISO-8601 format)
  created_at TEXT NOT NULL DEFAULT (datetime('now')),
  updated_at TEXT NOT NULL DEFAULT (datetime('now')),
  completed_at TEXT,                      -- When conversation ended

  -- Sanitization (ALWAYS true)
  sanitized BOOLEAN NOT NULL DEFAULT 1 CHECK (sanitized = 1),
  sanitization_version TEXT NOT NULL,     -- e.g., "1.0.0"

  -- Metrics
  message_count INTEGER NOT NULL DEFAULT 0,

  -- Additional data
  metadata JSON                           -- Flexible JSON field
);

-- Indexes
CREATE INDEX idx_conversations_session ON conversations(session_id);
CREATE INDEX idx_conversations_created ON conversations(created_at DESC);
CREATE INDEX idx_conversations_correlation ON conversations(correlation_id);

-- Trigger: Auto-update updated_at
CREATE TRIGGER conversations_updated_at
AFTER UPDATE ON conversations
FOR EACH ROW
BEGIN
  UPDATE conversations SET updated_at = datetime('now') WHERE id = NEW.id;
END;

-- ============================================================================
-- TABLE: messages
-- Stores individual sanitized messages
-- ============================================================================

CREATE TABLE IF NOT EXISTS messages (
  -- Identity
  id TEXT PRIMARY KEY,                    -- ULID
  conversation_id TEXT NOT NULL,

  -- Content (SANITIZED ONLY)
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
  content TEXT NOT NULL,                  -- SANITIZED content
  content_hash TEXT NOT NULL,             -- SHA-256 for deduplication

  -- Ordering
  sequence INTEGER NOT NULL,              -- Order within conversation (0, 1, 2, ...)

  -- Timestamps
  created_at TEXT NOT NULL DEFAULT (datetime('now')),

  -- Metrics
  token_count INTEGER,                    -- Approximate tokens

  -- Additional data
  metadata JSON,                          -- Tool calls, thinking, etc.

  -- Constraints
  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE,
  UNIQUE (conversation_id, sequence)
);

-- Indexes
CREATE INDEX idx_messages_conversation ON messages(conversation_id, sequence);
CREATE INDEX idx_messages_created ON messages(created_at DESC);
CREATE INDEX idx_messages_hash ON messages(content_hash);

-- Triggers: Update conversation message_count
CREATE TRIGGER messages_after_insert
AFTER INSERT ON messages
FOR EACH ROW
BEGIN
  UPDATE conversations
  SET message_count = message_count + 1
  WHERE id = NEW.conversation_id;
END;

CREATE TRIGGER messages_after_delete
AFTER DELETE ON messages
FOR EACH ROW
BEGIN
  UPDATE conversations
  SET message_count = message_count - 1
  WHERE id = OLD.conversation_id;
END;

-- ============================================================================
-- TABLE: learnings
-- Stores extracted insights with full-text search
-- ============================================================================

CREATE TABLE IF NOT EXISTS learnings (
  -- Identity
  id TEXT PRIMARY KEY,                    -- ULID
  conversation_id TEXT NOT NULL,
  source_message_ids JSON NOT NULL,       -- Array of message IDs

  -- Content
  category TEXT NOT NULL CHECK (
    category IN (
      'pattern',
      'best_practice',
      'anti_pattern',
      'bug_fix',
      'optimization',
      'tool_usage',
      'workflow',
      'decision'
    )
  ),
  title TEXT NOT NULL,                    -- Short summary
  content TEXT NOT NULL,                  -- Detailed learning (SANITIZED)

  -- Quality
  confidence REAL NOT NULL CHECK (confidence >= 0.0 AND confidence <= 1.0),
  tags JSON NOT NULL DEFAULT '[]',        -- Array of strings
  dedupe_hash TEXT NOT NULL UNIQUE,       -- Prevent duplicates

  -- Timestamps
  created_at TEXT NOT NULL DEFAULT (datetime('now')),

  -- Additional data
  metadata JSON,

  -- Constraints
  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- Indexes
CREATE INDEX idx_learnings_conversation ON learnings(conversation_id);
CREATE INDEX idx_learnings_category ON learnings(category, confidence DESC);
CREATE INDEX idx_learnings_confidence ON learnings(confidence DESC);
CREATE INDEX idx_learnings_created ON learnings(created_at DESC);
CREATE INDEX idx_learnings_dedupe ON learnings(dedupe_hash);

-- Full-Text Search (FTS5)
CREATE VIRTUAL TABLE learnings_fts USING fts5(
  learning_id UNINDEXED,
  title,
  content,
  tags,
  content='learnings',
  content_rowid='rowid'
);

-- FTS Sync Triggers
CREATE TRIGGER learnings_fts_insert
AFTER INSERT ON learnings
BEGIN
  INSERT INTO learnings_fts(rowid, learning_id, title, content, tags)
  VALUES (NEW.rowid, NEW.id, NEW.title, NEW.content, NEW.tags);
END;

CREATE TRIGGER learnings_fts_delete
AFTER DELETE ON learnings
BEGIN
  DELETE FROM learnings_fts WHERE rowid = OLD.rowid;
END;

CREATE TRIGGER learnings_fts_update
AFTER UPDATE ON learnings
BEGIN
  DELETE FROM learnings_fts WHERE rowid = OLD.rowid;
  INSERT INTO learnings_fts(rowid, learning_id, title, content, tags)
  VALUES (NEW.rowid, NEW.id, NEW.title, NEW.content, NEW.tags);
END;

-- ============================================================================
-- TABLE: job_queue
-- Persistent queue for async processing
-- ============================================================================

CREATE TABLE IF NOT EXISTS job_queue (
  -- Identity
  id TEXT PRIMARY KEY,                    -- ULID
  type TEXT NOT NULL,                     -- 'sanitize_ai_validation', 'extract_learning', 'mine_upload'

  -- Status (CANONICAL from STANDARDS.md)
  status TEXT NOT NULL DEFAULT 'queued' CHECK (
    status IN ('queued', 'in_progress', 'completed', 'failed', 'dead_letter')
  ),

  -- Priority and Scheduling
  priority INTEGER NOT NULL DEFAULT 5 CHECK (priority >= 1 AND priority <= 10),
  scheduled_at TEXT NOT NULL DEFAULT (datetime('now')),

  -- Worker Coordination (Lease-based)
  lease_owner TEXT,                       -- Worker ID (hostname:pid)
  lease_until TEXT,                       -- Lease expiry timestamp

  -- Payload
  payload TEXT NOT NULL,                  -- JSON job data
  idempotency_key TEXT UNIQUE,            -- Deduplication

  -- Retry Management
  attempts INTEGER NOT NULL DEFAULT 0,
  max_retries INTEGER NOT NULL DEFAULT 3,
  error TEXT,                             -- Last error message

  -- Timestamps (ALL ISO-8601)
  created_at TEXT NOT NULL DEFAULT (datetime('now')),
  updated_at TEXT NOT NULL DEFAULT (datetime('now')),
  started_at TEXT,                        -- When worker claimed
  completed_at TEXT,                      -- When finished

  -- Result
  result TEXT                             -- JSON result data
);

-- Indexes for worker queries
CREATE INDEX idx_job_queue_dequeue
  ON job_queue(status, priority, scheduled_at)
  WHERE status = 'queued';

CREATE INDEX idx_job_queue_lease_expiry
  ON job_queue(lease_until)
  WHERE status = 'in_progress';

CREATE INDEX idx_job_queue_type ON job_queue(type, status);
CREATE INDEX idx_job_queue_idempotency ON job_queue(idempotency_key);

-- Trigger: Auto-update updated_at
CREATE TRIGGER job_queue_updated_at
AFTER UPDATE ON job_queue
FOR EACH ROW
BEGIN
  UPDATE job_queue SET updated_at = datetime('now') WHERE id = NEW.id;
END;

-- ============================================================================
-- TABLE: uploads
-- Tracks uploads to global network (IPFS + blockchain)
-- ============================================================================

CREATE TABLE IF NOT EXISTS uploads (
  -- Identity
  id TEXT PRIMARY KEY,                    -- ULID
  learning_id TEXT NOT NULL UNIQUE,

  -- Upload Status
  ipfs_cid TEXT UNIQUE,                   -- IPFS Content Identifier
  chain_tx_hash TEXT UNIQUE,              -- Blockchain transaction hash
  status TEXT NOT NULL DEFAULT 'pending' CHECK (
    status IN ('pending', 'ipfs_uploaded', 'tx_submitted', 'confirmed', 'failed')
  ),

  -- Retry Management
  retries INTEGER NOT NULL DEFAULT 0,
  max_retries INTEGER NOT NULL DEFAULT 5,
  last_error TEXT,

  -- Rewards
  tokens_earned REAL,                     -- Reward if confirmed

  -- Timestamps
  created_at TEXT NOT NULL DEFAULT (datetime('now')),
  updated_at TEXT NOT NULL DEFAULT (datetime('now')),
  uploaded_at TEXT,                       -- IPFS upload timestamp
  confirmed_at TEXT,                      -- Blockchain confirmation timestamp

  -- Constraints
  FOREIGN KEY (learning_id) REFERENCES learnings(id) ON DELETE CASCADE
);

-- Indexes
CREATE INDEX idx_uploads_learning ON uploads(learning_id);
CREATE INDEX idx_uploads_status ON uploads(status);
CREATE INDEX idx_uploads_created ON uploads(created_at DESC);
CREATE UNIQUE INDEX idx_uploads_ipfs_cid ON uploads(ipfs_cid) WHERE ipfs_cid IS NOT NULL;
CREATE UNIQUE INDEX idx_uploads_tx_hash ON uploads(chain_tx_hash) WHERE chain_tx_hash IS NOT NULL;

-- Trigger: Auto-update updated_at
CREATE TRIGGER uploads_updated_at
AFTER UPDATE ON uploads
FOR EACH ROW
BEGIN
  UPDATE uploads SET updated_at = datetime('now') WHERE id = NEW.id;
END;

-- ============================================================================
-- TABLE: sanitization_log
-- Audit trail of PII detections and redactions
-- ============================================================================

CREATE TABLE IF NOT EXISTS sanitization_log (
  -- Identity
  id TEXT PRIMARY KEY,                    -- ULID
  conversation_id TEXT NOT NULL,
  message_id TEXT,                        -- NULL if conversation-level

  -- Detection
  category TEXT NOT NULL,                 -- PII type (api_key, email, file_path, etc.)
  rule_id TEXT,                           -- Which rule detected it
  original_snippet_hash TEXT NOT NULL,    -- SHA-256 of PII (NEVER store actual PII)
  replacement TEXT NOT NULL,              -- Replacement text used
  detector TEXT NOT NULL CHECK (detector IN ('rule', 'ai', 'hybrid')),
  confidence REAL NOT NULL CHECK (confidence >= 0.0 AND confidence <= 1.0),

  -- Timestamps
  created_at TEXT NOT NULL DEFAULT (datetime('now')),

  -- Additional data
  metadata JSON,

  -- Constraints
  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE,
  FOREIGN KEY (message_id) REFERENCES messages(id) ON DELETE CASCADE
);

-- Indexes
CREATE INDEX idx_sanitization_log_conversation ON sanitization_log(conversation_id);
CREATE INDEX idx_sanitization_log_message ON sanitization_log(message_id);
CREATE INDEX idx_sanitization_log_category ON sanitization_log(category);
CREATE INDEX idx_sanitization_log_created ON sanitization_log(created_at DESC);
```

**3. Generate Migration**

```bash
atlas migrate diff initial \
  --env local \
  --to file://src/database/schema.sql
```

**4. Apply Migration**

```bash
atlas migrate apply \
  --env local \
  --url sqlite://src/database/context.db
```

**5. Verify Schema**

```bash
atlas schema inspect \
  --env local \
  --url sqlite://src/database/context.db
```

### Option B: Custom TypeScript Migrations

If you prefer custom migrations, create `src/database/migrations/runner.ts`:

```typescript
import Database from 'better-sqlite3';
import fs from 'fs';
import path from 'path';

interface Migration {
  version: number;
  name: string;
  up: string;
  down: string;
}

/**
 * Create migrations tracking table
 */
function createMigrationsTable(db: Database.Database): void {
  db.exec(`
    CREATE TABLE IF NOT EXISTS _migrations (
      version INTEGER PRIMARY KEY,
      name TEXT NOT NULL,
      applied_at TEXT NOT NULL DEFAULT (datetime('now'))
    );
  `);
}

/**
 * Get current schema version
 */
function getCurrentVersion(db: Database.Database): number {
  const row = db.prepare('SELECT MAX(version) as version FROM _migrations').get() as { version: number | null };
  return row.version ?? 0;
}

/**
 * Load migration files from directory
 */
function loadMigrations(dir: string): Migration[] {
  const files = fs.readdirSync(dir).sort();
  const migrations: Migration[] = [];

  for (const file of files) {
    if (!file.endsWith('.sql')) continue;

    const match = file.match(/^(\d+)_(.+)\.sql$/);
    if (!match) continue;

    const version = parseInt(match[1], 10);
    const name = match[2];
    const content = fs.readFileSync(path.join(dir, file), 'utf8');

    // Split on -- UP / -- DOWN markers
    const [up, down] = content.split(/--\s*DOWN/i);
    const upSql = up.replace(/--\s*UP/i, '').trim();
    const downSql = down?.trim() || '';

    migrations.push({ version, name, up: upSql, down: downSql });
  }

  return migrations;
}

/**
 * Run pending migrations
 */
export function runMigrations(db: Database.Database, targetVersion?: number): void {
  createMigrationsTable(db);

  const currentVersion = getCurrentVersion(db);
  const migrationsDir = path.join(__dirname, 'sql');
  const migrations = loadMigrations(migrationsDir);

  const toApply = migrations.filter(m =>
    m.version > currentVersion && (!targetVersion || m.version <= targetVersion)
  );

  if (toApply.length === 0) {
    console.log('No migrations to apply');
    return;
  }

  for (const migration of toApply) {
    console.log(`Applying migration ${migration.version}: ${migration.name}`);

    const applyMigration = db.transaction(() => {
      db.exec(migration.up);
      db.prepare('INSERT INTO _migrations (version, name) VALUES (?, ?)').run(migration.version, migration.name);
    });

    applyMigration();
  }

  console.log(`Migrated to version ${toApply[toApply.length - 1].version}`);
}

/**
 * Rollback migrations to target version
 */
export function rollbackMigrations(db: Database.Database, targetVersion: number): void {
  const currentVersion = getCurrentVersion(db);
  const migrationsDir = path.join(__dirname, 'sql');
  const migrations = loadMigrations(migrationsDir);

  const toRollback = migrations
    .filter(m => m.version > targetVersion && m.version <= currentVersion)
    .reverse();

  if (toRollback.length === 0) {
    console.log('No migrations to rollback');
    return;
  }

  for (const migration of toRollback) {
    console.log(`Rolling back migration ${migration.version}: ${migration.name}`);

    const rollback = db.transaction(() => {
      db.exec(migration.down);
      db.prepare('DELETE FROM _migrations WHERE version = ?').run(migration.version);
    });

    rollback();
  }

  console.log(`Rolled back to version ${targetVersion}`);
}
```

Then create your first migration file at `src/database/migrations/sql/001_initial.sql` with the schema from Option A above, formatted as:

```sql
-- UP

[Full schema SQL here]

-- DOWN

DROP TRIGGER IF EXISTS sanitization_log_created;
-- ... (drop all objects in reverse order)
```

---

## Step 4: Validate Schema

Create a validation script `src/database/validate-schema.ts`:

```typescript
import { createDatabase, verifyConfiguration } from './config';
import Database from 'better-sqlite3';

interface ValidationResult {
  passed: boolean;
  errors: string[];
  warnings: string[];
  info: string[];
}

/**
 * Validate database schema against STANDARDS.md
 */
export function validateSchema(db: Database.Database): ValidationResult {
  const result: ValidationResult = {
    passed: true,
    errors: [],
    warnings: [],
    info: []
  };

  // Check configuration
  if (!verifyConfiguration(db)) {
    result.errors.push('Database configuration invalid');
    result.passed = false;
  }

  // Check all 6 canonical tables exist
  const requiredTables = [
    'conversations',
    'messages',
    'learnings',
    'job_queue',
    'uploads',
    'sanitization_log'
  ];

  const tables = db.prepare(`
    SELECT name FROM sqlite_master
    WHERE type='table' AND name NOT LIKE 'sqlite_%' AND name NOT LIKE '_%'
  `).all() as { name: string }[];

  const tableNames = tables.map(t => t.name);

  for (const table of requiredTables) {
    if (!tableNames.includes(table)) {
      result.errors.push(`Missing canonical table: ${table}`);
      result.passed = false;
    } else {
      result.info.push(`✓ Table exists: ${table}`);
    }
  }

  // Check forbidden tables (from old schema)
  const forbiddenTables = ['events', 'event_queue'];
  for (const table of forbiddenTables) {
    if (tableNames.includes(table)) {
      result.errors.push(`Forbidden table exists: ${table} (violates STANDARDS.md)`);
      result.passed = false;
    }
  }

  // Check indexes
  const criticalIndexes = [
    'idx_conversations_session',
    'idx_messages_conversation',
    'idx_learnings_category',
    'idx_job_queue_dequeue',
    'idx_uploads_learning',
    'idx_sanitization_log_conversation'
  ];

  const indexes = db.prepare(`
    SELECT name FROM sqlite_master WHERE type='index'
  `).all() as { name: string }[];

  const indexNames = indexes.map(i => i.name);

  for (const index of criticalIndexes) {
    if (!indexNames.includes(index)) {
      result.warnings.push(`Missing recommended index: ${index}`);
    } else {
      result.info.push(`✓ Index exists: ${index}`);
    }
  }

  // Check FTS table
  if (!tableNames.includes('learnings_fts')) {
    result.errors.push('Missing FTS table: learnings_fts');
    result.passed = false;
  } else {
    result.info.push('✓ FTS table exists: learnings_fts');
  }

  // Check triggers
  const triggers = db.prepare(`
    SELECT name FROM sqlite_master WHERE type='trigger'
  `).all() as { name: string }[];

  const criticalTriggers = [
    'conversations_updated_at',
    'messages_after_insert',
    'learnings_fts_insert',
    'job_queue_updated_at',
    'uploads_updated_at'
  ];

  const triggerNames = triggers.map(t => t.name);

  for (const trigger of criticalTriggers) {
    if (!triggerNames.includes(trigger)) {
      result.warnings.push(`Missing trigger: ${trigger}`);
    } else {
      result.info.push(`✓ Trigger exists: ${trigger}`);
    }
  }

  // Check foreign keys are enabled
  const foreignKeys = db.pragma('foreign_keys', { simple: true });
  if (foreignKeys !== 1) {
    result.errors.push('Foreign keys are NOT enabled');
    result.passed = false;
  } else {
    result.info.push('✓ Foreign keys enabled');
  }

  // Check WAL mode
  const journalMode = db.pragma('journal_mode', { simple: true });
  if (journalMode !== 'wal') {
    result.errors.push(`Journal mode is ${journalMode}, expected WAL`);
    result.passed = false;
  } else {
    result.info.push('✓ WAL mode enabled');
  }

  return result;
}

/**
 * Run validation and print results
 */
function main() {
  const dbPath = process.argv[2] || 'src/database/context.db';

  console.log(`Validating database: ${dbPath}\n`);

  const db = createDatabase({ path: dbPath });

  try {
    const result = validateSchema(db);

    // Print results
    if (result.info.length > 0) {
      console.log('INFO:');
      result.info.forEach(msg => console.log(`  ${msg}`));
      console.log();
    }

    if (result.warnings.length > 0) {
      console.log('WARNINGS:');
      result.warnings.forEach(msg => console.log(`  ⚠️  ${msg}`));
      console.log();
    }

    if (result.errors.length > 0) {
      console.log('ERRORS:');
      result.errors.forEach(msg => console.log(`  ❌ ${msg}`));
      console.log();
    }

    if (result.passed) {
      console.log('✅ VALIDATION PASSED');
      process.exit(0);
    } else {
      console.log('❌ VALIDATION FAILED');
      process.exit(1);
    }
  } finally {
    db.close();
  }
}

if (require.main === module) {
  main();
}
```

**Run Validation**:

```bash
npx tsx src/database/validate-schema.ts
```

**Expected Output**:
```
Validating database: src/database/context.db

INFO:
  ✓ Table exists: conversations
  ✓ Table exists: messages
  ✓ Table exists: learnings
  ✓ Table exists: job_queue
  ✓ Table exists: uploads
  ✓ Table exists: sanitization_log
  ✓ FTS table exists: learnings_fts
  ✓ Foreign keys enabled
  ✓ WAL mode enabled

✅ VALIDATION PASSED
```

---

## Step 5: Create Backup Procedures

Create `src/database/backup.ts`:

```typescript
import Database from 'better-sqlite3';
import fs from 'fs';
import path from 'path';

/**
 * Backup database to specified path
 */
export async function backupDatabase(
  sourceDb: Database.Database,
  destPath: string
): Promise<void> {
  return new Promise((resolve, reject) => {
    try {
      // Ensure backup directory exists
      const dir = path.dirname(destPath);
      if (!fs.existsSync(dir)) {
        fs.mkdirSync(dir, { recursive: true });
      }

      // Use SQLite backup API
      const backup = sourceDb.backup(destPath);

      const doBackup = () => {
        const remaining = backup.step(100); // Pages per step
        if (remaining === 0) {
          backup.close();
          resolve();
        } else {
          setImmediate(doBackup);
        }
      };

      doBackup();
    } catch (error) {
      reject(error);
    }
  });
}

/**
 * Verify backup integrity
 */
export function verifyBackup(backupPath: string): boolean {
  const db = new Database(backupPath, { readonly: true });
  try {
    const result = db.pragma('integrity_check', { simple: true });
    return result === 'ok';
  } finally {
    db.close();
  }
}

/**
 * Create timestamped backup
 */
export async function createTimestampedBackup(
  sourceDb: Database.Database,
  backupDir: string = 'backups'
): Promise<string> {
  const timestamp = new Date().toISOString().replace(/:/g, '-').replace(/\..+/, '');
  const backupPath = path.join(backupDir, `context-${timestamp}.db`);

  await backupDatabase(sourceDb, backupPath);

  // Verify backup
  if (!verifyBackup(backupPath)) {
    throw new Error('Backup verification failed');
  }

  console.log(`✓ Backup created: ${backupPath}`);
  return backupPath;
}

/**
 * Restore from backup
 */
export async function restoreFromBackup(
  backupPath: string,
  destPath: string
): Promise<void> {
  // Verify backup before restoring
  if (!verifyBackup(backupPath)) {
    throw new Error('Backup is corrupted, cannot restore');
  }

  // Create backup of current database (if exists)
  if (fs.existsSync(destPath)) {
    const currentBackup = `${destPath}.before-restore-${Date.now()}`;
    fs.copyFileSync(destPath, currentBackup);
    console.log(`✓ Current database backed up to: ${currentBackup}`);
  }

  // Copy backup to destination
  fs.copyFileSync(backupPath, destPath);

  console.log(`✓ Restored from backup: ${backupPath}`);
}

/**
 * Clean up old backups (keep last N)
 */
export function cleanupOldBackups(backupDir: string, keepCount: number = 7): void {
  if (!fs.existsSync(backupDir)) {
    return;
  }

  const backups = fs.readdirSync(backupDir)
    .filter(f => f.endsWith('.db'))
    .map(f => ({
      name: f,
      path: path.join(backupDir, f),
      mtime: fs.statSync(path.join(backupDir, f)).mtime
    }))
    .sort((a, b) => b.mtime.getTime() - a.mtime.getTime());

  // Keep latest N backups
  const toDelete = backups.slice(keepCount);

  for (const backup of toDelete) {
    fs.unlinkSync(backup.path);
    console.log(`✓ Deleted old backup: ${backup.name}`);
  }

  console.log(`✓ Kept ${Math.min(backups.length, keepCount)} most recent backups`);
}
```

**Test Backup**:

```bash
# Create test script
cat > src/database/test-backup.ts << 'EOF'
import { createDatabase } from './config';
import { createTimestampedBackup, verifyBackup } from './backup';

async function main() {
  const db = createDatabase({ path: 'src/database/context.db' });

  try {
    const backupPath = await createTimestampedBackup(db);
    console.log('Backup created:', backupPath);

    const isValid = verifyBackup(backupPath);
    console.log('Backup valid:', isValid);
  } finally {
    db.close();
  }
}

main().catch(console.error);
EOF

npx tsx src/database/test-backup.ts
```

---

## Step 6: Testing the Setup

Create comprehensive test suite `src/database/__tests__/schema.test.ts`:

```typescript
import { describe, it, expect, beforeEach, afterEach } from 'vitest';
import Database from 'better-sqlite3';
import { createDatabase } from '../config';
import { validateSchema } from '../validate-schema';
import { ulid } from 'ulid';

describe('Database Schema', () => {
  let db: Database.Database;

  beforeEach(() => {
    db = createDatabase({ path: ':memory:' });
    // Apply migrations here (if using custom migrations)
  });

  afterEach(() => {
    db.close();
  });

  it('should pass schema validation', () => {
    const result = validateSchema(db);
    expect(result.passed).toBe(true);
    expect(result.errors).toHaveLength(0);
  });

  it('should enforce foreign keys', () => {
    // Try to insert message without conversation
    expect(() => {
      db.prepare(`
        INSERT INTO messages (id, conversation_id, role, content, content_hash, sequence)
        VALUES (?, ?, ?, ?, ?, ?)
      `).run(ulid(), 'nonexistent', 'user', 'test', 'hash', 0);
    }).toThrow();
  });

  it('should cascade delete messages when conversation deleted', () => {
    const conversationId = ulid();
    const messageId = ulid();

    // Insert conversation
    db.prepare(`
      INSERT INTO conversations (id, session_id, correlation_id, sanitization_version)
      VALUES (?, ?, ?, ?)
    `).run(conversationId, 'session-1', ulid(), '1.0.0');

    // Insert message
    db.prepare(`
      INSERT INTO messages (id, conversation_id, role, content, content_hash, sequence)
      VALUES (?, ?, ?, ?, ?, ?)
    `).run(messageId, conversationId, 'user', 'test', 'hash', 0);

    // Verify message exists
    let message = db.prepare('SELECT * FROM messages WHERE id = ?').get(messageId);
    expect(message).toBeDefined();

    // Delete conversation
    db.prepare('DELETE FROM conversations WHERE id = ?').run(conversationId);

    // Verify message was cascade deleted
    message = db.prepare('SELECT * FROM messages WHERE id = ?').get(messageId);
    expect(message).toBeUndefined();
  });

  it('should update message_count trigger', () => {
    const conversationId = ulid();

    // Insert conversation
    db.prepare(`
      INSERT INTO conversations (id, session_id, correlation_id, sanitization_version)
      VALUES (?, ?, ?, ?)
    `).run(conversationId, 'session-1', ulid(), '1.0.0');

    // Check initial count
    let conv = db.prepare('SELECT message_count FROM conversations WHERE id = ?').get(conversationId) as any;
    expect(conv.message_count).toBe(0);

    // Insert message
    db.prepare(`
      INSERT INTO messages (id, conversation_id, role, content, content_hash, sequence)
      VALUES (?, ?, ?, ?, ?, ?)
    `).run(ulid(), conversationId, 'user', 'test', 'hash', 0);

    // Check count incremented
    conv = db.prepare('SELECT message_count FROM conversations WHERE id = ?').get(conversationId) as any;
    expect(conv.message_count).toBe(1);
  });

  it('should enforce job status enum', () => {
    expect(() => {
      db.prepare(`
        INSERT INTO job_queue (id, type, status, payload)
        VALUES (?, ?, ?, ?)
      `).run(ulid(), 'test', 'invalid_status', '{}');
    }).toThrow();
  });

  it('should support FTS search on learnings', () => {
    const conversationId = ulid();
    const learningId = ulid();

    // Insert conversation
    db.prepare(`
      INSERT INTO conversations (id, session_id, correlation_id, sanitization_version)
      VALUES (?, ?, ?, ?)
    `).run(conversationId, 'session-1', ulid(), '1.0.0');

    // Insert learning
    db.prepare(`
      INSERT INTO learnings (
        id, conversation_id, source_message_ids, category,
        title, content, confidence, dedupe_hash
      )
      VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    `).run(
      learningId,
      conversationId,
      JSON.stringify([]),
      'pattern',
      'TypeScript best practices',
      'Always use strict type checking',
      0.95,
      'hash-123'
    );

    // Search FTS
    const results = db.prepare(`
      SELECT l.title, l.content
      FROM learnings l
      JOIN learnings_fts fts ON l.rowid = fts.rowid
      WHERE learnings_fts MATCH 'typescript'
    `).all();

    expect(results).toHaveLength(1);
    expect(results[0]).toMatchObject({
      title: 'TypeScript best practices'
    });
  });
});
```

**Run Tests**:

```bash
npx vitest run src/database/__tests__/schema.test.ts
```

---

## Step 7: Common Troubleshooting

### Issue: Foreign Keys Not Working

**Symptoms**: Can insert orphaned records

**Solution**:
```typescript
// Check if foreign keys are enabled
const fk = db.pragma('foreign_keys', { simple: true });
console.log('Foreign keys:', fk); // Should be 1

// Enable if not
db.pragma('foreign_keys = ON');
```

### Issue: WAL Mode Not Persisting

**Symptoms**: `journal_mode` returns 'delete' instead of 'wal'

**Solution**:
```typescript
// Set WAL mode on database creation
db.pragma('journal_mode = WAL');

// Verify
console.log(db.pragma('journal_mode', { simple: true })); // Should be 'wal'

// WAL mode persists across connections
```

### Issue: Database Locked Errors

**Symptoms**: `SQLITE_BUSY` errors during writes

**Solutions**:
1. Increase busy timeout:
   ```typescript
   db.pragma('busy_timeout = 10000'); // 10 seconds
   ```

2. Use transactions for bulk operations:
   ```typescript
   const insertMany = db.transaction((items) => {
     for (const item of items) {
       stmt.run(item);
     }
   });
   ```

3. Check for long-running readers in WAL mode

### Issue: FTS Search Not Working

**Symptoms**: FTS queries return no results

**Solution**:
```typescript
// Rebuild FTS index
db.exec("INSERT INTO learnings_fts(learnings_fts) VALUES('rebuild')");

// Verify triggers exist
const triggers = db.prepare(`
  SELECT name FROM sqlite_master
  WHERE type='trigger' AND name LIKE 'learnings_fts%'
`).all();

console.log('FTS triggers:', triggers); // Should have 3 triggers
```

### Issue: Migration Version Mismatch

**Symptoms**: Migrations don't apply or report wrong version

**Solution**:
```typescript
// Check current version
const version = db.prepare(
  'SELECT MAX(version) as version FROM _migrations'
).get();

console.log('Current version:', version);

// Reset migration table (DANGER: Only in development)
db.exec('DROP TABLE IF EXISTS _migrations');
```

---

## Step 8: Production Checklist

Before deploying to production:

- [ ] **Schema Validation Passes**
  ```bash
  npx tsx src/database/validate-schema.ts
  ```

- [ ] **All Tests Pass**
  ```bash
  npm test
  ```

- [ ] **Backup System Configured**
  ```bash
  # Test backup creation
  npx tsx src/database/test-backup.ts
  ```

- [ ] **Database Files in .gitignore**
  ```bash
  grep "*.db" .gitignore  # Should exist
  ```

- [ ] **WAL Mode Enabled**
  ```sql
  PRAGMA journal_mode;  -- Should return 'wal'
  ```

- [ ] **Foreign Keys Enabled**
  ```sql
  PRAGMA foreign_keys;  -- Should return 1
  ```

- [ ] **Performance Indexes Created**
  ```sql
  SELECT COUNT(*) FROM sqlite_master WHERE type='index';
  -- Should be 20+ indexes
  ```

- [ ] **File Permissions Secure**
  ```bash
  ls -la src/database/*.db  # Should be -rw------- (600)
  ```

- [ ] **Monitoring Setup**
  - Database size tracking
  - Query performance monitoring
  - Backup verification schedule

---

## Next Steps

After completing this setup:

1. **Implement Repository Layer** - Create TypeScript repositories for each table
   - See: `docs/reference/reference-database-schema-2025-01-16.md` (Query Patterns section)

2. **Set Up Job Workers** - Implement async job processing
   - See: `docs/architecture/architecture-async-processing-2025-01-16.md`

3. **Create Hooks** - Build Claude Code hooks that use this database
   - See: `docs/architecture/architecture-hooks-event-capture-2025-01-16.md`

4. **Configure MCP Server** - Expose learnings via MCP
   - See: `docs/plans/plan-phase-5-mcp-server-2025-01-16.md`

5. **Set Up Monitoring** - Track database health and performance
   - Database size growth
   - Query latency (p50, p95, p99)
   - Backup success rate
   - Job queue depth

---

## Additional Resources

### Documentation
- [STANDARDS.md](../STANDARDS.md) - Canonical schema and patterns
- [Database Schema Reference](../reference/reference-database-schema-2025-01-16.md) - Complete schema DDL
- [ADR-005: Use SQLite](../decisions/decision-use-sqlite-2025-01-16.md) - Why SQLite for MVP
- [Async Processing Architecture](../architecture/architecture-async-processing-2025-01-16.md) - Job queue usage

### External Links
- [better-sqlite3 Documentation](https://github.com/WiseLibs/better-sqlite3/wiki)
- [SQLite PRAGMA Documentation](https://www.sqlite.org/pragma.html)
- [SQLite WAL Mode](https://www.sqlite.org/wal.html)
- [Atlas Migrations](https://atlasgo.io/getting-started)
- [FTS5 Full-Text Search](https://www.sqlite.org/fts5.html)

### Tools
- **SQLite CLI**: `sqlite3 src/database/context.db`
- **DB Browser for SQLite**: GUI for exploring database
- **Atlas**: Schema migrations and versioning
- **better-sqlite3-helper**: Additional utilities for better-sqlite3

---

## Summary

You now have a fully configured SQLite database for the Global Context Network with:

✅ Optimal SQLite configuration (WAL mode, foreign keys, performance tuning)
✅ All 6 canonical tables (conversations, messages, learnings, job_queue, uploads, sanitization_log)
✅ Full-text search on learnings (FTS5)
✅ Proper indexes for query performance
✅ Migration system (Atlas or custom TypeScript)
✅ Backup and recovery procedures
✅ Validation and testing scripts
✅ 100% alignment with STANDARDS.md

**Database File**: `src/database/context.db`

**Next**: Implement repository layer and start building hooks!

---

**Questions or Issues?**

- Check the troubleshooting section above
- Review the referenced documentation
- Verify your setup against the validation script
- Check that all prerequisites are met
````

## File: guides/guide-phase-0-foundation-setup-2025-01-16.md
````markdown
# Phase 0 Foundation Setup Guide

> Complete step-by-step guide to set up TypeScript, Vitest, and SQLite infrastructure

---
title: Phase 0 Foundation Setup Guide
category: guide
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [guide, phase-0, typescript, vitest, sqlite, setup]
---

## Overview

This guide walks you through setting up the foundation for the Global Context Network MVP. By the end, you'll have a production-ready TypeScript project with strict type checking, comprehensive testing infrastructure, and SQLite database with migrations.

**Time to complete**: 60-90 minutes

## What You'll Build

- TypeScript project with strict mode enabled
- Vitest testing infrastructure with coverage
- SQLite database with type-safe queries
- Migration system for schema evolution
- Linting and formatting toolchain
- Complete project structure

## Prerequisites

- **Node.js 18+**: Check with `node --version`
- **npm 8+**: Check with `npm --version`
- **Basic TypeScript knowledge**: Understanding of types, interfaces, async/await
- **Command line familiarity**: Comfortable with terminal/bash commands

### OS-Specific Requirements

**Windows Users**:
```bash
# Install build tools for better-sqlite3
npm install --global windows-build-tools
```

**macOS/Linux**:
```bash
# Ensure you have build-essential (usually pre-installed)
# macOS: Install Xcode Command Line Tools if prompted
```

## Step 1: Initialize TypeScript Project

### 1.1 Create Project Directory

```bash
# Create and enter project directory
mkdir global-context-network
cd global-context-network

# Initialize git
git init
```

### 1.2 Initialize npm Project

```bash
# Create package.json
npm init -y
```

### 1.3 Install Dependencies

```bash
# TypeScript and build tools
npm install -D typescript@5.3.3 ts-node@10.9.2 @types/node@20.10.6

# Testing infrastructure
npm install -D vitest@1.1.0 @vitest/ui@1.1.0 @vitest/coverage-v8@1.1.0

# Database
npm install better-sqlite3@9.2.2
npm install -D @types/better-sqlite3@7.6.8

# Code quality
npm install -D eslint@8.56.0 @typescript-eslint/eslint-plugin@6.17.0 @typescript-eslint/parser@6.17.0
npm install -D prettier@3.1.1

# Utility libraries
npm install dotenv@16.3.1
```

**Expected output**:
```
added 234 packages, and audited 235 packages in 12s
```

### 1.4 Create TypeScript Configuration

Create `tsconfig.json`:

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "commonjs",
    "lib": ["ES2022"],
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "esModuleInterop": true,
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "noImplicitAny": true,
    "strictNullChecks": true,
    "strictFunctionTypes": true,
    "strictPropertyInitialization": true,
    "noImplicitThis": true,
    "alwaysStrict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noImplicitReturns": true,
    "noFallthroughCasesInSwitch": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "outDir": "./dist",
    "rootDir": "./src",
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist", "**/*.test.ts"]
}
```

### 1.5 Create ESLint Configuration

Create `.eslintrc.cjs`:

```javascript
module.exports = {
  parser: '@typescript-eslint/parser',
  extends: [
    'eslint:recommended',
    'plugin:@typescript-eslint/recommended',
  ],
  parserOptions: {
    ecmaVersion: 2022,
    sourceType: 'module',
    project: './tsconfig.json',
  },
  rules: {
    '@typescript-eslint/no-explicit-any': 'error',
    '@typescript-eslint/explicit-function-return-type': 'warn',
    '@typescript-eslint/no-unused-vars': ['error', { argsIgnorePattern: '^_' }],
  },
};
```

### 1.6 Create Prettier Configuration

Create `.prettierrc`:

```json
{
  "semi": true,
  "trailingComma": "es5",
  "singleQuote": true,
  "printWidth": 100,
  "tabWidth": 2
}
```

### 1.7 Update package.json Scripts

Add to `package.json`:

```json
{
  "scripts": {
    "typecheck": "tsc --noEmit",
    "build": "tsc",
    "lint": "eslint src --ext .ts",
    "lint:fix": "eslint src --ext .ts --fix",
    "format": "prettier --write \"src/**/*.ts\"",
    "format:check": "prettier --check \"src/**/*.ts\"",
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:once": "vitest run",
    "coverage": "vitest run --coverage",
    "db:migrate": "ts-node src/db/migrate.ts",
    "db:reset": "rm -f data/context.db && npm run db:migrate"
  }
}
```

### Verification Step 1

```bash
# Type check should pass (no files yet, that's OK)
npm run typecheck

# Lint should pass
npm run lint
```

**Expected output**:
```
✓ No TypeScript errors
✓ ESLint: no problems
```

## Step 2: Set Up Testing Infrastructure

### 2.1 Create Vitest Configuration

Create `vitest.config.ts`:

```typescript
import { defineConfig } from 'vitest/config';
import path from 'path';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    coverage: {
      provider: 'v8',
      reporter: ['text', 'json', 'html', 'lcov'],
      reportsDirectory: './coverage',
      include: ['src/**/*.ts'],
      exclude: [
        'src/**/*.test.ts',
        'src/**/*.spec.ts',
        'src/types/**',
        'src/db/migrate.ts',
      ],
      all: true,
      lines: 85,
      functions: 85,
      branches: 80,
      statements: 85,
    },
    include: ['src/**/*.test.ts', 'src/**/*.spec.ts'],
    exclude: ['node_modules', 'dist'],
  },
  resolve: {
    alias: {
      '@': path.resolve(__dirname, './src'),
    },
  },
});
```

### 2.2 Create Test Utilities Directory

```bash
mkdir -p src/test-utils
```

Create `src/test-utils/db.ts`:

```typescript
import Database from 'better-sqlite3';
import { join } from 'path';
import { tmpdir } from 'os';
import { randomUUID } from 'crypto';

/**
 * Creates an in-memory SQLite database for testing
 */
export function createTestDatabase(): Database.Database {
  return new Database(':memory:', { verbose: console.log });
}

/**
 * Creates a temporary file-based database for testing
 */
export function createTempDatabase(): Database.Database {
  const dbPath = join(tmpdir(), `test-${randomUUID()}.db`);
  return new Database(dbPath);
}

/**
 * Runs migrations on a test database
 */
export function migrateTestDatabase(db: Database.Database): void {
  // Run schema creation
  db.exec(`
    CREATE TABLE IF NOT EXISTS conversations (
      id TEXT PRIMARY KEY,
      started_at INTEGER NOT NULL,
      ended_at INTEGER,
      message_count INTEGER DEFAULT 0,
      sanitized BOOLEAN DEFAULT 0,
      created_at INTEGER DEFAULT (unixepoch())
    );

    CREATE TABLE IF NOT EXISTS messages (
      id TEXT PRIMARY KEY,
      conversation_id TEXT NOT NULL,
      role TEXT NOT NULL,
      content TEXT NOT NULL,
      timestamp INTEGER NOT NULL,
      created_at INTEGER DEFAULT (unixepoch()),
      FOREIGN KEY (conversation_id) REFERENCES conversations(id)
    );

    CREATE INDEX idx_messages_conversation ON messages(conversation_id);
    CREATE INDEX idx_conversations_started ON conversations(started_at);
  `);
}

/**
 * Cleans up test database
 */
export function cleanupTestDatabase(db: Database.Database): void {
  try {
    db.close();
  } catch (error) {
    console.warn('Failed to close database:', error);
  }
}
```

### 2.3 Create Example Test

Create `src/test-utils/db.test.ts`:

```typescript
import { describe, it, expect, afterEach } from 'vitest';
import { createTestDatabase, migrateTestDatabase, cleanupTestDatabase } from './db';

describe('Test Database Utilities', () => {
  let db: ReturnType<typeof createTestDatabase>;

  afterEach(() => {
    if (db) {
      cleanupTestDatabase(db);
    }
  });

  it('should create an in-memory database', () => {
    db = createTestDatabase();
    expect(db).toBeDefined();
    expect(db.memory).toBe(true);
  });

  it('should run migrations successfully', () => {
    db = createTestDatabase();
    migrateTestDatabase(db);

    // Verify tables exist
    const tables = db
      .prepare("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
      .all();

    const tableNames = tables.map((t: any) => t.name);
    expect(tableNames).toContain('conversations');
    expect(tableNames).toContain('messages');
  });

  it('should insert and retrieve a conversation', () => {
    db = createTestDatabase();
    migrateTestDatabase(db);

    const conversationId = 'test-123';
    const startedAt = Date.now();

    db.prepare(
      `INSERT INTO conversations (id, started_at) VALUES (?, ?)`
    ).run(conversationId, startedAt);

    const result = db.prepare('SELECT * FROM conversations WHERE id = ?').get(conversationId);

    expect(result).toBeDefined();
    expect((result as any).id).toBe(conversationId);
    expect((result as any).started_at).toBe(startedAt);
  });
});
```

### Verification Step 2

```bash
# Run tests
npm run test:once
```

**Expected output**:
```
✓ src/test-utils/db.test.ts (3)
  ✓ Test Database Utilities (3)
    ✓ should create an in-memory database
    ✓ should run migrations successfully
    ✓ should insert and retrieve a conversation

Test Files  1 passed (1)
     Tests  3 passed (3)
```

## Step 3: Set Up Database Infrastructure

### 3.1 Create Database Directory

```bash
mkdir -p src/db
mkdir -p data
```

### 3.2 Create Database Schema

Create `src/db/schema.sql`:

```sql
-- Conversations table (sanitized)
CREATE TABLE IF NOT EXISTS conversations (
  id TEXT PRIMARY KEY,
  started_at INTEGER NOT NULL,
  ended_at INTEGER,
  message_count INTEGER DEFAULT 0,
  sanitized BOOLEAN DEFAULT 0,
  created_at INTEGER DEFAULT (unixepoch())
);

-- Messages table (sanitized)
CREATE TABLE IF NOT EXISTS messages (
  id TEXT PRIMARY KEY,
  conversation_id TEXT NOT NULL,
  role TEXT NOT NULL CHECK(role IN ('user', 'assistant')),
  content TEXT NOT NULL,
  timestamp INTEGER NOT NULL,
  created_at INTEGER DEFAULT (unixepoch()),
  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- Learnings table
CREATE TABLE IF NOT EXISTS learnings (
  id TEXT PRIMARY KEY,
  conversation_id TEXT NOT NULL,
  category TEXT NOT NULL,
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  tags TEXT, -- JSON array
  confidence REAL NOT NULL CHECK(confidence >= 0 AND confidence <= 1),
  created_at INTEGER DEFAULT (unixepoch()),
  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- Job queue table
CREATE TABLE IF NOT EXISTS job_queue (
  id TEXT PRIMARY KEY,
  type TEXT NOT NULL,
  payload TEXT NOT NULL, -- JSON
  status TEXT NOT NULL CHECK(status IN ('pending', 'processing', 'completed', 'failed')),
  attempts INTEGER DEFAULT 0,
  max_attempts INTEGER DEFAULT 3,
  error TEXT,
  created_at INTEGER DEFAULT (unixepoch()),
  updated_at INTEGER DEFAULT (unixepoch())
);

-- Sanitization log (audit trail)
CREATE TABLE IF NOT EXISTS sanitization_log (
  id TEXT PRIMARY KEY,
  conversation_id TEXT NOT NULL,
  pii_type TEXT NOT NULL,
  redaction_count INTEGER NOT NULL,
  timestamp INTEGER NOT NULL,
  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- Indexes for performance
CREATE INDEX IF NOT EXISTS idx_messages_conversation ON messages(conversation_id);
CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp);
CREATE INDEX IF NOT EXISTS idx_conversations_started ON conversations(started_at);
CREATE INDEX IF NOT EXISTS idx_learnings_conversation ON learnings(conversation_id);
CREATE INDEX IF NOT EXISTS idx_learnings_category ON learnings(category);
CREATE INDEX IF NOT EXISTS idx_learnings_confidence ON learnings(confidence);
CREATE INDEX IF NOT EXISTS idx_job_queue_status ON job_queue(status);
CREATE INDEX IF NOT EXISTS idx_job_queue_created ON job_queue(created_at);
```

### 3.3 Create Migration Runner

Create `src/db/migrate.ts`:

```typescript
import Database from 'better-sqlite3';
import { readFileSync } from 'fs';
import { join } from 'path';

const DB_PATH = process.env.DB_PATH || join(__dirname, '../../data/context.db');

export function runMigrations(): Database.Database {
  console.log(`Running migrations on database: ${DB_PATH}`);

  const db = new Database(DB_PATH);
  db.pragma('journal_mode = WAL');
  db.pragma('foreign_keys = ON');

  // Read and execute schema
  const schemaPath = join(__dirname, 'schema.sql');
  const schema = readFileSync(schemaPath, 'utf-8');

  db.exec(schema);

  console.log('✓ Migrations complete');
  return db;
}

// Run if executed directly
if (require.main === module) {
  runMigrations();
}
```

### 3.4 Create Database Connection Module

Create `src/db/index.ts`:

```typescript
import Database from 'better-sqlite3';
import { join } from 'path';

const DB_PATH = process.env.DB_PATH || join(__dirname, '../../data/context.db');

let dbInstance: Database.Database | null = null;

/**
 * Get singleton database connection
 */
export function getDatabase(): Database.Database {
  if (!dbInstance) {
    try {
      dbInstance = new Database(DB_PATH);
      dbInstance.pragma('journal_mode = WAL');
      dbInstance.pragma('foreign_keys = ON');
      console.log(`✓ Database connected: ${DB_PATH}`);
    } catch (error) {
      console.error('Failed to connect to database:', error);
      throw error;
    }
  }
  return dbInstance;
}

/**
 * Close database connection
 */
export function closeDatabase(): void {
  if (dbInstance) {
    try {
      dbInstance.close();
      dbInstance = null;
      console.log('✓ Database connection closed');
    } catch (error) {
      console.warn('Failed to close database:', error);
    }
  }
}

/**
 * Execute database query with error handling
 */
export function executeQuery<T>(
  queryFn: (db: Database.Database) => T
): T {
  const db = getDatabase();
  try {
    return queryFn(db);
  } catch (error) {
    console.error('Database query failed:', error);
    throw error;
  }
}
```

### 3.5 Create Database Test

Create `src/db/index.test.ts`:

```typescript
import { describe, it, expect, beforeAll, afterAll } from 'vitest';
import { getDatabase, closeDatabase, executeQuery } from './index';
import { join } from 'path';
import { tmpdir } from 'os';
import { randomUUID } from 'crypto';

// Use temp database for tests
process.env.DB_PATH = join(tmpdir(), `test-${randomUUID()}.db`);

describe('Database Connection', () => {
  beforeAll(async () => {
    // Run migrations
    const { runMigrations } = await import('./migrate');
    runMigrations();
  });

  afterAll(() => {
    closeDatabase();
  });

  it('should connect to database', () => {
    const db = getDatabase();
    expect(db).toBeDefined();
  });

  it('should execute queries successfully', () => {
    const result = executeQuery((db) => {
      return db.prepare('SELECT 1 as result').get();
    });

    expect(result).toEqual({ result: 1 });
  });

  it('should insert and retrieve data', () => {
    const conversationId = 'test-' + randomUUID();

    executeQuery((db) => {
      db.prepare(
        'INSERT INTO conversations (id, started_at) VALUES (?, ?)'
      ).run(conversationId, Date.now());
    });

    const conversation = executeQuery((db) => {
      return db.prepare('SELECT * FROM conversations WHERE id = ?').get(conversationId);
    });

    expect(conversation).toBeDefined();
    expect((conversation as any).id).toBe(conversationId);
  });
});
```

### Verification Step 3

```bash
# Run migrations
npm run db:migrate

# Run database tests
npm run test:once -- src/db/index.test.ts
```

**Expected output**:
```
Running migrations on database: /path/to/data/context.db
✓ Migrations complete

✓ src/db/index.test.ts (3)
  ✓ Database Connection (3)
```

## Step 4: Create Environment Configuration

### 4.1 Create .env File

Create `.env`:

```bash
# Database
DB_PATH=./data/context.db

# Logging
LOG_LEVEL=info

# Anthropic API (for future use)
ANTHROPIC_API_KEY=your-api-key-here

# MCP Test Runner (for future use)
MCP_TEST_RUNNER_URL=http://localhost:3000
```

### 4.2 Create .env.example

Create `.env.example`:

```bash
# Database
DB_PATH=./data/context.db

# Logging
LOG_LEVEL=info

# Anthropic API
ANTHROPIC_API_KEY=sk-ant-xxxxx

# MCP Test Runner
MCP_TEST_RUNNER_URL=http://localhost:3000
```

### 4.3 Create .gitignore

Create `.gitignore`:

```
# Dependencies
node_modules/

# Build output
dist/
*.tsbuildinfo

# Environment
.env
.env.local

# Database
data/
*.db
*.db-shm
*.db-wal

# Testing
coverage/
.vitest/

# Logs
logs/
*.log

# OS
.DS_Store
Thumbs.db

# IDE
.vscode/
.idea/
*.swp
*.swo

# Repomix
*repomix*.txt
```

## Step 5: Create Project Structure

### 5.1 Create Directory Structure

```bash
mkdir -p src/{hooks,sanitization,queue,learning,mcp,types}
```

### 5.2 Create Types Directory

Create `src/types/index.ts`:

```typescript
/**
 * Core domain types
 */

export interface Conversation {
  id: string;
  started_at: number;
  ended_at?: number;
  message_count: number;
  sanitized: boolean;
  created_at: number;
}

export interface Message {
  id: string;
  conversation_id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: number;
  created_at: number;
}

export interface Learning {
  id: string;
  conversation_id: string;
  category: LearningCategory;
  title: string;
  content: string;
  tags: string[];
  confidence: number;
  created_at: number;
}

export type LearningCategory =
  | 'pattern'
  | 'best_practice'
  | 'anti_pattern'
  | 'bug_fix'
  | 'optimization'
  | 'tool_usage'
  | 'workflow'
  | 'decision';

export interface Job {
  id: string;
  type: JobType;
  payload: unknown;
  status: JobStatus;
  attempts: number;
  max_attempts: number;
  error?: string;
  created_at: number;
  updated_at: number;
}

export type JobType = 'sanitize_conversation' | 'extract_learning' | 'mine_upload';

export type JobStatus = 'pending' | 'processing' | 'completed' | 'failed';

export interface SanitizationLog {
  id: string;
  conversation_id: string;
  pii_type: PIIType;
  redaction_count: number;
  timestamp: number;
}

export type PIIType =
  | 'api_key'
  | 'file_path'
  | 'email'
  | 'ip_address'
  | 'name'
  | 'phone'
  | 'url_token';
```

### 5.3 Verify Project Structure

```bash
tree -L 2 src/
```

**Expected output**:
```
src/
├── db
│   ├── index.test.ts
│   ├── index.ts
│   ├── migrate.ts
│   └── schema.sql
├── hooks
├── learning
├── mcp
├── queue
├── sanitization
├── test-utils
│   ├── db.test.ts
│   └── db.ts
└── types
    └── index.ts
```

## Final Verification

### Run All Checks

```bash
# Type check
npm run typecheck

# Lint
npm run lint

# Format check
npm run format:check

# Run all tests
npm run test:once

# Generate coverage
npm run coverage

# Build
npm run build
```

**Expected output**:
```
✓ TypeScript: No errors
✓ ESLint: No problems
✓ Prettier: All files formatted
✓ Tests: 6 passed
✓ Coverage: 100% (minimal code so far)
✓ Build: Successful
```

### Verify Database

```bash
# Check database was created
ls -lh data/context.db

# Inspect schema
sqlite3 data/context.db ".schema"
```

**Expected output**:
```
-rw-r--r-- 1 user staff 28K Jan 16 10:00 data/context.db

CREATE TABLE conversations (...);
CREATE TABLE messages (...);
CREATE TABLE learnings (...);
...
```

## You're Done When...

- ✅ All npm scripts run successfully
- ✅ Type checking passes with strict mode
- ✅ All tests pass (6/6)
- ✅ Coverage report generates
- ✅ Database migrations run without errors
- ✅ Project builds to `dist/` directory
- ✅ `.env` file configured
- ✅ Directory structure matches specification

## Troubleshooting

### Issue: better-sqlite3 build fails

**Windows**:
```bash
npm install --global windows-build-tools
npm rebuild better-sqlite3
```

**macOS**:
```bash
xcode-select --install
npm rebuild better-sqlite3
```

**Linux**:
```bash
sudo apt-get install build-essential
npm rebuild better-sqlite3
```

### Issue: TypeScript errors on strict mode

Check `tsconfig.json` has:
```json
{
  "compilerOptions": {
    "strict": true,
    "noImplicitAny": true
  }
}
```

Add explicit types to all functions and variables.

### Issue: Vitest not finding tests

Ensure test files end with `.test.ts` or `.spec.ts` and are in `src/` directory.

### Issue: Database locked

Close any open connections:
```bash
npm run db:reset
```

If still locked, find and kill SQLite processes:
```bash
# macOS/Linux
lsof | grep context.db
kill -9 <PID>
```

## Next Steps

Now that your foundation is set up, you can proceed to:

1. [Claude Agent SDK Integration](./guide-claude-agent-sdk-integration-2025-01-16.md) - Set up the Agent SDK for subagent orchestration
2. [Using Subagents](./guide-using-subagents-2025-01-16.md) - Learn to delegate to specialized agents
3. [Phase 1 Hook Development](./guide-phase-1-hook-development-2025-01-16.md) - Implement event capture hooks

## Related Documents

- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Database Schema Reference](../reference/reference-database-schema-2025-01-16.md)
- [Testing Strategy](../reference/reference-testing-strategy-2025-01-16.md)
- [Implementation Roadmap](../plans/plan-implementation-roadmap-2025-01-16.md)
````

## File: guides/guide-phase-1-hook-development-2025-01-16.md
````markdown
# Phase 1 Hook Development Guide

> Implement Claude Code hooks for event capture with <100ms performance

---
title: Phase 1 Hook Development Guide
category: guide
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [guide, phase-1, hooks, claude-code, event-capture]
---

## Overview

This guide walks you through implementing Claude Code hooks to capture conversation events. You'll learn how to create fast, non-blocking hooks that sanitize and queue events without impacting user experience.

**Time to complete**: 120-150 minutes

## What You'll Build

- UserPromptSubmit hook (captures user input)
- Stop hook (captures agent responses)
- Event queue system (persistent, async)
- Sanitization at hook boundary
- Complete event capture pipeline

## Prerequisites

- **Phase 0 Setup Complete**: [guide-phase-0-foundation-setup-2025-01-16.md](./guide-phase-0-foundation-setup-2025-01-16.md)
- **TDD Workflow Understood**: [guide-tdd-workflow-2025-01-16.md](./guide-tdd-workflow-2025-01-16.md)
- **Subagents Guide Complete**: [guide-using-subagents-2025-01-16.md](./guide-using-subagents-2025-01-16.md)
- **Understanding of Claude Code hooks**: Basic knowledge of lifecycle

## Understanding Claude Code Hooks

### What Are Hooks?

Hooks are scripts that Claude Code invokes at specific lifecycle events:

| Hook | When It Fires | Purpose |
|------|---------------|---------|
| `UserPromptSubmit` | User sends message | Capture user input |
| `Stop` | Agent finishes response | Capture agent output |

### Hook Requirements

**Critical Performance Constraint**: < 100ms execution time

Why? Hooks run synchronously in the user's workflow. Slow hooks = bad UX.

**Strategy**:
- ✅ Fast, rule-based sanitization (< 10ms)
- ✅ Fire-and-forget queue write (< 50ms)
- ✅ Async processing for expensive operations
- ❌ Never do: API calls, heavy computation, blocking I/O

### Hook Lifecycle

```
User Types → UserPromptSubmit Hook → Queue Event → Return (< 100ms)
                                           ↓
                                    Async Processing
                                    (sanitization, extraction)

Agent Responds → Stop Hook → Complete Event → Return (< 100ms)
                                    ↓
                              Queue for Processing
```

## Step 1: Create Hook Directory Structure

### 1.1 Set Up Directories

```bash
mkdir -p .claude/hooks
```

### 1.2 Create hooks.json Configuration

Create `.claude/hooks/hooks.json`:

```json
{
  "hooks": [
    {
      "name": "UserPromptSubmit",
      "script": ".claude/hooks/user-prompt-submit.ts",
      "enabled": true
    },
    {
      "name": "Stop",
      "script": ".claude/hooks/stop.ts",
      "enabled": true
    }
  ]
}
```

## Step 2: Implement Fast Sanitization

### 2.1 Create Sanitization Module

**CRITICAL**: Sanitize BEFORE any storage, including queue.

Create `src/sanitization/fast-sanitize.ts`:

```typescript
/**
 * Fast, rule-based sanitization for hook-level redaction
 * Performance target: < 10ms per message
 */

export interface SanitizationResult {
  sanitized: string;
  redactions: Array<{ type: string; count: number }>;
  durationMs: number;
}

// Regex patterns for common PII
const PII_PATTERNS = {
  API_KEY: /\b(sk-[a-zA-Z0-9]{48}|api[_-]?key[_:\s]*[a-zA-Z0-9]{20,})/gi,
  EMAIL: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g,
  IP_V4: /\b(?:\d{1,3}\.){3}\d{1,3}\b/g,
  ABSOLUTE_PATH: /(?:\/Users\/[^\/\s]+|C:\\Users\\[^\\\s]+)(?:[\/\\][^\s]+)*/g,
  URL_TOKEN: /\b(https?:\/\/[^\s]+[?&](token|key|secret|auth)=[^\s&]+)/gi,
};

export function fastSanitize(text: string): SanitizationResult {
  const startTime = Date.now();
  let sanitized = text;
  const redactions: Array<{ type: string; count: number }> = [];

  // API Keys
  const apiKeyCount = (text.match(PII_PATTERNS.API_KEY) || []).length;
  if (apiKeyCount > 0) {
    sanitized = sanitized.replace(PII_PATTERNS.API_KEY, '[REDACTED_API_KEY]');
    redactions.push({ type: 'api_key', count: apiKeyCount });
  }

  // Emails
  const emailCount = (text.match(PII_PATTERNS.EMAIL) || []).length;
  if (emailCount > 0) {
    sanitized = sanitized.replace(PII_PATTERNS.EMAIL, '[REDACTED_EMAIL]');
    redactions.push({ type: 'email', count: emailCount });
  }

  // IP Addresses
  const ipCount = (text.match(PII_PATTERNS.IP_V4) || []).length;
  if (ipCount > 0) {
    sanitized = sanitized.replace(PII_PATTERNS.IP_V4, '[REDACTED_IP]');
    redactions.push({ type: 'ip_address', count: ipCount });
  }

  // Absolute Paths (with usernames)
  const pathCount = (text.match(PII_PATTERNS.ABSOLUTE_PATH) || []).length;
  if (pathCount > 0) {
    sanitized = sanitized.replace(PII_PATTERNS.ABSOLUTE_PATH, '[REDACTED_PATH]');
    redactions.push({ type: 'file_path', count: pathCount });
  }

  // URL Tokens
  const urlTokenCount = (text.match(PII_PATTERNS.URL_TOKEN) || []).length;
  if (urlTokenCount > 0) {
    sanitized = sanitized.replace(PII_PATTERNS.URL_TOKEN, '[REDACTED_URL]');
    redactions.push({ type: 'url_token', count: urlTokenCount });
  }

  const durationMs = Date.now() - startTime;

  return { sanitized, redactions, durationMs };
}
```

### 2.2 Test Fast Sanitization

Create `src/sanitization/fast-sanitize.test.ts`:

```typescript
import { describe, it, expect } from 'vitest';
import { fastSanitize } from './fast-sanitize';

describe('fastSanitize', () => {
  it('should redact OpenAI API keys', () => {
    const input = 'My key is sk-abc123def456ghi789jkl012mno345pqr678stu901';
    const result = fastSanitize(input);
    expect(result.sanitized).toBe('My key is [REDACTED_API_KEY]');
    expect(result.redactions).toContainEqual({ type: 'api_key', count: 1 });
  });

  it('should redact email addresses', () => {
    const input = 'Contact me at user@example.com';
    const result = fastSanitize(input);
    expect(result.sanitized).toBe('Contact me at [REDACTED_EMAIL]');
  });

  it('should redact absolute file paths', () => {
    const input = 'File at /Users/john/Documents/secret.txt';
    const result = fastSanitize(input);
    expect(result.sanitized).toBe('File at [REDACTED_PATH]');
  });

  it('should execute in < 10ms', () => {
    const input = 'No PII here just normal text';
    const result = fastSanitize(input);
    expect(result.durationMs).toBeLessThan(10);
  });

  it('should handle empty strings', () => {
    const result = fastSanitize('');
    expect(result.sanitized).toBe('');
    expect(result.redactions).toHaveLength(0);
  });
});
```

Run tests:

```bash
npm run test:once -- sanitize
```

## Step 3: Implement Event Queue

### 3.1 Create Queue Module

Create `src/queue/event-queue.ts`:

```typescript
import Database from 'better-sqlite3';
import { randomUUID } from 'crypto';

export interface QueuedEvent {
  id: string;
  conversation_id: string;
  role: 'user' | 'assistant';
  content: string; // Already sanitized
  timestamp: number;
  status: 'pending' | 'processing' | 'completed' | 'failed';
  created_at: number;
}

export class EventQueue {
  private db: Database.Database;

  constructor(dbPath: string) {
    this.db = new Database(dbPath);
    this.db.pragma('journal_mode = WAL');
    this.initQueue();
  }

  private initQueue(): void {
    this.db.exec(`
      CREATE TABLE IF NOT EXISTS event_queue (
        id TEXT PRIMARY KEY,
        conversation_id TEXT NOT NULL,
        role TEXT NOT NULL CHECK(role IN ('user', 'assistant')),
        content TEXT NOT NULL,
        timestamp INTEGER NOT NULL,
        status TEXT NOT NULL DEFAULT 'pending',
        created_at INTEGER DEFAULT (unixepoch())
      );

      CREATE INDEX IF NOT EXISTS idx_queue_status ON event_queue(status);
      CREATE INDEX IF NOT EXISTS idx_queue_conversation ON event_queue(conversation_id);
    `);
  }

  /**
   * Enqueue event (must be fast - < 50ms)
   */
  enqueue(event: Omit<QueuedEvent, 'id' | 'status' | 'created_at'>): string {
    const id = randomUUID();

    this.db
      .prepare(
        `INSERT INTO event_queue (id, conversation_id, role, content, timestamp, status)
         VALUES (?, ?, ?, ?, ?, 'pending')`
      )
      .run(id, event.conversation_id, event.role, event.content, event.timestamp);

    return id;
  }

  /**
   * Get pending events for processing
   */
  getPending(limit = 100): QueuedEvent[] {
    return this.db
      .prepare(
        `SELECT * FROM event_queue WHERE status = 'pending' ORDER BY created_at LIMIT ?`
      )
      .all(limit) as QueuedEvent[];
  }

  /**
   * Update event status
   */
  updateStatus(id: string, status: QueuedEvent['status']): void {
    this.db.prepare(`UPDATE event_queue SET status = ? WHERE id = ?`).run(status, id);
  }

  close(): void {
    this.db.close();
  }
}
```

### 3.2 Test Event Queue

Create `src/queue/event-queue.test.ts`:

```typescript
import { describe, it, expect, beforeEach, afterEach } from 'vitest';
import { EventQueue } from './event-queue';
import { join } from 'path';
import { tmpdir } from 'os';
import { randomUUID } from 'crypto';
import { unlinkSync, existsSync } from 'fs';

describe('EventQueue', () => {
  let queue: EventQueue;
  let dbPath: string;

  beforeEach(() => {
    dbPath = join(tmpdir(), `test-queue-${randomUUID()}.db`);
    queue = new EventQueue(dbPath);
  });

  afterEach(() => {
    queue.close();
    if (existsSync(dbPath)) unlinkSync(dbPath);
  });

  it('should enqueue events quickly', () => {
    const start = Date.now();

    const eventId = queue.enqueue({
      conversation_id: 'conv-123',
      role: 'user',
      content: 'Test message',
      timestamp: Date.now(),
    });

    const duration = Date.now() - start;

    expect(eventId).toBeTruthy();
    expect(duration).toBeLessThan(50); // < 50ms requirement
  });

  it('should retrieve pending events', () => {
    queue.enqueue({
      conversation_id: 'conv-123',
      role: 'user',
      content: 'Message 1',
      timestamp: Date.now(),
    });

    const pending = queue.getPending();
    expect(pending).toHaveLength(1);
    expect(pending[0].status).toBe('pending');
  });

  it('should update event status', () => {
    const id = queue.enqueue({
      conversation_id: 'conv-123',
      role: 'user',
      content: 'Test',
      timestamp: Date.now(),
    });

    queue.updateStatus(id, 'completed');

    const pending = queue.getPending();
    expect(pending).toHaveLength(0); // No longer pending
  });
});
```

## Step 4: Implement UserPromptSubmit Hook

### 4.1 Create Hook Script

Create `.claude/hooks/user-prompt-submit.ts`:

```typescript
#!/usr/bin/env ts-node

import { fastSanitize } from '../../src/sanitization/fast-sanitize';
import { EventQueue } from '../../src/queue/event-queue';
import { randomUUID } from 'crypto';
import { join } from 'path';
import { appendFileSync } from 'fs';

const DB_PATH = process.env.DB_PATH || join(__dirname, '../../data/context.db');
const LOG_PATH = join(__dirname, '../../logs/hooks.log');

/**
 * UserPromptSubmit Hook
 * Captures user input, sanitizes it, and queues for processing
 * CRITICAL: Must execute in < 100ms
 */

interface HookInput {
  prompt: string;
  conversation_id?: string;
}

function log(message: string): void {
  const timestamp = new Date().toISOString();
  try {
    appendFileSync(LOG_PATH, `[${timestamp}] UserPromptSubmit: ${message}\n`);
  } catch {
    // Fail silently - never break user flow
  }
}

function main(): void {
  const startTime = Date.now();

  try {
    // Parse input from stdin
    const input = process.argv[2] || '{}';
    const data: HookInput = JSON.parse(input);

    const prompt = data.prompt || '';
    const conversationId = data.conversation_id || randomUUID();

    // SANITIZE BEFORE STORAGE (privacy guarantee)
    const sanitizationResult = fastSanitize(prompt);

    // Queue sanitized event (fast, non-blocking)
    const queue = new EventQueue(DB_PATH);
    queue.enqueue({
      conversation_id: conversationId,
      role: 'user',
      content: sanitizationResult.sanitized,
      timestamp: Date.now(),
    });
    queue.close();

    const duration = Date.now() - startTime;

    log(
      `Captured user prompt. Redactions: ${sanitizationResult.redactions.length}. Duration: ${duration}ms`
    );

    // CRITICAL: Verify < 100ms
    if (duration > 100) {
      log(`⚠ WARNING: Hook took ${duration}ms (> 100ms threshold)`);
    }
  } catch (error) {
    // FAIL SILENTLY - never throw to caller
    const errorMsg = error instanceof Error ? error.message : String(error);
    log(`Error: ${errorMsg}`);
  }
}

main();
```

Make executable:

```bash
chmod +x .claude/hooks/user-prompt-submit.ts
```

## Step 5: Implement Stop Hook

### 5.1 Create Stop Hook Script

Create `.claude/hooks/stop.ts`:

```typescript
#!/usr/bin/env ts-node

import { fastSanitize } from '../../src/sanitization/fast-sanitize';
import { EventQueue } from '../../src/queue/event-queue';
import { join } from 'path';
import { appendFileSync } from 'fs';

const DB_PATH = process.env.DB_PATH || join(__dirname, '../../data/context.db');
const LOG_PATH = join(__dirname, '../../logs/hooks.log');

interface HookInput {
  response: string;
  conversation_id: string;
}

function log(message: string): void {
  const timestamp = new Date().toISOString();
  try {
    appendFileSync(LOG_PATH, `[${timestamp}] Stop: ${message}\n`);
  } catch {
    // Fail silently
  }
}

function main(): void {
  const startTime = Date.now();

  try {
    const input = process.argv[2] || '{}';
    const data: HookInput = JSON.parse(input);

    const response = data.response || '';
    const conversationId = data.conversation_id;

    // Sanitize response
    const sanitizationResult = fastSanitize(response);

    // Queue sanitized event
    const queue = new EventQueue(DB_PATH);
    queue.enqueue({
      conversation_id: conversationId,
      role: 'assistant',
      content: sanitizationResult.sanitized,
      timestamp: Date.now(),
    });
    queue.close();

    const duration = Date.now() - startTime;

    log(
      `Captured assistant response. Redactions: ${sanitizationResult.redactions.length}. Duration: ${duration}ms`
    );

    if (duration > 100) {
      log(`⚠ WARNING: Hook took ${duration}ms (> 100ms threshold)`);
    }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error);
    log(`Error: ${errorMsg}`);
  }
}

main();
```

Make executable:

```bash
chmod +x .claude/hooks/stop.ts
```

## Step 6: Manual Testing

### 6.1 Create Test Script

Create `test-hooks.ts`:

```typescript
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);

async function testHooks() {
  console.log('Testing UserPromptSubmit hook...\n');

  const userPromptPayload = JSON.stringify({
    prompt: 'My API key is sk-abc123def456ghi789jkl012mno345pqr678stu901',
    conversation_id: 'test-conv-123',
  });

  const { stdout: userOutput } = await execAsync(
    `./.claude/hooks/user-prompt-submit.ts '${userPromptPayload}'`
  );

  console.log('UserPromptSubmit output:', userOutput || '(no output - good!)');

  console.log('\nTesting Stop hook...\n');

  const stopPayload = JSON.stringify({
    response: 'Sure! Contact me at admin@example.com for more info.',
    conversation_id: 'test-conv-123',
  });

  const { stdout: stopOutput } = await execAsync(
    `./.claude/hooks/stop.ts '${stopPayload}'`
  );

  console.log('Stop output:', stopOutput || '(no output - good!)');

  console.log('\nChecking queued events...\n');

  const { EventQueue } = await import('./src/queue/event-queue');
  const queue = new EventQueue('./data/context.db');

  const pending = queue.getPending();
  console.log(`Pending events: ${pending.length}`);

  pending.forEach((event) => {
    console.log(`  - ${event.role}: ${event.content.substring(0, 50)}...`);
  });

  queue.close();

  console.log('\n✓ Hook test complete');
}

testHooks().catch(console.error);
```

Run test:

```bash
ts-node test-hooks.ts
```

**Expected output**:
```
Testing UserPromptSubmit hook...
UserPromptSubmit output: (no output - good!)

Testing Stop hook...
Stop output: (no output - good!)

Checking queued events...
Pending events: 2
  - user: My API key is [REDACTED_API_KEY]
  - assistant: Sure! Contact me at [REDACTED_EMAIL] for more...

✓ Hook test complete
```

### 6.2 Check Logs

```bash
tail logs/hooks.log
```

**Expected output**:
```
[2025-01-16T10:30:00.000Z] UserPromptSubmit: Captured user prompt. Redactions: 1. Duration: 23ms
[2025-01-16T10:30:00.100Z] Stop: Captured assistant response. Redactions: 1. Duration: 19ms
```

## Verification

### Performance Check

All hook executions should be < 100ms:

```bash
grep "Duration:" logs/hooks.log | awk -F'Duration: ' '{print $2}' | sort -n
```

All values should be < 100ms.

### Privacy Check

No raw PII should be in database:

```bash
sqlite3 data/context.db "SELECT content FROM event_queue LIMIT 10"
```

Should see `[REDACTED_*]` placeholders, not actual PII.

## You're Done When...

- ✅ UserPromptSubmit hook captures user input
- ✅ Stop hook captures agent responses
- ✅ All PII sanitized before storage
- ✅ Hook execution < 100ms
- ✅ Events queued in database
- ✅ Logs show successful captures
- ✅ Manual test passes

## Troubleshooting

### Hook not firing

Check hooks.json is valid and scripts are executable:

```bash
cat .claude/hooks/hooks.json
ls -l .claude/hooks/*.ts
```

### Hook too slow (> 100ms)

Profile with timing:

```typescript
console.time('sanitize');
const result = fastSanitize(text);
console.timeEnd('sanitize');
```

Optimize regex patterns or reduce text length.

### Queue errors

Check database exists and is writable:

```bash
ls -lh data/context.db
sqlite3 data/context.db "SELECT COUNT(*) FROM event_queue"
```

## Next Steps

Hooks are now capturing events! Next:

1. **Phase 2**: [Sanitization Pipeline](../plans/plan-phase-2-sanitization-2025-01-16.md) - Full AI-powered sanitization
2. **Phase 3**: [Database & Storage](../plans/plan-phase-3-database-storage-2025-01-16.md) - Persistent storage
3. **Phase 4**: [Async Processing](../plans/plan-phase-4-async-processing-2025-01-16.md) - Background workers

## Related Documents

- [Hooks & Event Capture Architecture](../architecture/architecture-hooks-event-capture-2025-01-16.md)
- [Sanitization Pipeline Architecture](../architecture/architecture-sanitization-pipeline-2025-01-16.md)
- [TDD Workflow Guide](./guide-tdd-workflow-2025-01-16.md)
````

## File: guides/guide-tdd-workflow-2025-01-16.md
````markdown
# TDD Workflow Guide

> Master the Red-Green-Refactor cycle with AI-powered subagents

---
title: TDD Workflow with Subagents Guide
category: guide
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [guide, tdd, testing, red-green-refactor, workflow]
---

## Overview

This guide teaches you Test-Driven Development (TDD) enhanced with AI subagents. You'll learn the Red-Green-Refactor cycle where subagents generate tests, implement code, and validate quality automatically.

**Time to complete**: 60-90 minutes

## What You'll Learn

- The Red-Green-Refactor cycle with subagents
- How to write failing tests first (RED)
- Implementing minimal code to pass (GREEN)
- Refactoring with confidence (REFACTOR)
- Complete TDD workflow end-to-end
- Avoiding common TDD pitfalls

## Prerequisites

- **Testing Harness Complete**: [guide-testing-harness-usage-2025-01-16.md](./guide-testing-harness-usage-2025-01-16.md)
- **Subagents Guide Complete**: [guide-using-subagents-2025-01-16.md](./guide-using-subagents-2025-01-16.md)
- **Phase 0 Setup Complete**: [guide-phase-0-foundation-setup-2025-01-16.md](./guide-phase-0-foundation-setup-2025-01-16.md)

## TDD Philosophy

### Why Test First?

**Traditional Development** (Code-First):
```
Implement → Test → Debug → Fix → Repeat
```
Result: Tests miss edge cases, low coverage, bugs slip through

**Test-Driven Development** (Test-First):
```
Test → Implement → Validate → Refactor
```
Result: Comprehensive tests, high coverage, confidence in changes

### Benefits with Subagents

1. **Comprehensive Tests**: AI generates edge cases you might miss
2. **Quality Validation**: Automated quality gates ensure standards
3. **Faster Iteration**: Parallel generation and validation
4. **Consistent Patterns**: Subagents follow best practices
5. **Built-in Documentation**: Tests serve as specifications

## The Red-Green-Refactor Cycle

```
🔴 RED   → Write failing test
🟢 GREEN → Minimal code to pass
🔵 REFACTOR → Improve while keeping tests green
✅ VALIDATE → Quality gates pass
```

## Step 1: RED Phase - Write Failing Test

### 1.1 Delegate to Test Generator

**Goal**: Generate a comprehensive, failing test

Create `src/workflows/red-phase.ts`:

```typescript
import { invokeSubagent } from '../agents/orchestration/invoke';
import { testGeneratorSubagents } from '../agents/subagents/test-generators';

export async function redPhase(featureDescription: string): Promise<string> {
  console.log('🔴 RED PHASE: Generating failing test\n');

  const testCode = await invokeSubagent(
    'unit-test-generator',
    `Generate comprehensive failing test for:

${featureDescription}

Requirements:
- Test happy path
- Test edge cases: null, undefined, empty, very large inputs
- Test error conditions
- Use clear, descriptive names
- Follow arrange-act-assert pattern
- Aim for >85% coverage when implemented

The function doesn't exist yet - expect compilation errors.`,
    { agents: testGeneratorSubagents }
  );

  return testCode;
}
```

### 1.2 Validate Test Quality

```typescript
import { validateTestQuality } from '../agents/workflows/validate-tests';

export async function validateRedPhase(testFilePath: string): Promise<void> {
  console.log('📊 Validating test quality...\n');

  const validation = await validateTestQuality(testFilePath);

  if (validation.score < 0.8) {
    throw new Error(
      `Test quality insufficient: ${validation.score}\nIssues: ${validation.issues.join(', ')}`
    );
  }

  console.log(`✓ Test quality score: ${validation.score}`);
  if (validation.issues.length > 0) {
    console.log(`  Suggestions: ${validation.issues.join(', ')}`);
  }
}
```

### 1.3 Run Test (Confirm Failure)

```typescript
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);

export async function confirmTestFails(testPattern: string): Promise<void> {
  console.log('▶ Running test (expecting failure)...\n');

  try {
    await execAsync(`npm run test:once -- ${testPattern}`);
    throw new Error('⚠ WARNING: Test passed before implementation!');
  } catch (error: any) {
    if (error.message.includes('WARNING')) {
      throw error;
    }
    console.log('✓ Test failed as expected (good!)');
    console.log('  Error:', error.message.split('\n')[0]);
  }
}
```

### 1.4 Complete RED Phase Example

```typescript
export async function completeRedPhase(
  featureDescription: string,
  testFilePath: string
): Promise<string> {
  // 1. Generate test
  const testCode = await redPhase(featureDescription);

  // 2. Write test file
  writeFileSync(testFilePath, testCode);
  console.log(`✓ Test written to: ${testFilePath}\n`);

  // 3. Validate quality
  await validateRedPhase(testFilePath);

  // 4. Confirm it fails
  await confirmTestFails(testFilePath);

  console.log('\n🔴 RED PHASE COMPLETE\n');
  return testCode;
}
```

**Verification Checklist**:
- ✅ Test generated by subagent
- ✅ Test quality score ≥ 0.8
- ✅ Test fails for the right reason (function doesn't exist)
- ✅ Test is comprehensive (happy path + edge cases)

## Step 2: GREEN Phase - Minimal Implementation

### 2.1 Delegate to Implementation Agent

**Goal**: Write minimal code to make tests pass

Create `src/workflows/green-phase.ts`:

```typescript
import { invokeSubagent } from '../agents/orchestration/invoke';
import { implementationSubagents } from '../agents/subagents/implementation';

export async function greenPhase(
  testCode: string,
  featureDescription: string
): Promise<string> {
  console.log('🟢 GREEN PHASE: Implementing minimal code\n');

  const implementationCode = await invokeSubagent(
    'code-writer', // or specific agent like 'sanitization-developer'
    `Implement MINIMAL code to pass these tests:

Feature: ${featureDescription}

Tests:
${testCode}

Requirements:
- Write simplest possible implementation
- Don't over-engineer
- Handle all test cases
- Use strict TypeScript types
- Add JSDoc comments
- Focus on making tests pass, not perfection`,
    { agents: implementationSubagents }
  );

  return implementationCode;
}
```

### 2.2 Run Tests (Confirm Pass)

```typescript
export async function confirmTestsPass(testPattern: string): Promise<void> {
  console.log('▶ Running tests (expecting success)...\n');

  try {
    const { stdout } = await execAsync(`npm run test:once -- ${testPattern}`);
    console.log('✓ All tests passing!');

    // Extract test count
    const match = stdout.match(/(\d+) passed/);
    if (match) {
      console.log(`  Passed: ${match[1]} tests`);
    }
  } catch (error: any) {
    console.error('✗ Tests still failing:');
    console.error(error.message);
    throw new Error('GREEN phase failed: tests not passing');
  }
}
```

### 2.3 Validate Implementation

```typescript
import { invokeSubagent } from '../agents/orchestration/invoke';
import { validatorSubagents } from '../agents/subagents/validators';

export async function validateImplementation(
  testCode: string,
  implCode: string
): Promise<void> {
  console.log('📊 Validating implementation quality...\n');

  const validationResult = await invokeSubagent(
    'code-quality-validator',
    `Validate this implementation:

Tests:
${testCode}

Implementation:
${implCode}

Check for:
- All tests pass
- Handles all edge cases
- No security issues
- Proper error handling
- Type safety
- Code quality

Return JSON: { "score": 0.9, "passed": true, "issues": [] }`,
    { agents: validatorSubagents }
  );

  const validation = JSON.parse(validationResult);

  if (!validation.passed || validation.score < 0.8) {
    throw new Error(
      `Implementation quality insufficient: ${validation.score}\nIssues: ${validation.issues.join(', ')}`
    );
  }

  console.log(`✓ Implementation quality score: ${validation.score}`);
}
```

### 2.4 Complete GREEN Phase Example

```typescript
export async function completeGreenPhase(
  testCode: string,
  featureDescription: string,
  implFilePath: string,
  testPattern: string
): Promise<string> {
  // 1. Generate implementation
  const implCode = await greenPhase(testCode, featureDescription);

  // 2. Write implementation file
  writeFileSync(implFilePath, implCode);
  console.log(`✓ Implementation written to: ${implFilePath}\n`);

  // 3. Run tests
  await confirmTestsPass(testPattern);

  // 4. Validate implementation
  await validateImplementation(testCode, implCode);

  console.log('\n🟢 GREEN PHASE COMPLETE\n');
  return implCode;
}
```

**Verification Checklist**:
- ✅ Minimal implementation (not over-engineered)
- ✅ All tests pass
- ✅ Implementation quality score ≥ 0.8
- ✅ No security issues
- ✅ Proper type safety

## Step 3: REFACTOR Phase - Improve Quality

### 3.1 Identify Improvement Opportunities

**Goal**: Clean up code while keeping tests green

Create `src/workflows/refactor-phase.ts`:

```typescript
import { invokeSubagent } from '../agents/orchestration/invoke';
import { implementationSubagents } from '../agents/subagents/implementation';

export async function identifyRefactorings(implCode: string): Promise<string[]> {
  console.log('🔵 REFACTOR PHASE: Analyzing code...\n');

  const analysis = await invokeSubagent(
    'code-quality-validator',
    `Analyze this code for refactoring opportunities:

${implCode}

Look for:
- Code duplication
- Long functions (>20 lines)
- Complex conditionals
- Magic numbers
- Unclear variable names
- Missing abstractions

Return JSON array of specific refactorings: ["Extract regex to constant", ...]`,
    { agents: implementationSubagents }
  );

  return JSON.parse(analysis);
}
```

### 3.2 Apply Refactorings

```typescript
export async function applyRefactoring(
  implCode: string,
  refactoring: string
): Promise<string> {
  console.log(`  Applying: ${refactoring}`);

  const refactoredCode = await invokeSubagent(
    'code-writer',
    `Refactor this code:

Current Code:
${implCode}

Refactoring: ${refactoring}

Requirements:
- Apply refactoring only
- Don't change functionality
- Keep code working
- Improve readability/maintainability`,
    { agents: implementationSubagents }
  );

  return refactoredCode;
}
```

### 3.3 Test After Each Refactoring

```typescript
export async function refactorWithTesting(
  implCode: string,
  testPattern: string
): Promise<string> {
  const refactorings = await identifyRefactorings(implCode);

  if (refactorings.length === 0) {
    console.log('✓ No refactorings needed\n');
    return implCode;
  }

  console.log(`Found ${refactorings.length} refactoring opportunities\n`);

  let currentCode = implCode;

  for (const refactoring of refactorings) {
    // Apply refactoring
    currentCode = await applyRefactoring(currentCode, refactoring);

    // Run tests immediately
    try {
      await confirmTestsPass(testPattern);
      console.log(`  ✓ Tests still passing\n`);
    } catch (error) {
      console.error(`  ✗ Tests failed after refactoring!`);
      throw new Error(`Refactoring broke tests: ${refactoring}`);
    }
  }

  return currentCode;
}
```

### 3.4 Complete REFACTOR Phase Example

```typescript
export async function completeRefactorPhase(
  implCode: string,
  implFilePath: string,
  testPattern: string
): Promise<string> {
  console.log('🔵 REFACTOR PHASE: Improving code quality\n');

  // 1. Apply refactorings
  const refactoredCode = await refactorWithTesting(implCode, testPattern);

  // 2. Write refactored code
  writeFileSync(implFilePath, refactoredCode);

  // 3. Final validation
  await validateImplementation('', refactoredCode);

  console.log('\n🔵 REFACTOR PHASE COMPLETE\n');
  return refactoredCode;
}
```

**Verification Checklist**:
- ✅ Code quality improved
- ✅ Tests still passing after each change
- ✅ No functionality changed
- ✅ Code more maintainable
- ✅ Quality score increased

## Step 4: Complete TDD Cycle

### 4.1 Full Workflow

Create `src/workflows/complete-tdd-cycle.ts`:

```typescript
import { completeRedPhase } from './red-phase';
import { completeGreenPhase } from './green-phase';
import { completeRefactorPhase } from './refactor-phase';
import { getCoverageViaMCP } from '../agents/workflows/run-tests';

export async function completeTDDCycle(
  featureDescription: string,
  testFilePath: string,
  implFilePath: string
): Promise<void> {
  console.log('═══ TDD CYCLE START ═══\n');
  console.log(`Feature: ${featureDescription}\n`);

  const testPattern = testFilePath.replace(/^src\//, '').replace(/\.test\.ts$/, '');

  try {
    // RED: Write failing test
    const testCode = await completeRedPhase(featureDescription, testFilePath);

    // GREEN: Implement minimal code
    const implCode = await completeGreenPhase(
      testCode,
      featureDescription,
      implFilePath,
      testPattern
    );

    // REFACTOR: Improve code quality
    await completeRefactorPhase(implCode, implFilePath, testPattern);

    // COVERAGE: Verify coverage
    console.log('📈 Checking coverage...\n');
    const coverage = await getCoverageViaMCP();
    console.log('Coverage:', coverage);

    if (coverage.lines < 85) {
      console.warn(`⚠ Warning: Coverage ${coverage.lines}% < 85%`);
    } else {
      console.log('✓ Coverage excellent\n');
    }

    console.log('═══ TDD CYCLE COMPLETE ═══');
    console.log('✅ Feature implemented with tests');
  } catch (error) {
    console.error('\n✗ TDD cycle failed:', error);
    throw error;
  }
}
```

### 4.2 Example Usage

Create `src/workflows/examples/sanitize-api-keys-tdd.ts`:

```typescript
import { config } from 'dotenv';
import { completeTDDCycle } from '../complete-tdd-cycle';

config();

async function main() {
  await completeTDDCycle(
    'Function sanitizeApiKeys(text: string): string that redacts all API keys and tokens in text',
    'src/sanitization/sanitize.test.ts',
    'src/sanitization/sanitize.ts'
  );
}

main().catch(console.error);
```

Run it:

```bash
ts-node src/workflows/examples/sanitize-api-keys-tdd.ts
```

**Expected output**:
```
═══ TDD CYCLE START ═══

Feature: Function sanitizeApiKeys(text: string): string...

🔴 RED PHASE: Generating failing test
  → Invoking: unit-test-generator
  ✓ Completed: unit-test-generator
✓ Test written to: src/sanitization/sanitize.test.ts

📊 Validating test quality...
✓ Test quality score: 0.92

▶ Running test (expecting failure)...
✓ Test failed as expected (good!)

🔴 RED PHASE COMPLETE

🟢 GREEN PHASE: Implementing minimal code
  → Invoking: code-writer
  ✓ Completed: code-writer
✓ Implementation written to: src/sanitization/sanitize.ts

▶ Running tests (expecting success)...
✓ All tests passing!
  Passed: 8 tests

📊 Validating implementation quality...
✓ Implementation quality score: 0.88

🟢 GREEN PHASE COMPLETE

🔵 REFACTOR PHASE: Improving code quality
  Applying: Extract regex patterns to constants
  ✓ Tests still passing
  Applying: Extract validation to helper function
  ✓ Tests still passing

🔵 REFACTOR PHASE COMPLETE

📈 Checking coverage...
Coverage: { lines: 94.2, statements: 92.1, functions: 100, branches: 87.5 }
✓ Coverage excellent

═══ TDD CYCLE COMPLETE ═══
✅ Feature implemented with tests
```

## Step 5: Commit Discipline

### 5.1 Commit After Each Phase

```typescript
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);

export async function commitPhase(
  phase: 'red' | 'green' | 'refactor',
  message: string
): Promise<void> {
  await execAsync('git add .');
  await execAsync(`git commit -m "${phase.toUpperCase()}: ${message}"`);
  console.log(`✓ Committed: ${phase} phase\n`);
}
```

Usage:

```typescript
// After RED
await commitPhase('red', 'Add failing tests for sanitizeApiKeys');

// After GREEN
await commitPhase('green', 'Implement sanitizeApiKeys to pass tests');

// After REFACTOR
await commitPhase('refactor', 'Extract regex patterns to constants');
```

## Common Pitfalls and Solutions

### Pitfall 1: Skipping Test Generation

❌ **Wrong**: "Tests take too long, I'll just implement"

✅ **Right**: "Tests first, always. Use subagents to generate fast"

### Pitfall 2: Over-Implementation in GREEN

❌ **Wrong**: Implementing perfect, production-ready code in GREEN phase

✅ **Right**: Simplest possible code to pass tests. Improve in REFACTOR.

### Pitfall 3: Changing Tests After GREEN

❌ **Wrong**: Tests fail, so change tests to match implementation

✅ **Right**: Tests define requirements. Fix implementation, not tests.

### Pitfall 4: Skipping REFACTOR

❌ **Wrong**: "It works, ship it"

✅ **Right**: Clean code is as important as working code. Always refactor.

### Pitfall 5: Large Test Suites at Once

❌ **Wrong**: Write 50 tests, then implement everything

✅ **Right**: One test at a time, or small batches. Keep cycle fast.

## Verification

You're doing TDD correctly when:

- ✅ You always write tests first
- ✅ Tests fail before implementation
- ✅ You write minimal code to pass
- ✅ Tests pass after implementation
- ✅ You refactor with confidence
- ✅ Coverage is >85%
- ✅ All quality gates pass

## Next Steps

Now apply TDD to real features:

1. [Phase 1 Hook Development](./guide-phase-1-hook-development-2025-01-16.md) - Apply TDD to hooks
2. [Phase 2 Sanitization](../plans/plan-phase-2-sanitization-2025-01-16.md) - TDD for sanitization
3. Continue TDD for all features

## Related Documents

- [Testing Harness Architecture](../architecture/architecture-testing-harness-2025-01-16.md)
- [Subagent System Architecture](../architecture/architecture-subagent-system-2025-01-16.md)
- [Testing Strategy](../reference/reference-testing-strategy-2025-01-16.md)
````

## File: guides/guide-testing-harness-usage-2025-01-16.md
````markdown
# Testing Harness Usage Guide

> Learn how to use the Claude-powered testing harness for AI-driven test generation and validation

---
title: Testing Harness Usage Guide
category: guide
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [guide, testing, vitest, mcp, test-generation, validation]
---

## Overview

This guide teaches you how to use the Claude-powered testing harness - a system where AI subagents generate, validate, and run tests automatically. The harness ensures comprehensive test coverage and quality through AI-powered validation.

**Time to complete**: 90-120 minutes

## What You'll Learn

- Set up the testing MCP server
- Generate tests with test-generator subagents
- Validate test quality automatically
- Run tests and interpret results
- Use coverage validation
- Integrate testing into your workflow

## Prerequisites

- **Phase 0 Setup Complete**: [guide-phase-0-foundation-setup-2025-01-16.md](./guide-phase-0-foundation-setup-2025-01-16.md)
- **Subagents Guide Complete**: [guide-using-subagents-2025-01-16.md](./guide-using-subagents-2025-01-16.md)
- **Vitest Installed**: Should be done in Phase 0
- **Understanding of testing**: Basic test concepts

## Testing Philosophy

**Test-Driven Development with AI Validation**

Traditional TDD:
1. Write test manually
2. Run test (should fail)
3. Implement code
4. Run test (should pass)

AI-Enhanced TDD:
1. **AI generates comprehensive test** (validator checks quality)
2. Run test (should fail)
3. **AI implements minimal code** (validator checks implementation)
4. Run test (should pass)
5. **AI validates quality** (coverage, security, performance)

## Step 1: Set Up MCP Test Runner Server

### 1.1 Create MCP Server Directory

```bash
mkdir -p src/mcp/test-runner
```

### 1.2 Install MCP SDK

```bash
npm install @modelcontextprotocol/sdk
```

### 1.3 Create Test Runner Server

Create `src/mcp/test-runner/server.ts`:

```typescript
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} from '@modelcontextprotocol/sdk/types.js';
import { exec } from 'child_process';
import { promisify } from 'util';
import { readFileSync, existsSync } from 'fs';
import { join } from 'path';

const execAsync = promisify(exec);

class TestRunnerServer {
  private server: Server;

  constructor() {
    this.server = new Server(
      {
        name: 'test-runner',
        version: '1.0.0',
      },
      {
        capabilities: {
          tools: {},
        },
      }
    );

    this.setupToolHandlers();

    this.server.onerror = (error) => console.error('[MCP Error]', error);
    process.on('SIGINT', async () => {
      await this.server.close();
      process.exit(0);
    });
  }

  private setupToolHandlers() {
    this.server.setRequestHandler(ListToolsRequestSchema, async () => ({
      tools: [
        {
          name: 'run_unit_tests',
          description: 'Run Vitest unit tests',
          inputSchema: {
            type: 'object',
            properties: {
              testPattern: {
                type: 'string',
                description: 'Optional test file pattern (e.g., "sanitize")',
              },
            },
          },
        },
        {
          name: 'run_integration_tests',
          description: 'Run integration tests',
          inputSchema: {
            type: 'object',
            properties: {
              testPattern: {
                type: 'string',
                description: 'Optional test file pattern',
              },
            },
          },
        },
        {
          name: 'run_e2e_tests',
          description: 'Run end-to-end tests',
          inputSchema: {
            type: 'object',
            properties: {
              testPattern: {
                type: 'string',
                description: 'Optional test file pattern',
              },
            },
          },
        },
        {
          name: 'get_coverage_report',
          description: 'Get test coverage statistics',
          inputSchema: {
            type: 'object',
            properties: {},
          },
        },
        {
          name: 'validate_test_quality',
          description: 'Validates test code quality and returns score',
          inputSchema: {
            type: 'object',
            properties: {
              testFilePath: {
                type: 'string',
                description: 'Path to test file to validate',
              },
            },
            required: ['testFilePath'],
          },
        },
      ],
    }));

    this.server.setRequestHandler(CallToolRequestSchema, async (request) => {
      switch (request.params.name) {
        case 'run_unit_tests':
          return this.runUnitTests(request.params.arguments?.testPattern as string);

        case 'run_integration_tests':
          return this.runIntegrationTests(request.params.arguments?.testPattern as string);

        case 'run_e2e_tests':
          return this.runE2ETests(request.params.arguments?.testPattern as string);

        case 'get_coverage_report':
          return this.getCoverageReport();

        case 'validate_test_quality':
          return this.validateTestQuality(
            request.params.arguments?.testFilePath as string
          );

        default:
          throw new Error(`Unknown tool: ${request.params.name}`);
      }
    });
  }

  private async runUnitTests(testPattern?: string) {
    const pattern = testPattern ? ` -- ${testPattern}` : '';
    const cmd = `npm run test:once${pattern}`;

    try {
      const { stdout, stderr } = await execAsync(cmd);
      return {
        content: [
          {
            type: 'text',
            text: JSON.stringify({
              success: true,
              stdout,
              stderr,
            }),
          },
        ],
      };
    } catch (error: any) {
      return {
        content: [
          {
            type: 'text',
            text: JSON.stringify({
              success: false,
              error: error.message,
              stdout: error.stdout,
              stderr: error.stderr,
            }),
          },
        ],
      };
    }
  }

  private async runIntegrationTests(testPattern?: string) {
    const pattern = testPattern || 'integration';
    return this.runUnitTests(pattern);
  }

  private async runE2ETests(testPattern?: string) {
    const pattern = testPattern || 'e2e';
    return this.runUnitTests(pattern);
  }

  private async getCoverageReport() {
    const coveragePath = join(process.cwd(), 'coverage', 'coverage-summary.json');

    if (!existsSync(coveragePath)) {
      // Run coverage first
      try {
        await execAsync('npm run coverage');
      } catch (error) {
        return {
          content: [
            {
              type: 'text',
              text: JSON.stringify({
                success: false,
                error: 'Coverage generation failed',
              }),
            },
          ],
        };
      }
    }

    try {
      const coverage = JSON.parse(readFileSync(coveragePath, 'utf-8'));
      const total = coverage.total;

      return {
        content: [
          {
            type: 'text',
            text: JSON.stringify({
              success: true,
              coverage: {
                lines: total.lines.pct,
                statements: total.statements.pct,
                functions: total.functions.pct,
                branches: total.branches.pct,
              },
            }),
          },
        ],
      };
    } catch (error: any) {
      return {
        content: [
          {
            type: 'text',
            text: JSON.stringify({
              success: false,
              error: error.message,
            }),
          },
        ],
      };
    }
  }

  private async validateTestQuality(testFilePath: string) {
    if (!existsSync(testFilePath)) {
      return {
        content: [
          {
            type: 'text',
            text: JSON.stringify({
              score: 0,
              issues: [`Test file not found: ${testFilePath}`],
            }),
          },
        ],
      };
    }

    const testContent = readFileSync(testFilePath, 'utf-8');

    // Simple static analysis
    let score = 1.0;
    const issues: string[] = [];

    // Check for describe blocks
    if (!testContent.includes('describe(')) {
      score -= 0.2;
      issues.push('Missing describe() blocks');
    }

    // Check for it/test blocks
    const testCount = (testContent.match(/it\(|test\(/g) || []).length;
    if (testCount < 3) {
      score -= 0.3;
      issues.push('Insufficient test cases (< 3)');
    }

    // Check for expect statements
    const expectCount = (testContent.match(/expect\(/g) || []).length;
    if (expectCount < testCount) {
      score -= 0.2;
      issues.push('Some tests missing assertions');
    }

    // Check for edge case keywords
    const hasEdgeCases =
      testContent.includes('edge') ||
      testContent.includes('boundary') ||
      testContent.includes('null') ||
      testContent.includes('undefined') ||
      testContent.includes('empty');

    if (!hasEdgeCases) {
      score -= 0.2;
      issues.push('No obvious edge case testing');
    }

    return {
      content: [
        {
          type: 'text',
          text: JSON.stringify({
            score: Math.max(0, score),
            issues,
            strengths:
              score >= 0.8 ? ['Good test structure', 'Adequate coverage'] : [],
          }),
        },
      ],
    };
  }

  async run() {
    const transport = new StdioServerTransport();
    await this.server.connect(transport);
    console.error('Test Runner MCP server running on stdio');
  }
}

const server = new TestRunnerServer();
server.run().catch(console.error);
```

### 1.4 Add Server Script

Add to `package.json`:

```json
{
  "scripts": {
    "mcp:test-runner": "node dist/mcp/test-runner/server.js"
  }
}
```

### 1.5 Build and Run

```bash
# Build
npm run build

# Run MCP server (in separate terminal)
npm run mcp:test-runner
```

**Expected output**:
```
Test Runner MCP server running on stdio
```

## Step 2: Generate Tests with Subagents

### 2.1 Create Test Generator Workflow

Create `src/agents/workflows/generate-tests.ts`:

```typescript
import { invokeSubagent } from '../orchestration/invoke';
import { testGeneratorSubagents } from '../subagents/test-generators';

export async function generateUnitTests(
  functionDescription: string,
  filePath?: string
): Promise<string> {
  const prompt = `Generate comprehensive unit tests for:
  
Description: ${functionDescription}
${filePath ? `File: ${filePath}` : ''}

Requirements:
- Use Vitest (describe, it, expect)
- Test happy path
- Test edge cases (null, undefined, empty, very large)
- Test error conditions
- Use arrange-act-assert pattern
- Clear, descriptive test names
- Aim for >85% coverage`;

  return invokeSubagent('unit-test-generator', prompt, {
    agents: testGeneratorSubagents,
  });
}
```

### 2.2 Example Usage

```typescript
import { generateUnitTests } from './workflows/generate-tests';

const tests = await generateUnitTests(
  'Function sanitizeApiKeys(text: string): string that redacts API keys',
  'src/sanitization/sanitize.ts'
);

console.log(tests);
```

**Expected output**:
```typescript
import { describe, it, expect } from 'vitest';
import { sanitizeApiKeys } from './sanitize';

describe('sanitizeApiKeys', () => {
  it('should redact OpenAI API keys', () => {
    const input = 'My key is sk-abc123...';
    const result = sanitizeApiKeys(input);
    expect(result).toBe('My key is [REDACTED_API_KEY]');
  });

  it('should handle empty strings', () => {
    expect(sanitizeApiKeys('')).toBe('');
  });

  it('should handle null/undefined', () => {
    expect(() => sanitizeApiKeys(null as any)).toThrow();
  });

  // ... more tests
});
```

## Step 3: Validate Test Quality

### 3.1 Use MCP Validation Tool

Create `src/agents/workflows/validate-tests.ts`:

```typescript
import { invokeSubagent } from '../orchestration/invoke';
import { validatorSubagents } from '../subagents/validators';
import { mcpServers } from '../../mcp/config';

export async function validateTestQuality(
  testFilePath: string
): Promise<{
  score: number;
  passed: boolean;
  issues: string[];
}> {
  const prompt = `Validate test quality for: ${testFilePath}

Use the validate_test_quality tool to get automated quality score.
Then review manually for:
- Test structure
- Coverage completeness
- Clear naming
- Proper assertions

Return JSON with:
- score (0-1)
- passed (true if score >= 0.8)
- issues (array of strings)`;

  const response = await invokeSubagent('test-quality-validator', prompt, {
    agents: validatorSubagents,
    mcpServers,
  });

  return JSON.parse(response);
}
```

### 3.2 Quality Gate

```typescript
export async function testQualityGate(testFilePath: string): Promise<void> {
  const validation = await validateTestQuality(testFilePath);

  if (!validation.passed) {
    throw new Error(
      `Test quality gate failed (score: ${validation.score})\nIssues:\n${validation.issues.join('\n')}`
    );
  }

  console.log(`✓ Test quality gate passed (score: ${validation.score})`);
}
```

## Step 4: Run Tests and Get Coverage

### 4.1 Run Tests via MCP

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';
import { mcpServers } from '../mcp/config';

export async function runTestsViaMCP(
  testPattern?: string
): Promise<{ success: boolean; output: string }> {
  const prompt = `Run unit tests${testPattern ? ` matching: ${testPattern}` : ''}

Use the run_unit_tests tool.
Return the results.`;

  const response = query({
    prompt,
    options: {
      model: 'claude-sonnet-4-5',
      agents: {},
      mcpServers,
      tools: ['mcp__test-runner__run_unit_tests'],
      apiKey: process.env.ANTHROPIC_API_KEY,
    },
  });

  let result = '';
  for await (const message of response) {
    if (message.type === 'text') result += message.text;
  }

  const data = JSON.parse(result);
  return data;
}
```

### 4.2 Get Coverage Report

```typescript
export async function getCoverageViaMCP(): Promise<{
  lines: number;
  statements: number;
  functions: number;
  branches: number;
}> {
  const prompt = 'Get test coverage report using get_coverage_report tool';

  const response = query({
    prompt,
    options: {
      model: 'claude-sonnet-4-5',
      agents: {},
      mcpServers,
      tools: ['mcp__test-runner__get_coverage_report'],
      apiKey: process.env.ANTHROPIC_API_KEY,
    },
  });

  let result = '';
  for await (const message of response) {
    if (message.type === 'text') result += message.text;
  }

  const data = JSON.parse(result);
  return data.coverage;
}
```

## Step 5: Complete Testing Workflow

### 5.1 Full TDD Cycle with Testing Harness

Create `src/agents/workflows/tdd-with-harness.ts`:

```typescript
import { generateUnitTests } from './generate-tests';
import { validateTestQuality } from './validate-tests';
import { runTestsViaMCP, getCoverageViaMCP } from './run-tests';
import { invokeSubagent } from '../orchestration/invoke';
import { implementationSubagents } from '../subagents/implementation';

export async function tddCycleWithHarness(
  featureDescription: string
): Promise<void> {
  console.log('=== TDD Cycle with Testing Harness ===\n');

  // 1. RED: Generate test
  console.log('🔴 RED: Generating tests...');
  const testCode = await generateUnitTests(featureDescription);
  console.log('✓ Tests generated\n');

  // 2. Validate test quality
  console.log('📊 Validating test quality...');
  const validation = await validateTestQuality('./generated-test.ts');
  if (!validation.passed) {
    throw new Error(`Test quality insufficient: ${validation.score}`);
  }
  console.log(`✓ Test quality: ${validation.score}\n`);

  // 3. Run tests (should fail)
  console.log('▶ Running tests (expecting failure)...');
  const initialRun = await runTestsViaMCP();
  if (initialRun.success) {
    console.warn('⚠ Warning: Tests passed before implementation!');
  } else {
    console.log('✓ Tests failed as expected\n');
  }

  // 4. GREEN: Implement
  console.log('🟢 GREEN: Implementing code...');
  const implementation = await invokeSubagent(
    'code-writer',
    `Implement minimal code to pass these tests:\n\n${testCode}`,
    { agents: implementationSubagents }
  );
  console.log('✓ Implementation complete\n');

  // 5. Run tests (should pass)
  console.log('▶ Running tests (expecting success)...');
  const finalRun = await runTestsViaMCP();
  if (!finalRun.success) {
    throw new Error('Tests still failing after implementation');
  }
  console.log('✓ Tests passing\n');

  // 6. Coverage check
  console.log('📈 Checking coverage...');
  const coverage = await getCoverageViaMCP();
  console.log('Coverage:', coverage);

  if (coverage.lines < 85) {
    console.warn(`⚠ Warning: Line coverage ${coverage.lines}% < 85%`);
  } else {
    console.log('✓ Coverage acceptable\n');
  }

  console.log('=== TDD Cycle Complete ===');
}
```

## Verification

Create `src/agents/test-harness.ts`:

```typescript
import { config } from 'dotenv';
import { tddCycleWithHarness } from './workflows/tdd-with-harness';

config();

async function main() {
  await tddCycleWithHarness(
    'Function sanitizeApiKeys(text: string) that redacts API keys'
  );
}

main().catch(console.error);
```

```bash
# Start MCP server in one terminal
npm run mcp:test-runner

# Run harness in another terminal
ts-node src/agents/test-harness.ts
```

## You're Done When...

- ✅ MCP test-runner server running
- ✅ Can generate tests via subagents
- ✅ Test quality validation working
- ✅ Can run tests via MCP
- ✅ Coverage reports accessible
- ✅ Complete TDD cycle functioning

## Troubleshooting

### MCP Server Not Running

```bash
# Check if process is running
ps aux | grep test-runner

# Restart server
npm run mcp:test-runner
```

### Tool Not Found

Ensure MCP server exposes the tool and client references it correctly:
- Server: `tools: [{ name: 'run_unit_tests', ... }]`
- Client: `tools: ['mcp__test-runner__run_unit_tests']`

### Coverage Not Generated

```bash
# Run coverage manually first
npm run coverage

# Check coverage directory exists
ls coverage/
```

## Best Practices

1. **Always validate test quality** before implementation
2. **Use MCP tools for consistency** across subagents
3. **Set coverage thresholds** (85% minimum)
4. **Run tests frequently** during development
5. **Monitor MCP server health** for reliable testing
6. **Cost control**: Limit AI validation to critical tests

## Next Steps

- [TDD Workflow Guide](./guide-tdd-workflow-2025-01-16.md) - Deep dive into TDD patterns
- [Phase 1 Hook Development](./guide-phase-1-hook-development-2025-01-16.md) - Apply testing to hooks

## Related Documents

- [Testing Harness Architecture](../architecture/architecture-testing-harness-2025-01-16.md)
- [Testing Strategy Reference](../reference/reference-testing-strategy-2025-01-16.md)
````

## File: guides/guide-using-subagents-2025-01-16.md
````markdown
# Using Subagents Guide

> Learn how to delegate tasks to specialized Claude subagents for focused expertise

---
title: Using Subagents Guide
category: guide
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [guide, subagents, delegation, orchestration, patterns]
---

## Overview

This guide teaches you how to effectively use subagents in the Global Context Network project. Subagents are specialized Claude instances with focused prompts and tools, designed to handle specific tasks better than a general-purpose agent.

**Time to complete**: 60-90 minutes

## What You'll Learn

- When to delegate vs implement directly
- How to define effective subagent configurations
- Invoking subagents via Claude Agent SDK
- Monitoring subagent progress
- Handling subagent responses
- Parallel and sequential delegation patterns

## Prerequisites

- **Phase 0 Setup Complete**: [guide-phase-0-foundation-setup-2025-01-16.md](./guide-phase-0-foundation-setup-2025-01-16.md)
- **Claude Agent SDK Integrated**: [guide-claude-agent-sdk-integration-2025-01-16.md](./guide-claude-agent-sdk-integration-2025-01-16.md)
- **Understanding of TypeScript**: async/await, Promises, types

## Core Principle

**Never implement directly - always delegate to specialized subagents.**

Why? Subagents provide:
- **Focused expertise**: Each knows its domain deeply
- **Consistent quality**: Specialized prompts ensure standards
- **Parallel execution**: Independent tasks run simultaneously
- **Built-in validation**: Quality checks at every step

## When to Use Subagents

### Delegate to Subagents When:

✅ **Implementing features**: Use implementation subagents
✅ **Writing tests**: Use test generator subagents
✅ **Validating code**: Use quality validator subagents
✅ **Complex workflows**: Use orchestrator patterns
✅ **Multiple independent tasks**: Use parallel execution

### Implement Directly When:

❌ **Trivial changes**: Simple one-line fixes
❌ **Configuration files**: package.json, tsconfig.json
❌ **Already tested patterns**: Proven, simple code
❌ **Time-sensitive debugging**: Immediate fixes needed

## Step 1: Understanding Subagent Anatomy

### 1.1 Subagent Definition Structure

```typescript
import { AgentDefinition } from '@anthropic-ai/claude-agent-sdk';

const exampleAgent: AgentDefinition = {
  // What this agent does (for orchestrator selection)
  description: 'Implements TypeScript functions with strict types',

  // Specialized instructions (the agent's expertise)
  prompt: `You are an expert TypeScript developer.
  Your role: Implement functions following these rules:
  1. Use strict TypeScript types (never 'any')
  2. Add comprehensive JSDoc comments
  3. Handle edge cases explicitly
  4. Follow functional programming when possible
  5. Write defensive code with validation

  Quality bar: Production-ready, type-safe, well-documented code.`,

  // Available capabilities
  tools: ['Read', 'Write', 'Edit', 'Bash'],

  // Which model to use
  model: 'claude-sonnet-4-5',
};
```

### 1.2 Effective Prompt Design

**Bad Prompt** (too vague):
```typescript
prompt: "Write good code"
```

**Good Prompt** (specific, actionable):
```typescript
prompt: `You are a sanitization expert.
When implementing PII redaction:
1. Use regex for fast detection (API keys, emails, IPs)
2. Preserve code structure while masking values
3. Log redaction count for audit trail
4. Never miss potential PII - err on side of caution
5. Test with real-world examples

Performance: < 10ms per conversation
Accuracy: < 1% false negatives`
```

### 1.3 Tool Selection

Match tools to subagent needs:

| Subagent Type | Typical Tools |
|---------------|---------------|
| Implementation | Read, Write, Edit, Bash |
| Test Generator | Read, Write, Edit |
| Validator | Read, Grep, mcp__test-runner__* |
| Quality Checker | Read, Grep, Bash |
| Documentation | Read, Write, Edit |

## Step 2: Creating Specialized Subagents

### 2.1 Implementation Subagent

Create `src/agents/subagents/implementation.ts`:

```typescript
import { AgentDefinition } from '@anthropic-ai/claude-agent-sdk';

/**
 * Hook implementation specialist
 */
export const hookDeveloperAgent: AgentDefinition = {
  description: 'Implements Claude Code hooks with <100ms execution time',
  prompt: `You are a Claude Code hooks expert.

  When implementing hooks:
  1. NEVER block user interaction (< 100ms execution)
  2. Fail silently with structured logging
  3. Use fire-and-forget for I/O operations
  4. Validate input before processing
  5. Queue events asynchronously

  Event Schema:
  - id: unique identifier
  - conversation_id: thread identifier
  - role: 'user' | 'assistant'
  - content: sanitized message content
  - timestamp: Unix timestamp

  Error Handling:
  - Try-catch all operations
  - Log errors with context
  - Never throw to caller (hooks must not fail)

  Performance Budget: < 100ms total, < 50ms for queueing.`,
  tools: ['Read', 'Write', 'Edit', 'Bash'],
  model: 'claude-sonnet-4-5',
};

/**
 * Sanitization pipeline specialist
 */
export const sanitizationAgent: AgentDefinition = {
  description: 'Implements PII detection and redaction with hybrid approach',
  prompt: `You are a PII sanitization expert.

  Implement hybrid sanitization:

  Phase 1 - Rule-Based (Fast):
  - Regex for API keys: /sk-[a-zA-Z0-9]{48}/
  - Email: RFC 5322 compliant regex
  - IP addresses: IPv4/IPv6 patterns
  - File paths: absolute paths with usernames
  - URLs with tokens: query params containing 'token', 'key', 'secret'

  Phase 2 - AI-Powered (Accurate):
  - Context-aware name detection (distinguish from variables)
  - Company-specific terminology
  - Phone numbers (international formats)
  - Addresses

  Validation:
  - Combine rule-based + AI results
  - Log all redactions for audit
  - Performance: < 2s per conversation
  - Accuracy: < 1% false negatives, < 5% false positives

  Replacement Strategy:
  - API keys → [REDACTED_API_KEY]
  - Emails → [REDACTED_EMAIL]
  - Paths → [REDACTED_PATH]
  - Names → [REDACTED_NAME]`,
  tools: ['Read', 'Write', 'Edit'],
  model: 'claude-sonnet-4-5',
};

export const implementationSubagents = {
  'hook-developer': hookDeveloperAgent,
  'sanitization-developer': sanitizationAgent,
};
```

### 2.2 Test Generator Subagent

Create `src/agents/subagents/test-generators.ts`:

```typescript
import { AgentDefinition } from '@anthropic-ai/claude-agent-sdk';

/**
 * Unit test generator
 */
export const unitTestGenerator: AgentDefinition = {
  description: 'Generates comprehensive unit tests with Vitest',
  prompt: `You are a testing expert specializing in Vitest.

  When generating unit tests:

  Structure:
  - describe() for component/function
  - it() for individual behaviors
  - expect() for assertions (be specific, not just truthy)

  Coverage Requirements:
  - Happy path
  - Edge cases (empty, null, undefined, very large)
  - Error conditions
  - Boundary values

  Patterns:
  - Arrange-Act-Assert structure
  - One logical assertion per test
  - Clear, descriptive test names
  - Mock external dependencies
  - Use beforeEach for setup, afterEach for cleanup

  Quality:
  - Target: > 85% coverage
  - No flaky tests (deterministic)
  - Fast execution (< 100ms per test)
  - Independent tests (no shared state)

  Example:
  describe('sanitizeApiKeys', () => {
    it('should redact OpenAI API keys', () => {
      const input = 'My key is sk-abc123';
      const result = sanitizeApiKeys(input);
      expect(result).toBe('My key is [REDACTED_API_KEY]');
    });
  });`,
  tools: ['Read', 'Write', 'Edit'],
  model: 'claude-sonnet-4-5',
};

/**
 * Integration test generator
 */
export const integrationTestGenerator: AgentDefinition = {
  description: 'Generates integration tests for component interactions',
  prompt: `You are an integration testing expert.

  When generating integration tests:

  Scope:
  - Test real component interactions
  - Use actual database (test instance)
  - Test file I/O with temp files
  - Verify async behavior

  Patterns:
  - Set up realistic test data
  - Test complete workflows (hook → queue → sanitize → db)
  - Verify side effects (database writes, file creation)
  - Test error propagation between components
  - Clean up resources in afterEach

  Example:
  describe('Event Capture Flow', () => {
    it('should capture user prompt and queue for sanitization', async () => {
      const db = createTestDatabase();
      const queue = new EventQueue(db);

      await captureUserPrompt({ content: 'test' });

      const jobs = queue.getPendingJobs();
      expect(jobs).toHaveLength(1);
      expect(jobs[0].type).toBe('sanitize_conversation');
    });
  });`,
  tools: ['Read', 'Write', 'Edit', 'Bash'],
  model: 'claude-sonnet-4-5',
};

export const testGeneratorSubagents = {
  'unit-test-generator': unitTestGenerator,
  'integration-test-generator': integrationTestGenerator,
};
```

### 2.3 Validator Subagent

Create `src/agents/subagents/validators.ts`:

```typescript
import { AgentDefinition } from '@anthropic-ai/claude-agent-sdk';

/**
 * Test quality validator
 */
export const testQualityValidator: AgentDefinition = {
  description: 'Validates test code quality and completeness',
  prompt: `You are a test quality auditor.

  Review tests for:

  Structure (0.3 weight):
  - Proper describe/it nesting
  - Clear, descriptive names
  - Logical grouping

  Coverage (0.4 weight):
  - Happy path tested
  - Edge cases covered
  - Error conditions tested
  - Boundary values checked

  Quality (0.3 weight):
  - Specific assertions (not just truthy)
  - No test interdependencies
  - Proper mocking
  - Arrange-Act-Assert pattern

  Scoring:
  - 1.0: Excellent, comprehensive
  - 0.8-0.9: Good, minor gaps
  - 0.6-0.7: Acceptable, some issues
  - < 0.6: Fail, major problems

  Return JSON:
  {
    "score": 0.92,
    "strengths": ["Excellent edge case coverage"],
    "weaknesses": ["Missing error condition test"],
    "required_fixes": []
  }`,
  tools: ['Read', 'Grep', 'mcp__test-runner__validate_test_quality'],
  model: 'claude-sonnet-4-5',
};

/**
 * Code quality validator
 */
export const codeQualityValidator: AgentDefinition = {
  description: 'Reviews code for quality, security, and standards compliance',
  prompt: `You are a code quality auditor.

  Review code for:

  TypeScript Standards:
  - Strict mode compliance
  - No 'any' types
  - Proper type annotations
  - Type safety in edge cases

  Code Quality:
  - Clear naming
  - DRY (no duplication)
  - SOLID principles
  - Appropriate abstractions
  - Error handling

  Security:
  - SQL injection vulnerabilities
  - Command injection risks
  - Path traversal
  - Hardcoded secrets

  Performance:
  - O(n) complexity acceptable
  - No unnecessary iterations
  - Efficient database queries

  Fail if:
  - Security vulnerabilities found
  - Uses 'any' type
  - Missing error handling
  - Performance issues

  Return JSON with pass/fail and specific issues.`,
  tools: ['Read', 'Grep', 'Bash'],
  model: 'claude-sonnet-4-5',
};

export const validatorSubagents = {
  'test-quality-validator': testQualityValidator,
  'code-quality-validator': codeQualityValidator,
};
```

## Step 3: Invoking Subagents

### 3.1 Single Subagent Invocation

Create `src/agents/orchestration/invoke.ts`:

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';

export interface SubagentConfig {
  agents: Record<string, any>;
  mcpServers?: Record<string, any>;
}

/**
 * Invokes a single subagent and returns response
 */
export async function invokeSubagent(
  agentName: string,
  prompt: string,
  config: SubagentConfig
): Promise<string> {
  console.log(`→ Invoking: ${agentName}`);

  const response = query({
    prompt,
    options: {
      model: 'claude-sonnet-4-5',
      agents: config.agents,
      mcpServers: config.mcpServers,
      apiKey: process.env.ANTHROPIC_API_KEY,
    },
  });

  let fullResponse = '';

  for await (const message of response) {
    if (message.type === 'system' && message.subtype === 'subagent_start') {
      console.log(`  ▸ Started: ${message.agent_name}`);
    }

    if (message.type === 'text') {
      fullResponse += message.text;
    }

    if (message.type === 'system' && message.subtype === 'subagent_end') {
      console.log(`  ✓ Completed: ${message.agent_name}`);
    }
  }

  return fullResponse;
}
```

**Example usage**:

```typescript
import { invokeSubagent } from './orchestration/invoke';
import { implementationSubagents } from './subagents/implementation';

const hookCode = await invokeSubagent(
  'hook-developer',
  'Implement UserPromptSubmit hook that queues events in < 100ms',
  { agents: implementationSubagents }
);
```

### 3.2 Parallel Execution

```typescript
/**
 * Runs multiple independent subagents in parallel
 */
export async function runParallel(
  tasks: Array<{ agent: string; prompt: string }>,
  config: SubagentConfig
): Promise<string[]> {
  console.log(`Running ${tasks.length} subagents in parallel`);

  const promises = tasks.map((task) =>
    invokeSubagent(task.agent, task.prompt, config)
  );

  return Promise.all(promises);
}
```

**Example usage** (implement hooks + tests simultaneously):

```typescript
const [hookCode, hookTests] = await runParallel(
  [
    {
      agent: 'hook-developer',
      prompt: 'Implement UserPromptSubmit hook',
    },
    {
      agent: 'unit-test-generator',
      prompt: 'Generate tests for UserPromptSubmit hook',
    },
  ],
  {
    agents: { ...implementationSubagents, ...testGeneratorSubagents },
  }
);
```

### 3.3 Sequential with Dependencies

```typescript
/**
 * Runs subagents in sequence, passing results forward
 */
export async function runSequential(
  steps: Array<{
    agent: string;
    promptTemplate: (results: string[]) => string;
  }>,
  config: SubagentConfig
): Promise<string[]> {
  const results: string[] = [];

  for (const step of steps) {
    const prompt = step.promptTemplate(results);
    const result = await invokeSubagent(step.agent, prompt, config);
    results.push(result);
  }

  return results;
}
```

**Example usage** (TDD Red-Green-Refactor):

```typescript
const [test, implementation, validation] = await runSequential(
  [
    {
      agent: 'unit-test-generator',
      promptTemplate: () => 'Generate test for sanitizeApiKeys(text)',
    },
    {
      agent: 'sanitization-developer',
      promptTemplate: (results) =>
        `Implement minimal code to pass:\n\n${results[0]}`,
    },
    {
      agent: 'code-quality-validator',
      promptTemplate: (results) =>
        `Validate:\n\nTest: ${results[0]}\n\nCode: ${results[1]}`,
    },
  ],
  {
    agents: {
      ...testGeneratorSubagents,
      ...implementationSubagents,
      ...validatorSubagents,
    },
  }
);
```

## Step 4: Real-World Patterns

### 4.1 Implement Feature with Quality Gates

Create `src/agents/workflows/implement-feature.ts`:

```typescript
import { runSequential } from '../orchestration/invoke';
import {
  implementationSubagents,
  testGeneratorSubagents,
  validatorSubagents,
} from '../subagents';

export async function implementFeature(
  featureDescription: string
): Promise<{
  test: string;
  implementation: string;
  qualityScore: number;
}> {
  console.log('=== Implementing Feature with Quality Gates ===');

  const [test, implementation, qualityReport] = await runSequential(
    [
      // RED: Generate test
      {
        agent: 'unit-test-generator',
        promptTemplate: () => `Generate comprehensive tests for: ${featureDescription}`,
      },

      // GREEN: Implement
      {
        agent: 'sanitization-developer', // Use appropriate implementation agent
        promptTemplate: (results) =>
          `Implement minimal code to pass these tests:\n\n${results[0]}`,
      },

      // VALIDATE: Quality gate
      {
        agent: 'code-quality-validator',
        promptTemplate: (results) =>
          `Validate implementation:\n\nTests: ${results[0]}\n\nCode: ${results[1]}`,
      },
    ],
    {
      agents: {
        ...testGeneratorSubagents,
        ...implementationSubagents,
        ...validatorSubagents,
      },
    }
  );

  // Parse quality score from report
  const qualityData = JSON.parse(qualityReport);

  if (qualityData.score < 0.8) {
    throw new Error(
      `Quality gate failed: ${qualityData.score}\nIssues: ${qualityData.issues.join(', ')}`
    );
  }

  return {
    test,
    implementation,
    qualityScore: qualityData.score,
  };
}
```

### 4.2 Parallel Validation

```typescript
/**
 * Runs multiple validators in parallel
 */
export async function validateQuality(
  testCode: string,
  implCode: string
): Promise<{
  testQuality: number;
  codeQuality: number;
  passed: boolean;
}> {
  const [testReport, codeReport] = await runParallel(
    [
      {
        agent: 'test-quality-validator',
        prompt: `Validate test quality:\n\n${testCode}`,
      },
      {
        agent: 'code-quality-validator',
        prompt: `Validate code quality:\n\n${implCode}`,
      },
    ],
    { agents: validatorSubagents }
  );

  const testQuality = JSON.parse(testReport).score;
  const codeQuality = JSON.parse(codeReport).score;

  return {
    testQuality,
    codeQuality,
    passed: testQuality >= 0.8 && codeQuality >= 0.8,
  };
}
```

## Step 5: Monitoring and Debugging

### 5.1 Subagent Progress Logging

```typescript
export async function invokeWithLogging(
  agentName: string,
  prompt: string,
  config: SubagentConfig
): Promise<string> {
  const startTime = Date.now();

  console.log(`[${new Date().toISOString()}] Starting: ${agentName}`);
  console.log(`Prompt length: ${prompt.length} chars`);

  const response = await invokeSubagent(agentName, prompt, config);

  const duration = Date.now() - startTime;
  console.log(`[${new Date().toISOString()}] Completed in ${duration}ms`);
  console.log(`Response length: ${response.length} chars`);

  return response;
}
```

### 5.2 Error Context

```typescript
export async function safeInvoke(
  agentName: string,
  prompt: string,
  config: SubagentConfig
): Promise<string> {
  try {
    return await invokeSubagent(agentName, prompt, config);
  } catch (error) {
    console.error('Subagent failed:', {
      agent: agentName,
      promptPreview: prompt.substring(0, 100),
      error: error instanceof Error ? error.message : String(error),
    });
    throw error;
  }
}
```

## Verification

Create `src/agents/test-subagents.ts`:

```typescript
import { config } from 'dotenv';
import { implementFeature } from './workflows/implement-feature';

config();

async function main() {
  const result = await implementFeature(
    'Function sanitizeApiKeys(text: string): string that redacts API keys'
  );

  console.log('\n=== Results ===');
  console.log('Quality Score:', result.qualityScore);
  console.log('\nTest:\n', result.test);
  console.log('\nImplementation:\n', result.implementation);
}

main();
```

```bash
ts-node src/agents/test-subagents.ts
```

## You're Done When...

- ✅ Understand when to delegate vs implement
- ✅ Can define effective subagent configurations
- ✅ Can invoke subagents with proper error handling
- ✅ Understand parallel and sequential patterns
- ✅ Can monitor subagent progress
- ✅ Have working examples of all patterns

## Best Practices

1. **Specific Prompts**: Give clear, detailed instructions
2. **Appropriate Tools**: Match tools to tasks
3. **Error Handling**: Always wrap in try-catch
4. **Logging**: Track invocations and timing
5. **Validation**: Use quality gates consistently
6. **Parallel When Possible**: Independent tasks run together
7. **Sequential When Dependent**: Pass results forward

## Next Steps

- [Testing Harness Usage](./guide-testing-harness-usage-2025-01-16.md) - AI-powered testing
- [TDD Workflow Guide](./guide-tdd-workflow-2025-01-16.md) - Test-driven development
- [Phase 1 Hook Development](./guide-phase-1-hook-development-2025-01-16.md) - Apply subagents to hooks

## Related Documents

- [Subagent System Architecture](../architecture/architecture-subagent-system-2025-01-16.md)
- [Claude Agent SDK Integration](./guide-claude-agent-sdk-integration-2025-01-16.md)
- [Implementation Roadmap](../plans/plan-implementation-roadmap-2025-01-16.md)
````

## File: guides/INDEX.md
````markdown
# Guides Documentation

> Last updated: 2025-01-16

## Overview

This directory contains step-by-step how-to guides for developing the Global Context Network MVP. Each guide is designed to be completed in < 2 hours with working code examples and verification steps.

## Quick Start Path

**Recommended Learning Order**:

```
1. Phase 0 Foundation Setup (90 min)
        ↓
2. Database Setup (30-45 min)
        ↓
3. Claude Agent SDK Integration (90 min)
        ↓
4. Using Subagents (90 min)
        ↓
5. Testing Harness Usage (120 min)
        ↓
6. TDD Workflow (90 min)
        ↓
7. Phase 1 Hook Development (150 min)
```

**Total Time**: ~10.5-11 hours to complete all guides

## Available Guides

| Date | Document | Time | Status | Description |
|------|----------|------|--------|-------------|
| 2025-01-16 | [guide-phase-0-foundation-setup-2025-01-16.md](./guide-phase-0-foundation-setup-2025-01-16.md) | 90 min | ✅ Ready | Set up TypeScript, Vitest, SQLite foundation |
| 2025-01-16 | [guide-database-setup-2025-01-16.md](./guide-database-setup-2025-01-16.md) | 30-45 min | ✅ Ready | Step-by-step SQLite database setup with canonical schema and Atlas migrations |
| 2025-01-16 | [guide-claude-agent-sdk-integration-2025-01-16.md](./guide-claude-agent-sdk-integration-2025-01-16.md) | 90 min | ✅ Ready | Integrate Claude Agent SDK for subagent orchestration |
| 2025-01-16 | [guide-using-subagents-2025-01-16.md](./guide-using-subagents-2025-01-16.md) | 90 min | ✅ Ready | Delegate to specialized subagents effectively |
| 2025-01-16 | [guide-testing-harness-usage-2025-01-16.md](./guide-testing-harness-usage-2025-01-16.md) | 120 min | ✅ Ready | Use Claude-powered testing for AI-driven validation |
| 2025-01-16 | [guide-tdd-workflow-2025-01-16.md](./guide-tdd-workflow-2025-01-16.md) | 90 min | ✅ Ready | Master Red-Green-Refactor with subagents |
| 2025-01-16 | [guide-phase-1-hook-development-2025-01-16.md](./guide-phase-1-hook-development-2025-01-16.md) | 150 min | ✅ Ready | Implement Claude Code hooks for event capture |

## Guide Structure

Each guide follows a consistent format:

### 1. Overview
- What you'll learn
- Time to complete
- What you'll build

### 2. Prerequisites
- Required setup
- Prior guides to complete
- Knowledge requirements

### 3. Step-by-Step Instructions
- Numbered steps with clear goals
- Complete, copy-paste code examples
- Expected output shown
- Verification at each step

### 4. Troubleshooting
- Common issues and solutions
- OS-specific caveats
- Debugging tips

### 5. Verification
- "You're done when..." checklist
- How to verify everything works
- What to check

### 6. Next Steps
- Links to related guides
- What to learn next
- How to apply knowledge

## Guide Categories

### Foundation & Setup
- **Phase 0 Foundation Setup**: TypeScript project, Vitest, SQLite, linting
- **Claude Agent SDK Integration**: SDK installation, configuration, basic usage

### Development Patterns
- **Using Subagents**: Delegation patterns, parallel/sequential execution
- **TDD Workflow**: Red-Green-Refactor with AI validation
- **Testing Harness Usage**: AI-powered test generation and validation

### Feature Implementation
- **Phase 1 Hook Development**: Event capture with Claude Code hooks

## Learning Paths

### Path 1: Quick Start (Backend Developer)
If you're experienced with TypeScript/Node.js:

1. **Phase 0 Setup** (skim if familiar)
2. **Claude Agent SDK** (focus on query() API)
3. **Using Subagents** (core delegation patterns)
4. **Phase 1 Hooks** (apply to real feature)

**Time**: ~6 hours

### Path 2: Full TDD Journey
For comprehensive TDD mastery:

1. **Phase 0 Setup**
2. **Testing Harness Usage**
3. **TDD Workflow**
4. **Phase 1 Hooks** (applying TDD)

**Time**: ~7.5 hours

### Path 3: Subagent Expert
Deep dive into AI orchestration:

1. **Phase 0 Setup**
2. **Claude Agent SDK** (all patterns)
3. **Using Subagents** (parallel/sequential)
4. **Testing Harness** (MCP integration)
5. **TDD Workflow** (subagent-driven)

**Time**: ~8 hours

## Prerequisites by Guide

### Phase 0 Foundation Setup
- Node.js 18+
- Basic TypeScript knowledge
- Command line familiarity

### Claude Agent SDK Integration
- Phase 0 complete
- Anthropic API key
- Understanding of async/await

### Using Subagents
- Phase 0 complete
- Claude Agent SDK integrated
- Understanding of delegation patterns

### Testing Harness Usage
- Subagents guide complete
- Vitest installed
- MCP server basics

### TDD Workflow
- Testing harness complete
- Subagents guide complete
- Understanding of TDD concepts

### Phase 1 Hook Development
- Phase 0 complete
- TDD workflow understood
- Subagents guide complete

## Key Concepts Covered

### TypeScript & Tooling
- Strict mode configuration
- ESLint and Prettier setup
- Vitest test runner
- Type-safe database queries

### Claude Agent SDK
- query() API usage
- Subagent definitions
- Streaming responses
- MCP server integration
- Error handling and retry logic

### Testing
- Test-driven development
- AI-powered test generation
- Test quality validation
- Coverage requirements
- Integration and E2E testing

### Subagents
- Delegation patterns
- Parallel execution
- Sequential dependencies
- Quality gates
- Monitoring and debugging

### Claude Code Hooks
- Event capture
- Performance requirements (< 100ms)
- Fast sanitization
- Persistent queues
- Error handling

## Common Patterns

### Pattern 1: Generate Tests with Subagent

```typescript
const testCode = await invokeSubagent(
  'unit-test-generator',
  'Generate tests for sanitizeApiKeys(text)',
  { agents: testGeneratorSubagents }
);
```

### Pattern 2: Validate Quality

```typescript
const validation = await validateTestQuality('./test-file.ts');
if (!validation.passed) {
  throw new Error(`Quality gate failed: ${validation.score}`);
}
```

### Pattern 3: Parallel Execution

```typescript
const [impl, tests] = await runParallel(
  [
    { agent: 'code-writer', prompt: 'Implement feature' },
    { agent: 'test-generator', prompt: 'Generate tests' },
  ],
  config
);
```

### Pattern 4: TDD Cycle

```typescript
// RED: Generate failing test
const test = await generateTest(feature);

// GREEN: Implement minimal code
const impl = await implementFeature(test);

// REFACTOR: Improve quality
const refactored = await refactorCode(impl);
```

## Troubleshooting Resources

### Common Issues

**Issue**: TypeScript strict mode errors

**Solution**: See [Phase 0 Foundation Setup - Troubleshooting](./guide-phase-0-foundation-setup-2025-01-16.md#troubleshooting)

**Issue**: MCP server not connecting

**Solution**: See [Testing Harness Usage - Troubleshooting](./guide-testing-harness-usage-2025-01-16.md#troubleshooting)

**Issue**: Hooks too slow (> 100ms)

**Solution**: See [Phase 1 Hook Development - Troubleshooting](./guide-phase-1-hook-development-2025-01-16.md#troubleshooting)

## Related Categories

- [Architecture](../architecture/INDEX.md) - System design and component architecture
- [Plans](../plans/INDEX.md) - Implementation plans and roadmaps
- [Reference](../reference/INDEX.md) - Technical specifications and APIs
- [Decisions](../decisions/INDEX.md) - ADRs explaining choices

## Quick Reference

### Environment Variables

```bash
# Required for all guides
ANTHROPIC_API_KEY=sk-ant-xxxxx

# Database
DB_PATH=./data/context.db

# MCP Test Runner
MCP_TEST_RUNNER_URL=http://localhost:3000

# Logging
LOG_LEVEL=info
```

### Essential Commands

```bash
# Type checking
npm run typecheck

# Linting
npm run lint

# Tests
npm run test

# Coverage
npm run coverage

# Build
npm run build

# Database migrations
npm run db:migrate

# MCP Test Runner
npm run mcp:test-runner
```

### File Structure

```
project/
├── .claude/
│   └── hooks/           # Claude Code hooks
│       ├── hooks.json
│       ├── user-prompt-submit.ts
│       └── stop.ts
├── src/
│   ├── agents/          # Subagent configurations
│   │   ├── subagents/
│   │   ├── orchestration/
│   │   └── workflows/
│   ├── db/              # Database utilities
│   ├── queue/           # Event queue
│   ├── sanitization/    # PII redaction
│   ├── mcp/             # MCP servers
│   └── types/           # TypeScript types
├── tests/               # Test files
├── data/                # SQLite database
└── docs/                # Documentation
```

## Tips for Success

1. **Follow the order**: Guides build on each other
2. **Complete verification**: Check "You're done when..." sections
3. **Run all examples**: Don't skip code execution
4. **Use provided code**: Copy-paste to avoid typos
5. **Check expected output**: Verify results match
6. **Troubleshoot early**: Don't skip error checks
7. **Ask questions**: Reference architecture docs for deep dives

## Getting Help

If you're stuck:

1. Check the guide's **Troubleshooting** section
2. Review **Expected output** sections
3. Consult related **Architecture** documents
4. Review **Reference** documentation for APIs
5. Check **Decisions** for context on choices

## Contributing

When adding new guides:

1. Follow the standard structure
2. Include copy-paste code examples
3. Show expected outputs
4. Add verification steps
5. Include troubleshooting section
6. Link to related documents
7. Update this INDEX.md
8. Test all code examples work

## Naming Convention

```
guide-{topic}-YYYY-MM-DD.md
```

Examples:
- `guide-phase-0-foundation-setup-2025-01-16.md`
- `guide-using-subagents-2025-01-16.md`
- `guide-tdd-workflow-2025-01-16.md`

---

*These guides are designed to be beginner-friendly while maintaining professional depth. Complete them in order for the best learning experience.*
````

## File: plans/INDEX.md
````markdown
# Plans Documentation

> Last updated: 2025-01-16

## Overview

This directory contains implementation plans, task breakdowns, and roadmaps for the Global Context Network MVP. All plans follow a subagent-driven development approach with Claude-powered testing.

## Master Plans

| Date | Document | Status | Description |
|------|----------|--------|-------------|
| 2025-01-16 | [plan-global-context-network-mvp-2025-01-16.md](./plan-global-context-network-mvp-2025-01-16.md) | Active | Complete MVP implementation plan overview |
| 2025-01-16 | [plan-implementation-roadmap-2025-01-16.md](./plan-implementation-roadmap-2025-01-16.md) | Active | 7-phase roadmap with timeline and dependencies |
| 2025-01-16 | [plan-iterative-build-strategy-2025-01-16.md](./plan-iterative-build-strategy-2025-01-16.md) | Active | 15-level iteration plan with working software at each step |
| 2025-01-16 | [plan-subagent-workflow-2025-01-16.md](./plan-subagent-workflow-2025-01-16.md) | Active | How specialized subagents work together |
| 2025-01-16 | [plan-original-user-vision-2025-01-16.md](./plan-original-user-vision-2025-01-16.md) | Active | User's original concept and requirements |

## Phase-Specific Task Plans

| Phase | Focus | Duration | Document | Status |
|-------|-------|----------|----------|--------|
| 0 | Foundation | 2-3 days | [plan-phase-0-tasks-2025-01-16.md](./plan-phase-0-tasks-2025-01-16.md) | Planned |
| 1 | Event Capture | 3-4 days | [plan-phase-1-tasks-2025-01-16.md](./plan-phase-1-tasks-2025-01-16.md) | Planned |
| 2 | Sanitization | 7-10 days | [plan-phase-2-tasks-2025-01-16.md](./plan-phase-2-tasks-2025-01-16.md) | Planned |
| 3 | Database | 2-3 days | [plan-phase-3-tasks-2025-01-16.md](./plan-phase-3-tasks-2025-01-16.md) | Planned |
| 4 | Async Processing | 5-7 days | [plan-phase-4-tasks-2025-01-16.md](./plan-phase-4-tasks-2025-01-16.md) | Planned |
| 5 | Learning Extraction | 6-8 days | [plan-phase-5-tasks-2025-01-16.md](./plan-phase-5-tasks-2025-01-16.md) | Planned |
| 6 | MCP Server | 3-4 days | [plan-phase-6-tasks-2025-01-16.md](./plan-phase-6-tasks-2025-01-16.md) | Planned |
| 7 | Mining & Upload | 4-10 days | [plan-phase-7-tasks-2025-01-16.md](./plan-phase-7-tasks-2025-01-16.md) | MVP+ |

## Total Plan Documents

**Count**: 12 documents (5 master plans + 7 phase plans)

## Key Implementation Principles

### Subagent-Driven Development
- All implementation delegated to specialized subagents
- Implementation, test generation, and validation subagents
- Parallel execution where possible
- Quality gates enforced automatically

### Test-Driven Development (TDD)
- Tests generated first (Red phase)
- Minimal implementation (Green phase)
- Refactor for quality (Refactor phase)
- Automated quality gates

### Privacy-First Architecture
- Pre-sanitize in hooks before any persistence
- Full sanitization before final storage
- Gold dataset with precision/recall metrics
- Zero plaintext raw data on disk
- Chain-of-thought excluded

### Quality Gates
- ≥ 85% test coverage
- Precision ≥ 98% per PII category
- Recall ≥ 95% per PII category
- Performance SLOs met (hooks < 100ms, queries < 100ms, MCP < 200ms)
- Security scan clean
- Lint + type-check passing

## Timeline Summary

**Total Duration**: 7-9 weeks (with 15-20% buffer)

### Week 1
- Phase 0: Foundation (days 1-3)
- Phase 1: Event Capture (days 4-7)

### Week 2-3
- Phase 2: Sanitization Pipeline (7-10 days)

### Week 3
- Phase 3: Database & Storage (2-3 days)

### Week 4
- Phase 4: Async Processing (5-7 days)

### Weeks 4-5
- Phase 5: Learning Extraction (6-8 days)

### Weeks 5-6
- Phase 6: MCP Server (3-4 days)

### Weeks 6-9 (MVP+)
- Phase 7: Mining & Upload (4-10 days, optional for MVP)

## Phase Dependencies

```
Phase 0 (Foundation)
    ↓
Phase 1 (Event Capture)
    ↓
Phase 2 (Sanitization)
    ↓
Phase 3 (Database)
    ↓
Phase 4 (Async Processing)
    ↓
Phase 5 (Learning Extraction)
    ↓
Phase 6 (MCP Server)
    ↓
Phase 7 (Mining & Upload) [MVP+]
```

## Critical Milestones

1. **Local Capture** (End of Week 1): Hooks working, events captured
2. **Privacy Guarantee** (End of Week 3): PII sanitization validated
3. **Persistent Learnings** (End of Week 5): Learnings extracted and stored
4. **Query Interface** (End of Week 6): MCP server accessible to agents
5. **Global Network** (Week 7-9): IPFS + blockchain integration [MVP+]

## MVP vs MVP+ Scope

### Core MVP (Weeks 1-6)
- ✅ Event capture via hooks
- ✅ PII sanitization with metrics
- ✅ Database storage
- ✅ Async job processing
- ✅ Learning extraction
- ✅ MCP query interface
- ✅ Local cross-project sharing

**Value**: Privacy-first learning capture and local sharing

### MVP+ (Weeks 7-9)
- ✅ IPFS upload
- ✅ Blockchain integration
- ✅ Token reward tracking
- ✅ Global network sharing

**Value**: Global sharing with incentives

## GPT-5 Review Feedback Incorporated

This implementation plan incorporates comprehensive feedback from GPT-5 review on:

### Critical Issues Addressed
1. **Privacy Contradiction Resolved**: Pre-sanitization in hooks before any persistence
2. **Gold Dataset**: 1000+ labeled PII examples with precision/recall thresholds per category
3. **Chain-of-Thought Exclusion**: Never stored, even sanitized
4. **Timeline Adjusted**: 7-10 days for sanitization (was 4-5), Phase 7 marked as MVP+
5. **Acceptance Criteria Clarified**: Measurable, automated, per-category metrics

### Enhanced Task Breakdowns
1. **Phase 1**: Added event schema, correlation IDs, idempotency, backpressure, cross-platform support
2. **Phase 2**: Enumerated exact PII categories, redaction formats, AI prompt templates, adversarial tests
3. **Phase 3**: Added migration versioning, data retention, purge policies, vacuum schedule
4. **Phase 4**: Added locking strategy, idempotency design, checkpoint/resume, chaos tests
5. **Phase 5**: Specified extraction approach, dedup strategy, negative set, human review process
6. **Phase 6**: Added auth model, pagination, rate limiting, input validation
7. **Phase 7**: Clarified smart contract vs simple registry, key management, user controls

### New Testing Requirements
1. **Performance**: p95/p99 latencies under load with concurrent clients
2. **Reliability**: Crash recovery, DB locked, disk full, concurrent writers
3. **Security**: Adversarial PII tests, prompt injection, SQLi prevention, log redaction
4. **Metrics**: Per-category precision/recall, F1 scores, gold dataset scoring

## Related Categories

- [Architecture](../architecture/INDEX.md) - System design and components
- [Decisions](../decisions/INDEX.md) - Architecture Decision Records
- [Guides](../guides/INDEX.md) - How-to documentation
- [Reference](../reference/INDEX.md) - Technical specifications

## Using These Plans

### For Implementation
1. Start with [MVP Plan](./plan-global-context-network-mvp-2025-01-16.md) for overview
2. Review [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md) for timeline
3. **Use [Iterative Build Strategy](./plan-iterative-build-strategy-2025-01-16.md) for step-by-step implementation**
4. Understand [Subagent Workflow](./plan-subagent-workflow-2025-01-16.md) for process
5. Execute phase-specific task plans in order

### For Understanding Scope
1. Read [Original User Vision](./plan-original-user-vision-2025-01-16.md) for context
2. Review phase plans to understand effort required
3. Check timeline and dependencies for scheduling

### For Quality Assurance
1. Follow TDD workflow in [Subagent Workflow](./plan-subagent-workflow-2025-01-16.md)
2. Enforce quality gates documented in each phase plan
3. Use acceptance criteria to validate completeness

---

*All plans follow the subagent-driven development model with Claude-powered testing harness. Each phase includes detailed tasks, success criteria, testing strategy, and clear dependencies.*
````

## File: plans/plan-global-context-network-mvp-2025-01-16.md
````markdown
# Global Context Network MVP - Complete Implementation Plan

> Comprehensive plan for building the Global Context Network using Claude Agent SDK and subagent-driven development

---
title: Global Context Network MVP Implementation Plan
category: plan
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [mvp, implementation, roadmap, blockchain, learning-network]
---

## Goal

Build a production-ready MVP that captures Claude Code conversations, sanitizes PII, extracts learnings, and uploads them to a global network with token rewards - all using subagent-driven development and Claude-powered testing.

## Background

The Global Context Network addresses a critical problem: valuable learnings from AI-assisted development are lost to individual conversations. This MVP creates a privacy-first system that:

1. Captures all Claude Code interactions via hooks
2. Sanitizes PII before any storage
3. Extracts valuable, reusable learnings
4. Shares learnings globally via IPFS + blockchain
5. Rewards quality contributions with tokens
6. Enables agents to query learnings via MCP

**Unique Innovation**: "Mining through learning" - users earn tokens by contributing quality learnings, not computational work.

## Approach

### Development Philosophy

**Subagent-Driven Development**: ALL implementation delegated to specialized Claude agents
- Implementation subagents build features
- Test subagents generate and validate tests
- Quality gate subagents enforce standards
- Integration subagents verify workflows

**Claude Testing Harness**: Self-validating system using Claude Agent SDK
- Tests generated first (TDD)
- Implementation validated automatically
- Quality gates enforced at every step
- No manual testing required

**Privacy-First Architecture**: Zero-trust PII handling
- Sanitize BEFORE database storage
- Hybrid approach (rules + AI)
- Audit all redactions
- Never store raw data

### Technology Stack

| Layer | Technology | Rationale |
|-------|-----------|-----------|
| Runtime | Node.js + TypeScript | Type safety, async-first |
| Database | SQLite | Simple, embedded, ACID |
| Testing | Vitest | Fast, modern, TypeScript-first |
| Sanitization | Regex + Claude API | Hybrid approach |
| MCP Server | @modelcontextprotocol/sdk | Standard protocol |
| Blockchain | TBD (Ethereum/Celestia) | EVM compatibility |
| Storage | IPFS | Decentralized, content-addressed |
| Queue | SQLite-based | Simple, persistent |

## Implementation Phases

### Phase 0: Foundation (Week 1)
**Goal**: TypeScript project, database schema, test infrastructure

- [ ] TypeScript project setup with strict mode
- [ ] Vitest testing framework configuration
- [ ] ESLint + Prettier setup
- [ ] Database schema design and migrations
- [ ] Test utilities and helpers
- [ ] CI/CD pipeline configuration

**Success Criteria**:
- TypeScript compiles with no errors
- Vitest runs and reports correctly
- Database migrations work bidirectionally
- All code passes linting

**Duration**: 2-3 days

### Phase 1: Event Capture (Week 1-2)
**Goal**: Hook into Claude Code and capture conversations

- [ ] UserPromptSubmit hook implementation
- [ ] Stop hook implementation
- [ ] Event collector to aggregate events
- [ ] Persistent event queue (SQLite-based)
- [ ] Hook performance monitoring (< 100ms)
- [ ] Error handling (never block user)

**Success Criteria**:
- Hooks execute in < 100ms
- Events persisted across restarts
- Zero user-blocking errors
- Complete conversation capture

**Duration**: 3-4 days

### Phase 2: Sanitization Pipeline (Week 2-3)
**Goal**: Remove ALL PII before database storage

- [ ] Rule-based PII detector (regex patterns)
- [ ] AI-powered sanitizer (Claude API)
- [ ] Hybrid validation pipeline
- [ ] Sanitization audit logger
- [ ] Test suite with 1000+ PII cases
- [ ] Performance optimization (< 2s per conversation)

**Success Criteria**:
- Zero PII leaks in test suite
- < 1% false positive rate (rules)
- < 5% false negative rate (AI)
- All redactions audited

**Duration**: 4-5 days

### Phase 3: Database & Storage (Week 3)
**Goal**: Persist sanitized data with ACID guarantees

- [ ] Repository pattern implementation
- [ ] Conversation table with indexes
- [ ] Messages table with relationships
- [ ] Learnings table with scoring
- [ ] Job queue table
- [ ] Query optimization (< 100ms)

**Success Criteria**:
- All queries < 100ms
- ACID compliance verified
- Migrations reversible
- Proper indexing

**Duration**: 2-3 days

### Phase 4: Async Processing (Week 4)
**Goal**: Background job processing without blocking

- [ ] Job queue implementation
- [ ] Worker process architecture
- [ ] Retry logic with exponential backoff
- [ ] Dead letter queue for failures
- [ ] Job status tracking
- [ ] Graceful shutdown handling

**Success Criteria**:
- Jobs never lost
- Proper retry on failures
- Workers scale independently
- Clean shutdown/restart

**Duration**: 3-4 days

### Phase 5: Learning Extraction (Week 4-5)
**Goal**: Extract valuable, reusable learnings

- [ ] Conversation analyzer (value detection)
- [ ] Category-specific extractors
- [ ] Quality scoring algorithm
- [ ] Deduplication logic
- [ ] Learning categorization
- [ ] Confidence threshold tuning

**Success Criteria**:
- Confidence scores ≥ 0.6
- Proper categorization
- No duplicate learnings
- Valuable insights extracted

**Duration**: 4-5 days

### Phase 6: MCP Server (Week 5-6)
**Goal**: Enable agents to query learnings

- [ ] MCP protocol server setup
- [ ] search_learnings tool implementation
- [ ] get_learning_by_id tool
- [ ] get_learning_context tool
- [ ] Resource endpoints (recent, top-rated)
- [ ] Query performance (< 200ms)

**Success Criteria**:
- MCP protocol compliant
- All queries < 200ms
- Proper error handling
- Claude Code integration

**Duration**: 3-4 days

### Phase 7: Mining & Upload (Week 6-7)
**Goal**: Upload to global network with rewards

- [ ] IPFS client integration
- [ ] Content upload to IPFS
- [ ] CID generation and tracking
- [ ] Blockchain integration
- [ ] Token reward calculation
- [ ] Upload status tracking

**Success Criteria**:
- Successful IPFS uploads
- CIDs properly stored
- Blockchain transactions confirmed
- Token rewards tracked

**Duration**: 4-5 days

## Testing Strategy

### Test Pyramid

- **70% Unit Tests**: Isolated component testing
- **20% Integration Tests**: Component interactions
- **10% E2E Tests**: Full system workflows

### Critical Coverage Areas

1. **Sanitization**: 1000+ PII test cases, zero leaks
2. **Hooks**: Non-blocking, error handling, performance
3. **Queue**: No job loss, proper ordering, retry logic
4. **Database**: ACID compliance, concurrency, migrations
5. **MCP**: Protocol compliance, performance, error handling

### Quality Gates

**Before ANY commit**:
- [ ] All tests pass
- [ ] Lint passes
- [ ] Type check passes
- [ ] Coverage ≥ 85%
- [ ] No security vulnerabilities

## Risks & Mitigations

### High-Priority Risks

**Risk**: PII leakage
- **Impact**: Critical - destroys user trust
- **Mitigation**: Sanitize before storage, 1000+ test cases, audit logging
- **Validation**: GPT-5 review of sanitization logic

**Risk**: Hook performance blocking user
- **Impact**: High - ruins UX
- **Mitigation**: < 100ms requirement, performance monitoring, fail-silent
- **Validation**: Load testing with realistic conversations

**Risk**: Job queue failures losing data
- **Impact**: High - learnings lost
- **Mitigation**: Persistent queue, retry logic, dead letter queue
- **Validation**: Chaos testing (kill workers, simulate failures)

### Medium-Priority Risks

**Risk**: Learning quality too low
- **Impact**: Medium - network value diminished
- **Mitigation**: Quality scoring, confidence thresholds, test data validation
- **Validation**: Manual review of extracted learnings

**Risk**: MCP server performance
- **Impact**: Medium - poor agent experience
- **Mitigation**: Query optimization, indexing, caching
- **Validation**: Load testing with concurrent queries

**Risk**: Blockchain integration complexity
- **Impact**: Medium - delays MVP
- **Mitigation**: Use established libraries, testnet first, defer if needed
- **Validation**: Incremental integration with fallback

## Success Criteria

### Functional Requirements

- ✅ **Event Capture**: All conversations captured via hooks
- ✅ **Privacy**: Zero PII leaks (validated by test suite)
- ✅ **Storage**: Sanitized data persisted in SQLite
- ✅ **Learning Extraction**: Quality learnings extracted
- ✅ **Query Interface**: MCP server accessible from Claude Code
- ✅ **Network Upload**: Learnings uploaded to IPFS/blockchain
- ✅ **Token Rewards**: Rewards tracked and distributed

### Performance Requirements

- ✅ Hook execution: < 100ms
- ✅ Event queueing: < 50ms
- ✅ Sanitization: < 2s per conversation
- ✅ Database queries: < 100ms
- ✅ MCP queries: < 200ms
- ✅ Learning extraction: < 5s per conversation

### Quality Requirements

- ✅ Test coverage: ≥ 85%
- ✅ TypeScript strict mode: 100% compliance
- ✅ PII test suite: 1000+ cases, zero leaks
- ✅ Documentation: All components documented
- ✅ Error handling: Graceful degradation everywhere

### User Experience Requirements

- ✅ Transparent to workflow (no blocking)
- ✅ Easy to query learnings via MCP
- ✅ Trust in privacy guarantees
- ✅ Learnings actually useful
- ✅ No performance impact on Claude Code

## Timeline

### Week 1
- Days 1-3: Phase 0 (Foundation)
- Days 4-7: Phase 1 (Event Capture)

### Week 2
- Days 1-4: Phase 1 completion
- Days 5-7: Phase 2 (Sanitization) start

### Week 3
- Days 1-2: Phase 2 completion
- Days 3-5: Phase 3 (Database)

### Week 4
- Days 1-4: Phase 4 (Async Processing)
- Days 5-7: Phase 5 (Learning Extraction) start

### Week 5
- Days 1-2: Phase 5 completion
- Days 3-7: Phase 6 (MCP Server)

### Week 6
- Days 1-2: Phase 6 completion
- Days 3-7: Phase 7 (Mining & Upload)

### Week 7
- Days 1-2: Phase 7 completion
- Days 3-5: Integration testing
- Days 6-7: Documentation and polish

**Total**: 7 weeks

## Dependencies

### External Dependencies
- Claude Code (hooks support)
- Claude API (sanitization)
- IPFS node/gateway
- Blockchain network (testnet initially)

### Internal Dependencies
- Phase 0 → All other phases (foundation)
- Phase 1 → Phase 2 (events to sanitize)
- Phase 2 → Phase 3 (sanitized data to store)
- Phase 3 → Phase 4 (storage for jobs)
- Phase 4 → Phase 5 (async processing for extraction)
- Phase 5 → Phase 6 (learnings to query)
- Phase 6 → Phase 7 (MCP for status)

## Post-MVP Enhancements

### Near-Term (Months 1-3)
- Semantic search for learnings
- Learning recommendations
- User feedback on learning quality
- Enhanced categorization

### Medium-Term (Months 3-6)
- Validator network (multi-agent validation)
- Quorum-based consensus
- Advanced token economics
- Multi-user support

### Long-Term (Months 6+)
- Distributed storage
- Advanced analytics
- Trend analysis
- Community curation tools

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Subagent System Architecture](../architecture/architecture-subagent-system-2025-01-16.md)
- [Testing Harness Architecture](../architecture/architecture-testing-harness-2025-01-16.md)

### Plans
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
- [Subagent Workflow](./plan-subagent-workflow-2025-01-16.md)
- [Phase-Specific Plans](./plan-phase-0-tasks-2025-01-16.md)

### Decisions
- [ADR: Subagent-Driven Development](../decisions/decision-subagent-driven-development-2025-01-16.md)
- [ADR: Claude Testing Harness](../decisions/decision-claude-testing-harness-2025-01-16.md)
- [ADR: Sanitize Before Storage](../decisions/decision-sanitize-before-storage-2025-01-16.md)

### Guides
- [Using Subagents](../guides/guide-using-subagents-2025-01-16.md)
- [TDD Workflow](../guides/guide-tdd-workflow-2025-01-16.md)
- [Testing Harness Usage](../guides/guide-testing-harness-usage-2025-01-16.md)

---

*This plan serves as the master implementation roadmap for the Global Context Network MVP. All phase-specific plans reference and detail this high-level overview.*
````

## File: plans/plan-implementation-roadmap-2025-01-16.md
````markdown
# Implementation Roadmap - Global Context Network MVP

> 7-phase implementation roadmap with timeline and dependencies

---
title: Global Context Network Implementation Roadmap
category: plan
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [roadmap, timeline, phases, dependencies]
---

## Overview

This document provides the complete implementation roadmap for the Global Context Network MVP, broken down into 7 phases with clear dependencies, deliverables, and success criteria.

**Total Duration**: 7-9 weeks (with 15-20% buffer for integration and testing)

**Development Approach**: Subagent-driven development with Claude-powered testing harness

## Roadmap Summary

| Phase | Focus | Duration | Deliverables | Status |
|-------|-------|----------|--------------|--------|
| 0 | Foundation | 2-3 days | TypeScript setup, DB schema, test infrastructure | Planned |
| 1 | Event Capture | 3-4 days | Hooks, event queue, correlation IDs | Planned |
| 2 | Sanitization | 7-10 days | Rule + AI sanitizer, gold dataset, metrics | Planned |
| 3 | Database & Storage | 2-3 days | Repository pattern, migrations, indexes | Planned |
| 4 | Async Processing | 5-7 days | Job queue, workers, idempotency | Planned |
| 5 | Learning Extraction | 6-8 days | Extractors, dedup, quality scoring | Planned |
| 6 | MCP Server | 3-4 days | MCP protocol, query tools, resources | Planned |
| 7 | Mining & Upload | 4-10 days | IPFS upload, blockchain integration | MVP+ |

## Phase Dependencies

```
Phase 0 (Foundation)
    ↓
Phase 1 (Event Capture)  ←  Requires: Project setup, DB schema
    ↓
Phase 2 (Sanitization)   ←  Requires: Events to sanitize
    ↓
Phase 3 (Database)       ←  Requires: Sanitized data to store
    ↓
Phase 4 (Async Queue)    ←  Requires: Storage for jobs
    ↓
Phase 5 (Learning)       ←  Requires: Async processing
    ↓
Phase 6 (MCP Server)     ←  Requires: Learnings to query
    ↓
Phase 7 (Upload)         ←  Requires: Quality learnings
```

## Critical Path

**Week 1**: Phase 0 + Phase 1
**Weeks 2-3**: Phase 2 (Sanitization - critical for privacy)
**Week 3**: Phase 3 (Database)
**Week 4**: Phase 4 (Async Processing)
**Weeks 4-5**: Phase 5 (Learning Extraction)
**Weeks 5-6**: Phase 6 (MCP Server)
**Weeks 6-7+**: Phase 7 (Network Upload - MVP+)

## Key Milestones

### Milestone 1: Local Capture (End of Week 1)
- ✅ Hooks working and non-blocking
- ✅ Events persisted to queue
- ✅ TypeScript + Vitest running
- **Value**: Can capture conversations

### Milestone 2: Privacy Guarantee (End of Week 3)
- ✅ PII sanitization working
- ✅ Gold dataset with precision/recall metrics
- ✅ Zero plaintext raw data on disk
- **Value**: Trust in privacy

### Milestone 3: Persistent Learnings (End of Week 5)
- ✅ Learnings extracted and scored
- ✅ Database storing sanitized data
- ✅ Deduplication working
- **Value**: Learnings saved locally

### Milestone 4: Query Interface (End of Week 6)
- ✅ MCP server running
- ✅ Agents can query learnings
- ✅ Performance < 200ms
- **Value**: Cross-project sharing

### Milestone 5: Global Network (Week 7-9) - MVP+
- ✅ IPFS uploads working
- ✅ Blockchain integration
- ✅ Token tracking
- **Value**: Global sharing + rewards

## Timeline Details

### Week 1: Foundation + Event Capture

**Days 1-3: Phase 0**
- TypeScript project with strict mode
- Vitest configuration
- Database schema design
- Migration system
- Test utilities

**Days 4-7: Phase 1**
- UserPromptSubmit hook
- Stop hook
- Event collector
- Persistent queue (with pre-sanitization for privacy)
- Performance monitoring

**Deliverables**:
- `tsconfig.json` with strict settings
- `vitest.config.ts` configured
- Database migrations 001-004
- Hook scripts in `.claude/hooks/`
- Event queue implementation

**Acceptance Criteria**:
- TypeScript compiles with no errors
- All tests pass
- Hooks execute < 100ms (p95)
- Events persisted with correlation IDs

### Week 2-3: Sanitization Pipeline

**Days 1-3: Rule-Based Detector**
- PII regex patterns (emails, phones, IPs, paths, keys, URLs)
- Per-category unit tests
- Redaction format conventions
- Performance optimization

**Days 4-7: AI Sanitizer**
- Claude API integration
- Context-aware detection
- Prompt templates (temp=0 for determinism)
- Fallback to rules-only

**Days 8-10: Gold Dataset & Metrics**
- Build/adopt labeled PII dataset
- Precision/recall scoring harness
- Per-category thresholds (precision ≥ 98%, recall ≥ 95%)
- Adversarial test suite

**Deliverables**:
- Rule-based sanitizer with 20+ PII patterns
- AI sanitizer with prompt templates
- Gold dataset with 1000+ labeled examples
- Scoring harness with per-category metrics
- Audit logging system

**Acceptance Criteria**:
- Precision ≥ 98% per PII category
- Recall ≥ 95% per PII category
- No plaintext raw data on disk
- Sanitization < 2s per conversation
- Chain-of-thought excluded from storage

### Week 3: Database & Storage

**Days 1-2: Repository Pattern**
- Conversation repository
- Messages repository
- Learnings repository
- Job queue repository

**Days 3: Query Optimization**
- Indexes for common queries
- WAL mode enabled
- Foreign key constraints
- Concurrent writer tests

**Deliverables**:
- Repository implementations
- Migration scripts with versioning
- Query optimization indexes
- ACID compliance tests

**Acceptance Criteria**:
- All queries < 100ms
- No orphan messages (FK constraints enforced)
- Migrations reversible
- Concurrent writes don't corrupt data

### Week 4: Async Processing

**Days 1-3: Job Queue**
- SQLite-based queue
- Priority handling
- Job status tracking
- Advisory locks for concurrency

**Days 4-5: Workers**
- Worker process architecture
- Job idempotency (dedupe keys)
- Graceful shutdown
- Checkpoint/resume

**Days 6-7: Reliability**
- Exponential backoff retry
- Dead letter queue
- Crash recovery tests
- Chaos testing (DB locked, disk full)

**Deliverables**:
- Job queue implementation
- Worker processes for 3 job types
- Retry logic with backoff
- Monitoring and metrics

**Acceptance Criteria**:
- Jobs never lost
- Idempotent processing verified
- Graceful shutdown within 30s
- Crash recovery E2E tests pass

### Week 4-5: Learning Extraction

**Days 1-2: Conversation Analyzer**
- Value detection (is conversation worth learning from?)
- Category identification
- Quality pre-filtering

**Days 3-5: Extractors**
- Rule-based + LLM approach
- Category-specific extractors
- Metadata schema (tags, provenance, version)
- Prompt templates

**Days 6-8: Quality & Dedup**
- Confidence scoring (≥ 0.6 threshold)
- Deduplication (cosine similarity < 0.85)
- Negative set to prevent trivial learnings
- Human-in-the-loop sample review

**Deliverables**:
- Conversation analyzer
- 8 category extractors
- Quality scoring algorithm
- Deduplication system
- Sample review process

**Acceptance Criteria**:
- 90% of sample rated "useful" by reviewer
- No duplicate learnings (verified by similarity)
- Confidence scores calibrated
- Metadata complete with provenance

### Week 5-6: MCP Server

**Days 1-2: Protocol Setup**
- MCP SDK integration
- Server configuration
- Loopback binding (127.0.0.1)
- Optional API key auth

**Days 3-4: Tools Implementation**
- `search_learnings` with filters
- `get_learning_by_id`
- `get_learning_context`
- Input validation and bounds

**Days 5: Resources & Performance**
- Resource endpoints (recent, top-rated, stats)
- Query optimization
- Pagination and sorting
- Rate limiting

**Deliverables**:
- MCP server implementation
- 3 tools + 3 resources
- Authentication layer
- Performance tests

**Acceptance Criteria**:
- MCP protocol conformance tests pass
- All queries < 200ms
- Invalid queries rejected gracefully
- No data leaks via MCP

### Week 6-9: Mining & Upload (MVP+)

**Days 1-3: IPFS Integration**
- IPFS client setup (pinning provider vs self-hosted)
- Content upload implementation
- CID generation and tracking
- Retry logic

**Days 4-7: Blockchain Integration**
- Choose network (testnet first)
- Wallet generation and key management
- CID anchoring (simple registry or contract)
- Transaction confirmation

**Days 8-10: Token System (if time)**
- Reward calculation
- Upload status tracking
- User controls (opt-in, manual approval)
- License metadata

**Deliverables**:
- IPFS upload working
- Blockchain integration
- Key management system
- User controls

**Acceptance Criteria**:
- CID retrievable via 2+ gateways
- N confirmed blocks for on-chain record
- Keys stored securely (OS keychain)
- Manual approval gate before upload

## Risk Management

### Critical Risks

**PII Leakage** (Impact: Critical)
- Mitigation: Pre-sanitize in hook, gold dataset, audit logging
- Gates: Zero leaks in test suite, precision/recall thresholds

**Hook Performance** (Impact: High)
- Mitigation: < 100ms requirement, fast pre-sanitizer, monitoring
- Gates: p95/p99 latency tests, load testing

**Data Loss** (Impact: High)
- Mitigation: Idempotent jobs, persistent queue, crash recovery
- Gates: Chaos tests, concurrent writer tests

### Deferred Risks (Post-MVP)

- Validator network attacks
- Multi-user access control
- International PII patterns
- IP/licensing compliance

## Quality Gates

**Before Phase Completion**:
- [ ] All unit tests pass
- [ ] Integration tests pass
- [ ] Lint + type-check clean
- [ ] Coverage ≥ 85%
- [ ] Performance SLOs met
- [ ] Security scan clean
- [ ] Documentation updated

**Before MVP Release**:
- [ ] E2E tests pass
- [ ] Privacy audit complete (zero PII leaks)
- [ ] Performance benchmarks met
- [ ] User controls working (opt-in, delete)
- [ ] Installation guide complete
- [ ] Monitoring/logging working

## Parallel Work Opportunities

### Week 1
- Phase 0 foundation (sequential)
- Phase 1 hooks can be prototyped in parallel

### Week 2-3
- Rule-based sanitizer (parallel track 1)
- AI sanitizer (parallel track 2)
- Gold dataset creation (parallel track 3)

### Week 4
- Job queue (parallel track 1)
- Workers (parallel track 2, depends on queue)

### Week 5-6
- Learning extraction refinement
- MCP server implementation (can start earlier)

## Post-MVP Roadmap

### Months 1-3
- Semantic search for learnings
- Enhanced categorization
- User feedback loops
- Analytics dashboard

### Months 3-6
- Validator network (multi-agent)
- Quorum-based consensus
- Advanced token economics
- Multi-user support

### Months 6+
- Distributed storage
- Trend analysis
- Community curation
- Mobile support

## Related Documents

- [MVP Plan](./plan-global-context-network-mvp-2025-01-16.md)
- [Subagent Workflow](./plan-subagent-workflow-2025-01-16.md)
- [Phase 0 Tasks](./plan-phase-0-tasks-2025-01-16.md)
- [Phase 1 Tasks](./plan-phase-1-tasks-2025-01-16.md)
- [Phase 2 Tasks](./plan-phase-2-tasks-2025-01-16.md)
- [Phase 3 Tasks](./plan-phase-3-tasks-2025-01-16.md)
- [Phase 4 Tasks](./plan-phase-4-tasks-2025-01-16.md)
- [Phase 5 Tasks](./plan-phase-5-tasks-2025-01-16.md)
- [Phase 6 Tasks](./plan-phase-6-tasks-2025-01-16.md)
- [Phase 7 Tasks](./plan-phase-7-tasks-2025-01-16.md)
````

## File: plans/plan-iterative-build-strategy-2025-01-16.md
````markdown
# Iterative Build Strategy - Global Context Network

> A level-by-level implementation plan where each iteration produces a working, testable, demonstrable system

---
title: Iterative Build Strategy
category: plan
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [iteration, tdd, vertical-slice, working-software, incremental]
---

## Overview

This document defines a **working-software-first** build strategy for the Global Context Network. Each level produces a complete vertical slice through the system that can be built, tested, and demonstrated independently.

**Core Principle**: Always have working code. Never break what already works. Build complexity incrementally through proven layers.

**Critical Reviews Context**:
- GPT-5 identified critical contradictions in privacy flow and schema
- Gemini flagged unrealistic 7-9 week timeline
- Both emphasized missing core architecture documents
- This plan addresses those issues with concrete, buildable steps

## The Iteration Philosophy

### What Makes a Good Iteration?

✅ **GOOD Iteration**:
- Adds ONE capability that works end-to-end
- Has passing tests proving it works
- Can be demonstrated to stakeholders
- Takes hours or days, not weeks
- Leaves system in working state

❌ **BAD Iteration**:
- Builds infrastructure for future features
- Requires multiple future iterations to work
- Has no demonstrable value
- Optimizes before basics work
- Leaves system broken or incomplete

### Example: Email PII Detection

```
✅ GOOD: "Add email PII detection with 3 test cases"
   - Write regex for email detection
   - Write 3 tests (basic, edge case, false positive)
   - Tests pass
   - Demo: Show email getting redacted
   - Duration: 2-4 hours

❌ BAD: "Build complete sanitization pipeline with all 34 PII types"
   - Too large, takes weeks
   - Can't test until all 34 types work
   - No intermediate value
   - High risk of rework
```

## Iteration Levels (15 Levels)

### Level 0: Project Foundation (Hello World)
**Duration**: 4-6 hours
**Goal**: TypeScript project compiles and runs basic test

**Deliverable**:
```typescript
// src/hello.ts
export function hello(name: string): string {
  return `Hello, ${name}!`;
}

// tests/hello.test.ts
import { describe, it, expect } from 'vitest';
import { hello } from '../src/hello';

describe('hello', () => {
  it('should greet by name', () => {
    expect(hello('World')).toBe('Hello, World!');
  });
});
```

**Tests**:
- `npm test` passes
- `npm run type-check` passes
- `npm run lint` passes

**Acceptance**:
- [ ] TypeScript strict mode enabled
- [ ] Vitest configured and running
- [ ] ESLint passing
- [ ] Can build with `npm run build`

**What NOT to do**:
- ❌ Set up database
- ❌ Configure hooks
- ❌ Plan entire architecture

---

### Level 1: SQLite Connection (Write + Read)
**Duration**: 3-4 hours
**Goal**: Write one record to SQLite, read it back

**Deliverable**:
```typescript
// src/database/connection.ts
import Database from 'better-sqlite3';

export function createDb(path: string = ':memory:'): Database.Database {
  const db = new Database(path);
  db.pragma('journal_mode = WAL');
  db.pragma('foreign_keys = ON');
  return db;
}

// src/database/test-write.ts
export function writeTestRecord(db: Database.Database, message: string): string {
  const id = Date.now().toString();
  db.prepare('CREATE TABLE IF NOT EXISTS test (id TEXT PRIMARY KEY, message TEXT)').run();
  db.prepare('INSERT INTO test (id, message) VALUES (?, ?)').run(id, message);
  return id;
}

export function readTestRecord(db: Database.Database, id: string): string | null {
  const row = db.prepare('SELECT message FROM test WHERE id = ?').get(id) as { message: string } | undefined;
  return row?.message ?? null;
}
```

**Tests**:
```typescript
it('should write and read a record', () => {
  const db = createDb();
  const id = writeTestRecord(db, 'test message');
  const message = readTestRecord(db, id);
  expect(message).toBe('test message');
  db.close();
});
```

**Acceptance**:
- [ ] Can create in-memory database
- [ ] Can write record
- [ ] Can read record back
- [ ] Test passes

**What NOT to do**:
- ❌ Build migrations system
- ❌ Design full schema
- ❌ Add indexes or constraints

---

### Level 2: Hook Skeleton (Receive Event, Log It)
**Duration**: 4-6 hours
**Goal**: Hook receives stdin event and logs to console

**Deliverable**:
```typescript
// .claude/hooks/src/userPromptSubmit.ts
async function readStdin(): Promise<string> {
  const chunks: Buffer[] = [];
  for await (const chunk of process.stdin) {
    chunks.push(chunk);
  }
  return Buffer.concat(chunks).toString('utf-8');
}

async function main() {
  try {
    const input = await readStdin();
    const event = JSON.parse(input);
    console.log('Event received:', {
      type: event.type,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    // Fail silently - never block user
    console.error('Hook error:', error);
  }
}

main();
```

**Build Script**:
```json
// .claude/hooks/package.json
{
  "scripts": {
    "build": "tsc",
    "test": "vitest"
  }
}
```

**Hook Config**:
```json
// .claude/hooks.json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/dist/userPromptSubmit.js"
  }
}
```

**Tests**:
```typescript
it('should parse event from stdin', async () => {
  const mockEvent = { type: 'UserPromptSubmit', content: 'test' };
  const result = await parseEvent(JSON.stringify(mockEvent));
  expect(result.type).toBe('UserPromptSubmit');
});
```

**Acceptance**:
- [ ] TypeScript compiles to `.js`
- [ ] Hook runs when triggered
- [ ] Logs event to console
- [ ] Doesn't crash on malformed input
- [ ] Completes in <100ms (measured)

**What NOT to do**:
- ❌ Write to database
- ❌ Sanitize content
- ❌ Process async

---

### Level 3: Fast Sanitization (One PII Type)
**Duration**: 4-6 hours
**Goal**: Redact email addresses from text

**Deliverable**:
```typescript
// src/sanitization/fast-sanitize.ts
const EMAIL_REGEX = /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g;

export function sanitizeEmails(text: string): string {
  return text.replace(EMAIL_REGEX, '[REDACTED_EMAIL]');
}
```

**Tests**:
```typescript
describe('sanitizeEmails', () => {
  it('should redact basic email', () => {
    expect(sanitizeEmails('Contact: user@example.com'))
      .toBe('Contact: [REDACTED_EMAIL]');
  });

  it('should redact multiple emails', () => {
    expect(sanitizeEmails('user1@ex.com and user2@ex.com'))
      .toBe('[REDACTED_EMAIL] and [REDACTED_EMAIL]');
  });

  it('should not redact non-emails', () => {
    expect(sanitizeEmails('Not an email: test@'))
      .toBe('Not an email: test@');
  });
});
```

**Performance Test**:
```typescript
it('should sanitize in <10ms', () => {
  const text = 'user@example.com '.repeat(100);
  const start = performance.now();
  sanitizeEmails(text);
  const duration = performance.now() - start;
  expect(duration).toBeLessThan(10);
});
```

**Acceptance**:
- [ ] 3 unit tests pass
- [ ] Performance test passes (<10ms for 100 emails)
- [ ] No false positives in test cases

**What NOT to do**:
- ❌ Add other PII types (phones, IPs, etc.)
- ❌ Add AI validation
- ❌ Build configuration system

---

### Level 4: Hook + Sanitize + Write (End-to-End)
**Duration**: 6-8 hours
**Goal**: Hook receives event, sanitizes email, writes to SQLite

**Deliverable**:
```typescript
// .claude/hooks/src/userPromptSubmit.ts
import { performance } from 'node:perf_hooks';
import Database from 'better-sqlite3';
import { sanitizeEmails } from '../../src/sanitization/fast-sanitize';

interface Event {
  type: string;
  content: string;
  timestamp: string;
}

async function main() {
  const start = performance.now();

  try {
    const input = await readStdin();
    const event: Event = JSON.parse(input);

    // Fast sanitization
    const sanitized = sanitizeEmails(event.content);

    // Write to database
    const db = new Database('.claude/context.db');
    db.pragma('journal_mode = WAL');

    db.prepare(`
      CREATE TABLE IF NOT EXISTS messages (
        id TEXT PRIMARY KEY,
        content TEXT NOT NULL,
        created_at TEXT NOT NULL
      )
    `).run();

    db.prepare('INSERT INTO messages (id, content, created_at) VALUES (?, ?, ?)')
      .run(Date.now().toString(), sanitized, new Date().toISOString());

    db.close();

    const duration = performance.now() - start;
    console.log(`Hook completed in ${duration.toFixed(2)}ms`);

  } catch (error) {
    console.error('Hook failed:', error);
  }
}
```

**Integration Test**:
```typescript
it('should sanitize and persist event end-to-end', async () => {
  const event = {
    type: 'UserPromptSubmit',
    content: 'Email me at user@example.com',
    timestamp: new Date().toISOString()
  };

  await runHook(event);

  const db = new Database('.claude/context.db');
  const messages = db.prepare('SELECT content FROM messages').all();
  expect(messages[0].content).toContain('[REDACTED_EMAIL]');
  expect(messages[0].content).not.toContain('user@example.com');
  db.close();
});
```

**Acceptance**:
- [ ] End-to-end test passes
- [ ] Email is redacted before storage
- [ ] Raw email NEVER written to disk
- [ ] Hook completes in <100ms
- [ ] Database file created at `.claude/context.db`

**What NOT to do**:
- ❌ Add more PII types yet
- ❌ Build job queue
- ❌ Add correlation IDs

---

### Level 5: Add 3 More PII Types
**Duration**: 6-8 hours
**Goal**: Redact phones, IPs, and file paths

**Deliverable**:
```typescript
// src/sanitization/fast-sanitize.ts
const EMAIL_REGEX = /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g;
const PHONE_REGEX = /\b(\+\d{1,3}[-.]?)?\(?\d{3}\)?[-.]?\d{3}[-.]?\d{4}\b/g;
const IP_REGEX = /\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b/g;
const PATH_REGEX = /\/Users\/[a-zA-Z0-9_-]+\/[^\s]*/g;

export interface SanitizationResult {
  sanitized: string;
  redactions: number;
}

export function fastSanitize(text: string): SanitizationResult {
  let redactions = 0;
  let result = text;

  result = result.replace(EMAIL_REGEX, () => { redactions++; return '[REDACTED_EMAIL]'; });
  result = result.replace(PHONE_REGEX, () => { redactions++; return '[REDACTED_PHONE]'; });
  result = result.replace(IP_REGEX, () => { redactions++; return '[REDACTED_IP]'; });
  result = result.replace(PATH_REGEX, () => { redactions++; return '[REDACTED_PATH]'; });

  return { sanitized: result, redactions };
}
```

**Tests** (3 per type = 12 total):
```typescript
describe('fastSanitize', () => {
  describe('emails', () => {
    it('should redact basic email', () => { /* ... */ });
    it('should redact email with subdomain', () => { /* ... */ });
    it('should not redact partial email', () => { /* ... */ });
  });

  describe('phones', () => {
    it('should redact US phone', () => { /* ... */ });
    it('should redact international phone', () => { /* ... */ });
    it('should not redact random numbers', () => { /* ... */ });
  });

  // ... similar for IPs and paths
});
```

**Acceptance**:
- [ ] 12 unit tests pass (3 per PII type)
- [ ] Performance still <50ms for typical conversation
- [ ] No false positives in tests
- [ ] Integration test still passes

**What NOT to do**:
- ❌ Add ALL 34 PII types at once
- ❌ Add AI validation
- ❌ Build audit logging yet

---

### Level 6: Correlation IDs (Multi-Message Conversations)
**Duration**: 4-6 hours
**Goal**: Track messages belonging to same conversation

**Deliverable**:
```typescript
// src/database/schema.ts
export function createSchema(db: Database.Database): void {
  db.exec(`
    CREATE TABLE IF NOT EXISTS conversations (
      id TEXT PRIMARY KEY,
      created_at TEXT NOT NULL,
      updated_at TEXT NOT NULL
    );

    CREATE TABLE IF NOT EXISTS messages (
      id TEXT PRIMARY KEY,
      conversation_id TEXT NOT NULL,
      role TEXT NOT NULL,
      content TEXT NOT NULL,
      sequence INTEGER NOT NULL,
      created_at TEXT NOT NULL,
      FOREIGN KEY (conversation_id) REFERENCES conversations(id)
    );

    CREATE INDEX IF NOT EXISTS idx_messages_conversation
      ON messages(conversation_id, sequence);
  `);
}
```

**Hook Update**:
```typescript
// Get or create conversation ID from session
const conversationId = getOrCreateConversationId(event);

db.prepare('INSERT OR IGNORE INTO conversations (id, created_at, updated_at) VALUES (?, ?, ?)')
  .run(conversationId, now, now);

const sequence = getNextSequence(db, conversationId);

db.prepare(`
  INSERT INTO messages (id, conversation_id, role, content, sequence, created_at)
  VALUES (?, ?, ?, ?, ?, ?)
`).run(messageId, conversationId, event.role, sanitized, sequence, now);
```

**Tests**:
```typescript
it('should link messages to conversation', () => {
  const conversationId = 'conv-1';

  insertMessage(db, conversationId, 'user', 'Hello', 1);
  insertMessage(db, conversationId, 'assistant', 'Hi', 2);

  const messages = db.prepare(
    'SELECT role, sequence FROM messages WHERE conversation_id = ? ORDER BY sequence'
  ).all(conversationId);

  expect(messages).toHaveLength(2);
  expect(messages[0].role).toBe('user');
  expect(messages[1].role).toBe('assistant');
});
```

**Acceptance**:
- [ ] Conversations table created
- [ ] Messages linked via FK
- [ ] Sequence numbers auto-increment
- [ ] Can query full conversation
- [ ] Foreign key constraint enforced

**What NOT to do**:
- ❌ Build async job queue
- ❌ Add learning extraction
- ❌ Implement ULID (use simple IDs for now)

---

### Level 7: Job Queue (One Job Type)
**Duration**: 6-8 hours
**Goal**: Queue one type of job and process it

**Deliverable**:
```typescript
// src/database/job-queue.ts
export type JobStatus = 'queued' | 'in_progress' | 'completed' | 'failed' | 'dead_letter';

export interface Job {
  id: string;
  type: string;
  payload: string;
  status: JobStatus;
  attempts: number;
  created_at: string;
  updated_at: string;
}

export function createJobQueue(db: Database.Database): void {
  db.exec(`
    CREATE TABLE IF NOT EXISTS job_queue (
      id TEXT PRIMARY KEY,
      type TEXT NOT NULL,
      payload TEXT NOT NULL,
      status TEXT NOT NULL,
      attempts INTEGER DEFAULT 0,
      created_at TEXT NOT NULL,
      updated_at TEXT NOT NULL
    );

    CREATE INDEX IF NOT EXISTS idx_jobs_status_type
      ON job_queue(status, type);
  `);
}

export function enqueueJob(db: Database.Database, type: string, payload: unknown): string {
  const id = Date.now().toString();
  const now = new Date().toISOString();

  db.prepare(`
    INSERT INTO job_queue (id, type, payload, status, created_at, updated_at)
    VALUES (?, ?, ?, 'queued', ?, ?)
  `).run(id, type, JSON.stringify(payload), now, now);

  return id;
}

export function dequeueJob(db: Database.Database, type: string): Job | null {
  const job = db.prepare(`
    SELECT * FROM job_queue
    WHERE status = 'queued' AND type = ?
    ORDER BY created_at ASC
    LIMIT 1
  `).get(type) as Job | undefined;

  if (!job) return null;

  db.prepare(`
    UPDATE job_queue
    SET status = 'in_progress', updated_at = ?
    WHERE id = ?
  `).run(new Date().toISOString(), job.id);

  return job;
}
```

**Worker**:
```typescript
// src/workers/test-worker.ts
async function processTestJob(payload: { message: string }): Promise<void> {
  console.log('Processing:', payload.message);
  // Simulate work
  await new Promise(resolve => setTimeout(resolve, 100));
}

async function runWorker(db: Database.Database): Promise<void> {
  while (true) {
    const job = dequeueJob(db, 'test');
    if (!job) {
      await new Promise(resolve => setTimeout(resolve, 1000));
      continue;
    }

    try {
      await processTestJob(JSON.parse(job.payload));
      completeJob(db, job.id);
    } catch (error) {
      failJob(db, job.id, error);
    }
  }
}
```

**Tests**:
```typescript
it('should enqueue and dequeue job', () => {
  const id = enqueueJob(db, 'test', { message: 'hello' });
  const job = dequeueJob(db, 'test');

  expect(job).not.toBeNull();
  expect(job!.id).toBe(id);
  expect(job!.status).toBe('in_progress');
});
```

**Acceptance**:
- [ ] Can enqueue job
- [ ] Can dequeue job
- [ ] Status changes queued → in_progress
- [ ] Worker processes job
- [ ] Job marked completed

**What NOT to do**:
- ❌ Add retry logic
- ❌ Add dead letter queue
- ❌ Handle concurrency

---

### Level 8: Simple Learning Extraction (Fixed Category)
**Duration**: 8-10 hours
**Goal**: Extract one learning from a conversation

**Deliverable**:
```typescript
// src/learning/simple-extractor.ts
export interface Learning {
  id: string;
  category: 'technical' | 'workflow' | 'insight';
  title: string;
  content: string;
  conversation_id: string;
  created_at: string;
}

export function extractSimpleLearning(
  conversation: { messages: Array<{ role: string; content: string }> }
): Learning | null {
  // Simple heuristic: Look for assistant messages with code blocks
  const hasCode = conversation.messages.some(
    msg => msg.role === 'assistant' && msg.content.includes('```')
  );

  if (!hasCode) return null;

  return {
    id: Date.now().toString(),
    category: 'technical',
    title: 'Code example shared',
    content: 'Conversation contained code examples',
    conversation_id: 'conv-1',
    created_at: new Date().toISOString()
  };
}
```

**Database**:
```typescript
db.exec(`
  CREATE TABLE IF NOT EXISTS learnings (
    id TEXT PRIMARY KEY,
    category TEXT NOT NULL,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    conversation_id TEXT NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id)
  );
`);
```

**Tests**:
```typescript
it('should extract learning from code conversation', () => {
  const conversation = {
    messages: [
      { role: 'user', content: 'How do I sort an array?' },
      { role: 'assistant', content: 'Use ```js\narr.sort()\n```' }
    ]
  };

  const learning = extractSimpleLearning(conversation);
  expect(learning).not.toBeNull();
  expect(learning!.category).toBe('technical');
});

it('should not extract from non-code conversation', () => {
  const conversation = {
    messages: [
      { role: 'user', content: 'Hello' },
      { role: 'assistant', content: 'Hi there' }
    ]
  };

  const learning = extractSimpleLearning(conversation);
  expect(learning).toBeNull();
});
```

**Acceptance**:
- [ ] Can extract learning from code conversation
- [ ] Returns null for non-valuable conversations
- [ ] Learning stored in database
- [ ] Foreign key to conversation enforced

**What NOT to do**:
- ❌ Use AI for extraction yet
- ❌ Add quality scoring
- ❌ Add deduplication
- ❌ Support all 8 categories

---

### Level 9: Basic MCP Server (One Tool)
**Duration**: 6-8 hours
**Goal**: MCP server with one tool: `get_learning`

**Deliverable**:
```typescript
// src/mcp/server.ts
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import Database from 'better-sqlite3';

const db = new Database('.claude/context.db');

const server = new Server({
  name: 'context-server',
  version: '0.1.0'
}, {
  capabilities: {
    tools: {}
  }
});

server.setRequestHandler('tools/list', async () => ({
  tools: [{
    name: 'get_learning',
    description: 'Get a learning by ID',
    inputSchema: {
      type: 'object',
      properties: {
        id: { type: 'string' }
      },
      required: ['id']
    }
  }]
}));

server.setRequestHandler('tools/call', async (request) => {
  if (request.params.name === 'get_learning') {
    const { id } = request.params.arguments as { id: string };

    const learning = db.prepare(
      'SELECT * FROM learnings WHERE id = ?'
    ).get(id);

    return {
      content: [{
        type: 'text',
        text: JSON.stringify(learning, null, 2)
      }]
    };
  }

  throw new Error('Unknown tool');
});

const transport = new StdioServerTransport();
await server.connect(transport);
```

**Test**:
```typescript
it('should return learning by id', async () => {
  // Insert test learning
  db.prepare(`
    INSERT INTO learnings (id, category, title, content, conversation_id, created_at)
    VALUES (?, ?, ?, ?, ?, ?)
  `).run('learn-1', 'technical', 'Test', 'Content', 'conv-1', new Date().toISOString());

  const result = await callTool('get_learning', { id: 'learn-1' });
  const learning = JSON.parse(result.content[0].text);

  expect(learning.id).toBe('learn-1');
  expect(learning.title).toBe('Test');
});
```

**Acceptance**:
- [ ] MCP server starts
- [ ] Lists one tool
- [ ] Tool returns learning
- [ ] Invalid ID returns null gracefully
- [ ] Response < 200ms

**What NOT to do**:
- ❌ Add search tool yet
- ❌ Add resources
- ❌ Add authentication

---

### Level 10: Add Search Tool
**Duration**: 4-6 hours
**Goal**: Search learnings by keyword

**Deliverable**:
```typescript
server.setRequestHandler('tools/list', async () => ({
  tools: [
    {
      name: 'get_learning',
      description: 'Get a learning by ID',
      inputSchema: {
        type: 'object',
        properties: {
          id: { type: 'string' }
        },
        required: ['id']
      }
    },
    {
      name: 'search_learnings',
      description: 'Search learnings by keyword',
      inputSchema: {
        type: 'object',
        properties: {
          query: { type: 'string' },
          limit: { type: 'number', default: 10 }
        },
        required: ['query']
      }
    }
  ]
}));

server.setRequestHandler('tools/call', async (request) => {
  if (request.params.name === 'search_learnings') {
    const { query, limit = 10 } = request.params.arguments as { query: string; limit?: number };

    const learnings = db.prepare(`
      SELECT * FROM learnings
      WHERE title LIKE ? OR content LIKE ?
      ORDER BY created_at DESC
      LIMIT ?
    `).all(`%${query}%`, `%${query}%`, limit);

    return {
      content: [{
        type: 'text',
        text: JSON.stringify(learnings, null, 2)
      }]
    };
  }
  // ... existing get_learning handler
});
```

**Tests**:
```typescript
it('should search learnings by title', async () => {
  insertLearning(db, { id: 'l1', title: 'TypeScript tips', content: 'Use strict mode' });
  insertLearning(db, { id: 'l2', title: 'React hooks', content: 'useState is great' });

  const result = await callTool('search_learnings', { query: 'TypeScript' });
  const learnings = JSON.parse(result.content[0].text);

  expect(learnings).toHaveLength(1);
  expect(learnings[0].id).toBe('l1');
});

it('should limit results', async () => {
  for (let i = 0; i < 20; i++) {
    insertLearning(db, { id: `l${i}`, title: `Learning ${i}`, content: 'test' });
  }

  const result = await callTool('search_learnings', { query: 'Learning', limit: 5 });
  const learnings = JSON.parse(result.content[0].text);

  expect(learnings).toHaveLength(5);
});
```

**Acceptance**:
- [ ] Can search by title
- [ ] Can search by content
- [ ] Limit works
- [ ] Returns empty array for no matches
- [ ] Performance < 200ms

**What NOT to do**:
- ❌ Add semantic search
- ❌ Add filters by category
- ❌ Add FTS5 index yet

---

### Level 11: Retry Logic + Dead Letter Queue
**Duration**: 6-8 hours
**Goal**: Retry failed jobs with exponential backoff

**Deliverable**:
```typescript
// src/workers/reliable-worker.ts
const MAX_ATTEMPTS = 3;

async function processWithRetry(db: Database.Database, job: Job): Promise<void> {
  try {
    await processJob(job);
    completeJob(db, job.id);
  } catch (error) {
    const attempts = job.attempts + 1;

    if (attempts >= MAX_ATTEMPTS) {
      // Move to dead letter queue
      db.prepare(`
        UPDATE job_queue
        SET status = 'dead_letter', attempts = ?, updated_at = ?
        WHERE id = ?
      `).run(attempts, new Date().toISOString(), job.id);
    } else {
      // Retry with backoff
      const backoffMs = Math.pow(2, attempts) * 1000;

      db.prepare(`
        UPDATE job_queue
        SET status = 'failed', attempts = ?, updated_at = ?
        WHERE id = ?
      `).run(attempts, new Date().toISOString(), job.id);

      await new Promise(resolve => setTimeout(resolve, backoffMs));

      // Re-queue
      db.prepare(`
        UPDATE job_queue
        SET status = 'queued', updated_at = ?
        WHERE id = ?
      `).run(new Date().toISOString(), job.id);
    }
  }
}
```

**Tests**:
```typescript
it('should retry failed job', async () => {
  let attempts = 0;
  const failingJob = () => {
    attempts++;
    if (attempts < 2) throw new Error('Fail');
  };

  const jobId = enqueueJob(db, 'test', {});
  const job = dequeueJob(db, 'test')!;

  await processWithRetry(db, job, failingJob);

  expect(attempts).toBe(2);
  const final = db.prepare('SELECT status FROM job_queue WHERE id = ?').get(jobId);
  expect(final.status).toBe('completed');
});

it('should move to dead letter after max attempts', async () => {
  const alwaysFails = () => { throw new Error('Always fails'); };

  const jobId = enqueueJob(db, 'test', {});

  for (let i = 0; i < MAX_ATTEMPTS; i++) {
    const job = dequeueJob(db, 'test')!;
    await processWithRetry(db, job, alwaysFails);
  }

  const final = db.prepare('SELECT status, attempts FROM job_queue WHERE id = ?').get(jobId);
  expect(final.status).toBe('dead_letter');
  expect(final.attempts).toBe(MAX_ATTEMPTS);
});
```

**Acceptance**:
- [ ] Jobs retry on failure
- [ ] Exponential backoff works
- [ ] Dead letter queue after max attempts
- [ ] Tests pass

**What NOT to do**:
- ❌ Add monitoring dashboard
- ❌ Add alerting
- ❌ Add manual retry UI

---

### Level 12: AI Sanitization (Async Validation)
**Duration**: 8-10 hours
**Goal**: AI validates pre-sanitized content for missed PII

**Deliverable**:
```typescript
// src/sanitization/ai-validator.ts
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

export async function validateSanitization(
  sanitizedText: string
): Promise<{ isClean: boolean; issues: string[] }> {
  const prompt = `You are a PII detection validator. Check if this sanitized text still contains any PII:

Text:
${sanitizedText}

Respond with JSON:
{
  "isClean": true/false,
  "issues": ["description of any PII found"]
}`;

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1000,
    temperature: 0,
    messages: [{ role: 'user', content: prompt }]
  });

  const result = JSON.parse(response.content[0].text);
  return result;
}
```

**Job Handler**:
```typescript
// src/workers/sanitization-worker.ts
async function processSanitizationJob(payload: { messageId: string }): Promise<void> {
  const message = db.prepare('SELECT content FROM messages WHERE id = ?').get(payload.messageId);

  const validation = await validateSanitization(message.content);

  if (!validation.isClean) {
    // Log to audit table
    db.prepare(`
      INSERT INTO sanitization_log (id, message_id, issues, created_at)
      VALUES (?, ?, ?, ?)
    `).run(Date.now().toString(), payload.messageId, JSON.stringify(validation.issues), new Date().toISOString());
  }
}
```

**Tests**:
```typescript
it('should detect missed PII', async () => {
  const result = await validateSanitization('My name is John Doe and I live at 123 Main St');
  expect(result.isClean).toBe(false);
  expect(result.issues.length).toBeGreaterThan(0);
});

it('should pass clean text', async () => {
  const result = await validateSanitization('The function takes two parameters and returns a string');
  expect(result.isClean).toBe(true);
});
```

**Acceptance**:
- [ ] AI validation works
- [ ] Issues logged to audit table
- [ ] Completes in <2s
- [ ] Gracefully handles API errors

**What NOT to do**:
- ❌ Re-sanitize on the fly
- ❌ Block hook on AI validation
- ❌ Add complex retry logic yet

---

### Level 13: ULID Migration + Timestamps
**Duration**: 4-6 hours
**Goal**: Replace simple IDs with ULID

**Deliverable**:
```typescript
import { ulid } from 'ulid';

// Before: const id = Date.now().toString();
// After:  const id = ulid();

// Benefits:
// - Sortable by creation time
// - Globally unique
// - Collision-resistant
```

**Migration**:
```sql
-- No data migration needed if starting fresh
-- Just update all ID generation code
```

**Tests**:
```typescript
it('should generate sortable ULIDs', () => {
  const id1 = ulid();
  const id2 = ulid();
  expect(id2 > id1).toBe(true); // Lexicographically sortable
});
```

**Acceptance**:
- [ ] All new IDs use ULID
- [ ] IDs are sortable
- [ ] No collisions in tests

**What NOT to do**:
- ❌ Migrate existing data (if any)
- ❌ Change database schema

---

### Level 14: Full Sanitization (All PII Types)
**Duration**: 10-12 hours
**Goal**: Add remaining PII patterns (API keys, SSH keys, etc.)

**Deliverable**:
```typescript
// src/sanitization/patterns.ts
export const PII_PATTERNS = {
  EMAIL: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g,
  PHONE: /\b(\+\d{1,3}[-.]?)?\(?\d{3}\)?[-.]?\d{3}[-.]?\d{4}\b/g,
  IP: /\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b/g,
  PATH: /\/Users\/[a-zA-Z0-9_-]+\/[^\s]*/g,
  API_KEY_OPENAI: /sk-[a-zA-Z0-9]{48}/g,
  API_KEY_ANTHROPIC: /sk-ant-[a-zA-Z0-9-]{95}/g,
  AWS_KEY: /AKIA[0-9A-Z]{16}/g,
  GITHUB_TOKEN: /ghp_[a-zA-Z0-9]{36}/g,
  JWT: /eyJ[a-zA-Z0-9_-]+\.eyJ[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+/g,
  SSH_KEY: /-----BEGIN (RSA|OPENSSH) PRIVATE KEY-----[\s\S]+?-----END (RSA|OPENSSH) PRIVATE KEY-----/g,
  CREDIT_CARD: /\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b/g,
  SSN: /\b\d{3}-\d{2}-\d{4}\b/g
};

export function comprehensiveSanitize(text: string): SanitizationResult {
  let result = text;
  let redactions = 0;

  for (const [type, pattern] of Object.entries(PII_PATTERNS)) {
    result = result.replace(pattern, () => {
      redactions++;
      return `[REDACTED_${type}]`;
    });
  }

  return { sanitized: result, redactions };
}
```

**Tests** (3 per type = 36 tests):
```typescript
describe('comprehensiveSanitize', () => {
  // 3 tests per PII type
  // - Basic case
  // - Edge case
  // - False positive check
});
```

**Performance Test**:
```typescript
it('should sanitize in <50ms', () => {
  const text = generateTestConversation(); // ~10KB
  const start = performance.now();
  comprehensiveSanitize(text);
  const duration = performance.now() - start;
  expect(duration).toBeLessThan(50);
});
```

**Acceptance**:
- [ ] All 12 PII types covered
- [ ] 36 tests pass (3 per type)
- [ ] Performance <50ms
- [ ] No false positives in test suite

**What NOT to do**:
- ❌ Add pseudonymization yet
- ❌ Add configuration for patterns
- ❌ Build UI for pattern management

---

### Level 15: End-to-End Integration Test
**Duration**: 6-8 hours
**Goal**: Test complete flow from hook to MCP query

**Deliverable**:
```typescript
// tests/e2e/full-flow.test.ts
describe('End-to-End Flow', () => {
  it('should capture, sanitize, extract, and query learning', async () => {
    // 1. Simulate hook event
    const event = {
      type: 'UserPromptSubmit',
      content: 'My email is user@example.com. Here is code: ```js\nconst x = 1;\n```',
      role: 'assistant',
      timestamp: new Date().toISOString()
    };

    await runHook(event);

    // 2. Verify sanitization
    const messages = db.prepare('SELECT content FROM messages ORDER BY created_at DESC LIMIT 1').all();
    expect(messages[0].content).toContain('[REDACTED_EMAIL]');
    expect(messages[0].content).not.toContain('user@example.com');

    // 3. Process learning extraction job
    const learningJob = dequeueJob(db, 'extract_learning');
    await processLearningJob(learningJob!);

    // 4. Verify learning created
    const learnings = db.prepare('SELECT * FROM learnings').all();
    expect(learnings.length).toBeGreaterThan(0);

    // 5. Query via MCP
    const searchResult = await callMcpTool('search_learnings', { query: 'code' });
    const found = JSON.parse(searchResult.content[0].text);
    expect(found.length).toBeGreaterThan(0);
    expect(found[0].category).toBe('technical');
  });

  it('should handle multiple conversations', async () => {
    // Create 3 conversations
    // Extract learnings from each
    // Verify all queryable via MCP
  });

  it('should maintain privacy throughout', async () => {
    // Inject PII-heavy event
    // Verify ZERO raw PII in database
    // Verify ZERO raw PII in logs
    // Verify ZERO raw PII in MCP responses
  });
});
```

**Acceptance**:
- [ ] Full E2E test passes
- [ ] Privacy test passes (zero PII leaks)
- [ ] Multi-conversation test passes
- [ ] All components integrated
- [ ] System works end-to-end

**What NOT to do**:
- ❌ Add IPFS upload
- ❌ Add blockchain integration
- ❌ Build UI

---

## Dependency Graph

```
Level 0: Foundation
   ↓
Level 1: SQLite
   ↓
Level 2: Hook Skeleton  ←────────┐
   ↓                              │
Level 3: Fast Sanitize (1 type)   │
   ↓                              │
Level 4: Hook + Sanitize + Write  │ (Uses Level 2 + 3 + 1)
   ↓                              │
Level 5: More PII Types           │
   ↓                              │
Level 6: Correlation IDs ─────────┘
   ↓
Level 7: Job Queue
   ↓
Level 8: Simple Learning
   ↓
Level 9: MCP Server (1 tool)
   ↓
Level 10: MCP Search
   ↓
Level 11: Retry Logic ←───── (Enhances Level 7)
   ↓
Level 12: AI Validation ←───── (Enhances Level 5)
   ↓
Level 13: ULID Migration
   ↓
Level 14: Full Sanitization
   ↓
Level 15: E2E Integration
```

**Critical Path**: 0 → 1 → 2 → 3 → 4 → 5 → 6 → 7 → 8 → 9 → 10 → 15

**Optional Enhancements**: 11, 12, 13, 14 (can be done in parallel with critical path)

---

## Timeline Estimates

### Aggressive (Best Case): 9-11 weeks
- Assumes no blockers
- Perfect focus
- Minimal rework

### Realistic (Expected): 14-16 weeks
- Some rework on sanitization patterns
- Testing harness learning curve
- Integration debugging

### Conservative (Buffer): 18-20 weeks
- Significant rework
- Novel approaches need iteration
- Unexpected technical challenges

**Gemini Review Warning**: "7-9 week timeline is unrealistic"
**Recommendation**: Plan for 16 weeks (4 months) with 2-week sprints per level

---

## Quality Gates (Every Level)

Before moving to next level:

- [ ] All unit tests pass
- [ ] Integration test passes (if applicable)
- [ ] Lint + type-check clean
- [ ] Code reviewed (self or peer)
- [ ] Performance budget met
- [ ] Documentation updated
- [ ] Demo prepared

**Never proceed if**:
- Tests are failing
- Performance regresses
- Previous level breaks

---

## Anti-Patterns to Avoid

### 1. Building Infrastructure for Future Features
❌ BAD:
```typescript
// Building generic queue system with plugins, priority queues,
// distributed workers, monitoring, before having ONE job type working
```

✅ GOOD:
```typescript
// Simple queue with one job type
// Add complexity AFTER basics work
```

### 2. Premature Optimization
❌ BAD:
```typescript
// Level 3: Add caching, connection pooling, query optimization
```

✅ GOOD:
```typescript
// Level 3: Get one sanitization pattern working
// Optimize in Level 14 when you know what's slow
```

### 3. Multi-Level Changes
❌ BAD:
```typescript
// Level 4: Add sanitization + job queue + learning extraction + MCP
```

✅ GOOD:
```typescript
// Level 4: Hook + sanitize + write
// Level 7: Job queue
// Level 8: Learning extraction
// Level 9: MCP
```

### 4. Skipping Tests
❌ BAD:
```typescript
// "I'll add tests later after I build more features"
```

✅ GOOD:
```typescript
// Write test FIRST (TDD)
// Test passes before moving to next level
```

### 5. Breaking Working Code
❌ BAD:
```typescript
// Level 10: Refactor entire sanitization system
// (Breaks Levels 4-9)
```

✅ GOOD:
```typescript
// Level 10: Add new feature without breaking existing
// Refactor incrementally with tests proving nothing broke
```

---

## Examples: Good vs Bad Iterations

### Example 1: Learning Extraction

❌ **BAD Level**: "Build complete learning extraction system"
- Too vague
- Too large (weeks of work)
- No clear acceptance criteria
- Can't test until complete

✅ **GOOD Levels**:
- Level 8: "Extract learning if conversation has code block" (8 hours)
  - Clear heuristic
  - One test case
  - Demonstrable

- Level 8.1: "Add category detection for code vs workflow" (4 hours)
  - Builds on Level 8
  - Two test cases
  - Easy to verify

- Level 8.2: "Add AI extraction for non-code learnings" (8 hours)
  - Builds on 8.1
  - Separate job type
  - Fallback to heuristic

### Example 2: MCP Server

❌ **BAD Level**: "Build MCP server with all tools and resources"
- 5+ tools at once
- Can't test until all work
- High risk of rework

✅ **GOOD Levels**:
- Level 9: "MCP server with get_learning tool" (6 hours)
  - One tool works
  - Can test with MCP Inspector
  - Demonstrable

- Level 10: "Add search_learnings tool" (4 hours)
  - Builds on existing server
  - Independent feature
  - Easy to test

- Level 10.1: "Add resources for recent learnings" (3 hours)
  - Separate MCP concept
  - Doesn't break tools
  - Progressive enhancement

### Example 3: Sanitization

❌ **BAD Level**: "Build sanitization pipeline with all PII types and AI validation"
- 12 PII types at once
- AI integration coupled
- Overwhelming test matrix

✅ **GOOD Levels**:
- Level 3: "Sanitize emails only" (4 hours)
  - 3 test cases
  - Performance measured
  - Working end-to-end

- Level 5: "Add phone, IP, path patterns" (6 hours)
  - 3 types × 3 tests = 9 tests
  - Still fast
  - Incremental value

- Level 14: "Add remaining 8 PII types" (10 hours)
  - After basics proven
  - Know performance characteristics
  - Test infrastructure exists

---

## Testing Strategy Per Level

### Level 0-6: Unit + Integration
```typescript
// Unit: Test individual functions
it('should sanitize email', () => { /* ... */ });

// Integration: Test component interaction
it('should write sanitized message to DB', () => { /* ... */ });
```

### Level 7-12: Unit + Integration + Worker
```typescript
// Worker: Test async processing
it('should process queued job', async () => { /* ... */ });
```

### Level 15: E2E
```typescript
// End-to-end: Test full system flow
it('should capture, sanitize, extract, and query', async () => { /* ... */ });
```

**Coverage Requirements**:
- Level 0-6: ≥85% line coverage per level
- Level 7-14: ≥90% line coverage (critical path)
- Level 15: 100% E2E flow coverage

---

## When to Add Complexity

### Decision Tree

```
Is basic feature working?
  No → Focus on basic feature
  Yes ↓

Do all tests pass?
  No → Fix tests first
  Yes ↓

Is performance acceptable?
  No → Profile and optimize
  Yes ↓

Is there demonstrable value in enhancement?
  No → Skip, move to next level
  Yes ↓

Can enhancement be done without breaking existing?
  No → Defer to later level
  Yes ↓

Add complexity ✅
```

### Example: When to add AI sanitization?

❌ Too early:
- Level 3: Basics not working yet
- Level 4: Integration not proven
- Level 5: Pattern coverage incomplete

✅ Right time:
- Level 12: After rule-based works
- After performance budget proven
- After job queue working
- As async enhancement, not blocker

---

## Success Metrics Per Level

### Level 0: Foundation
- ✅ `npm test` runs
- ✅ TypeScript compiles
- ✅ One test passes

### Level 4: First E2E
- ✅ Hook completes <100ms
- ✅ Email redacted before disk
- ✅ Integration test passes

### Level 8: First Learning
- ✅ Learning extracted
- ✅ Stored in database
- ✅ Can query by ID

### Level 10: First Search
- ✅ Search returns results
- ✅ Query <200ms
- ✅ Limit works

### Level 15: Complete MVP
- ✅ Full E2E test passes
- ✅ Zero PII leaks verified
- ✅ All performance budgets met
- ✅ System demonstrable

---

## Vertical Slice Philosophy

### NOT: Horizontal Layers
```
Week 1-3: Build entire database layer
Week 4-6: Build entire API layer
Week 7-9: Build entire UI layer
```
**Problem**: Nothing works until week 9

### YES: Vertical Slices
```
Week 1: One feature end-to-end (hook → DB)
Week 2: Add one sanitization type
Week 3: Add one learning type
Week 4: Add one MCP tool
```
**Benefit**: Working demo every week

### Visualization

```
❌ Horizontal:
┌─────────────────┐
│   UI Layer      │ ← Week 9
├─────────────────┤
│   API Layer     │ ← Week 6
├─────────────────┤
│   DB Layer      │ ← Week 3
└─────────────────┘
(Nothing works until week 9)

✅ Vertical:
┌──┬──┬──┬──┬──┐
│E │S │L │M │F │
│2 │a │e │C │u │
│E │n │a │P │l │
│  │i │r │  │l │
│H │t │n │S │S │
│o │i │i │e │y │
│o │z │n │a │s │
│k │e │g │r │t │
│  │  │  │c │e │
│  │  │  │h │m │
└──┴──┴──┴──┴──┘
W1  W2  W3  W4  W5
(Working system every week)
```

---

## Related Documents

- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md) - 7-phase plan
- [STANDARDS](../STANDARDS.md) - Canonical standards
- [GPT-5 Review](../reviews/gpt5-holistic-review-2025-01-16.txt) - Critical issues
- [Gemini Review](../reviews/gemini-holistic-review-2025-01-16.txt) - Timeline reality check

---

## Summary

**15 levels, each level is:**
1. **Buildable** - Working code that compiles and runs
2. **Testable** - Has passing tests proving it works
3. **Demonstrable** - Can show value to stakeholders
4. **Independent** - Doesn't require future levels to work
5. **Incremental** - Adds one clear capability

**Timeline**: 14-16 weeks (realistic) vs 7-9 weeks (original, unrealistic)

**Philosophy**: Always have working code. Build complexity through proven layers. Test first, optimize later.

**Critical Path**: 0→1→2→3→4→5→6→7→8→9→10→15 (11 levels minimum for basic MVP)

**Next Steps**:
1. Start Level 0 immediately
2. Complete each level fully before proceeding
3. Never break what works
4. Demonstrate progress weekly
````

## File: plans/plan-original-user-vision-2025-01-16.md
````markdown
# Original User Vision - Global Context Network

> Capturing the user's original concept and requirements

---
title: Original User Vision for Global Context Network
category: plan
date: 2025-01-16
status: active
authors: Dennison (User) + Claude
tags: [vision, requirements, original-concept, blockchain, learning-network]
---

## User's Original Request

**Date**: 2025-01-16

### The Core Idea

*"I'm thinking about building a global context network for AI agents to learn from one another. Basically right now when I work through a project like when I'm doing a TypeScript development project there will be some sort of problem that I have to end up solving and working through with the agent. The learnings from this tend to just be lost to this specific project but I think it would be really interesting if these learnings could be shared with my other projects but also globally with everyone."*

### Unique Approach

*"It would be to use Claude Code hooks so to start with Claude Code so that before every prompt or before the LLM takes action we write to database my prompt and then when the LLM finishes action you know we write to the database everything that the LLM said and you know ideally all of their thinking processes and we do that for the entire conversation."*

### Privacy-First Design

*"There's a second step where another LLM takes out all the personally identifiable information so that's like API keys directories you know specific names of things so that we aren't leaking information and that could actually happen in the step before writing to the database just in case so we never have to worry about leaking data."*

### Decentralized Storage

*"Then we put that data on chain or we put it into IPFS and put a hash a pin of hash of it into the blockchain or we could put it into something like Celestia for example so it goes from the agent itself working through sanitation for private information into a database and then from a database into a global pool that's available for everyone."*

### Mining Through Learning

*"The process of providing it to this global pool would be sort of like a new-age version of cryptocurrency mining so by providing context to the pool you are effectively mining but you don't get the payout immediately because you could just stream junk from an LLM into this."*

### Validator Network

*"What we would have is validators there'd be a validator network which would be other agents who basically read this sort of context thread read this complete summary to digest learnings from it and decide whether or not this is a valid addition to the context pool."*

### Data Processing Pipeline

*"You have an agent that sanitizes the personal information before it goes into the database and then you have another agent that asynchronously summarizes the learnings of the process and then we store the learnings of the process into the global pool or maybe we store both."*

*"Then we have a validator network of agents which validate that this submission is good or you know contains good information or it seems valid basically it's like a eval running evals on this submitted data. You would need sort of like a quorum of agents to decide that it is valid and if it's valid then we distribute tokens out to the agent that provided it."*

### Query Interface

*"Now that information gets stored into the sort of global context database and we have a model context protocol server that agents can use when they start a process project where they can check this global database for learnings."*

*"So if you're familiar with context 7 which is basically database around docs this would essentially be like a database around learnings so we could share these learnings and effectively by doing that you are mining so you're providing learnings to the network and then the network is rewarding with tokens."*

## MVP Scope (User-Defined)

*"I would like to build an MVP of this and what I'm thinking of is hooks for Claude code that take both every prompt end prompt and then you know any context in between runs it through sanitization in a sub-agent probably with Claude code then stores it directly into a SQLite database."*

*"Then kicks off another async process so there's sort of like a queue of analyzing each chunk that comes in and you know updating an idea of the learnings that are going so that you know you don't have to remember you know you can close the window without losing all the learnings but each time you know you update there's some new learnings the learnings are stored in the database as well."*

*"And then after that it goes into another process async like a mining process it may be this like a miner that's running at the same time or just runs in the background because you were running in the first place and then that uploads it to the context Network."*

*"Then you have model context protocol which allows the ages to query that network and maybe potentially you know generate a key pair to collect funds."*

## Key Requirements Identified

### 1. Capture Layer
- ✅ Use Claude Code hooks
- ✅ Capture every user prompt (UserPromptSubmit)
- ✅ Capture every agent response (Stop)
- ✅ Include thinking processes if available
- ✅ Never block the user

### 2. Privacy Layer
- ✅ Sanitize BEFORE database storage
- ✅ Remove API keys
- ✅ Remove directory paths
- ✅ Remove specific names
- ✅ Use another agent for sanitization
- ✅ Never leak PII

### 3. Storage Layer
- ✅ SQLite database
- ✅ Store sanitized conversations
- ✅ Store extracted learnings
- ✅ Persist across window closes
- ✅ Queue-based processing

### 4. Learning Extraction
- ✅ Async processing
- ✅ Summarize learnings continuously
- ✅ Update as new chunks arrive
- ✅ Store learnings in database

### 5. Mining/Upload
- ✅ Background process
- ✅ Async upload to network
- ✅ IPFS or blockchain storage
- ✅ Token reward tracking

### 6. Query Interface
- ✅ Model Context Protocol server
- ✅ Allow agents to query learnings
- ✅ Key pair for fund collection
- ✅ Similar to Context7 for docs

### 7. Validator Network (Future)
- 🔮 Agent validators
- 🔮 Quality evaluation
- 🔮 Quorum-based validation
- 🔮 Token distribution on validation

## Design Principles Extracted

### Privacy-First
*"Sanitization happens before writing to database just in case so we never have to worry about leaking data."*

**Implementation**: Zero-trust PII handling with sanitization before storage.

### Async Everything
*"Kicks off another async process... a mining process... running in the background."*

**Implementation**: Job queue system with async workers for all processing.

### Never Lose Data
*"You can close the window without losing all the learnings."*

**Implementation**: Persistent queue and continuous learning extraction.

### Quality Through Validation
*"Validators... decide whether or not this is a valid addition."*

**Implementation**: Quality scoring and validator network (future).

### Incentive Alignment
*"By providing learnings to the network... the network is rewarding with tokens."*

**Implementation**: Token rewards for quality contributions.

## Evolution from Original Vision to MVP Plan

### What Stayed the Same
1. ✅ Claude Code hooks for capture
2. ✅ Sanitization before storage
3. ✅ SQLite database
4. ✅ Async processing
5. ✅ Learning extraction
6. ✅ MCP server interface
7. ✅ IPFS/blockchain upload

### What Was Enhanced
1. **Subagent-Driven Development**: All implementation via specialized subagents
2. **Claude Testing Harness**: Self-validating system using Claude Agent SDK
3. **Hybrid Sanitization**: Rule-based + AI for better accuracy
4. **Detailed Architecture**: 7-phase implementation plan with testable components
5. **Quality Gates**: Automated validation at every step

### What Was Deferred to Post-MVP
1. Validator network (quorum-based validation)
2. Multi-user support
3. Distributed storage
4. Complex token economics
5. Smart contract deployment

## Success Metrics (From User Intent)

### Functional Success
- ✅ Learnings captured from every conversation
- ✅ Zero PII leaks
- ✅ Learnings queryable via MCP
- ✅ Works across different projects
- ✅ Async processing doesn't block workflow

### User Experience Success
- ✅ Transparent to normal workflow
- ✅ No performance impact on Claude Code
- ✅ Easy to query learnings
- ✅ Learnings actually useful
- ✅ Trust in privacy guarantees

### Network Success (Future)
- 🔮 High-quality learnings contributed
- 🔮 Learnings validated by community
- 🔮 Token rewards distributed fairly
- 🔮 Network grows organically
- 🔮 Agents actively query network

## User's Vision for Impact

### Individual Developer Level
- Learn from own past experiences
- Share learnings across projects
- Build personal knowledge base
- Improve over time automatically

### Team Level
- Share learnings within organization
- Onboard new developers faster
- Standardize best practices
- Collective intelligence growth

### Global Level
- AI agents learn from each other
- Best practices emerge organically
- Problems solved once, shared globally
- Accelerate software development
- Democratize expert knowledge

## Key Insights from User

### Problem Statement
*"The learnings from this tend to just be lost to this specific project."*

**Solution**: Persistent storage and global sharing of learnings.

### Value Proposition
*"It would be really interesting if these learnings could be shared with my other projects but also globally with everyone."*

**Implementation**: MCP server for cross-project queries + global network for sharing.

### Trust Requirement
*"We never have to worry about leaking data."*

**Guarantee**: Sanitize BEFORE storage, never trust raw data.

### Quality Control
*"You could just stream junk from an LLM into this."*

**Solution**: Quality scoring + validator network + token incentives.

## Alignment with Claude Code Philosophy

The user's vision aligns perfectly with Claude Code's capabilities:

1. **Hooks System**: Native support for capturing events
2. **Subagent Support**: Built-in task delegation
3. **MCP Integration**: Standard protocol for tools
4. **File Access**: Can read/write for learning storage
5. **Bash Access**: Can run background processes

## Implementation Strategy

### Phase-Based Rollout

**Phases 0-3 (Weeks 1-3)**: Core Infrastructure
- Foundation, event capture, sanitization, storage
- **User Value**: Learnings saved locally, queryable

**Phases 4-6 (Weeks 4-6)**: Intelligence Layer
- Async processing, learning extraction, MCP server
- **User Value**: Quality learnings, cross-project queries

**Phase 7 (Week 6-7)**: Global Network
- IPFS upload, blockchain integration
- **User Value**: Share globally, earn rewards

### Post-MVP Enhancements

**Validator Network**:
- Multi-agent quality validation
- Quorum-based consensus
- Token distribution on validation

**Advanced Features**:
- Semantic search
- Learning recommendations
- Trend analysis
- Community curation

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Subagent System](../architecture/architecture-subagent-system-2025-01-16.md)
- [Sanitization Pipeline](../architecture/architecture-sanitization-pipeline-2025-01-16.md)

### Plans
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
- [Global Context Network MVP](./plan-global-context-network-mvp-2025-01-16.md)

### Decisions
- [ADR: Use Claude Hooks](../decisions/decision-use-claude-hooks-2025-01-16.md)
- [ADR: Sanitize Before Storage](../decisions/decision-sanitize-before-storage-2025-01-16.md)

---

*This document preserves the user's original vision and requirements to ensure the implementation stays aligned with the core intent.*
````

## File: plans/plan-phase-0-tasks-2025-01-16.md
````markdown
# Phase 0: Foundation Tasks

> TypeScript project setup, database schema, and test infrastructure

---
title: Phase 0 Foundation Tasks
category: plan
date: 2025-01-16
status: active
tags: [phase-0, foundation, typescript, database, testing]
---

## Goal

Establish project foundation with TypeScript, database schema, and test infrastructure to support all subsequent phases.

## Duration

2-3 days

## Tasks

### TypeScript Project Setup
- [ ] Initialize Node.js project with npm/pnpm
- [ ] Configure TypeScript with strict mode enabled
- [ ] Set up tsconfig.json with paths, outDir, strict settings
- [ ] Configure package.json scripts (build, dev, test, lint)
- [ ] Install core dependencies (@types/node, tsx, etc.)

**Subagent**: `foundation-setup-agent`

**Acceptance**: TypeScript compiles with no errors, strict mode enabled

### ESLint + Prettier Setup
- [ ] Install ESLint with TypeScript plugin
- [ ] Configure .eslintrc.json with strict rules
- [ ] Install Prettier
- [ ] Configure .prettierrc with consistent settings
- [ ] Add lint and format scripts to package.json

**Acceptance**: Lint and format scripts run successfully

### Vitest Testing Framework
- [ ] Install Vitest and related packages
- [ ] Configure vitest.config.ts
- [ ] Set up coverage reporting (c8 or v8)
- [ ] Create test utilities directory
- [ ] Add sample test to verify setup

**Subagent**: `test-infrastructure-agent`

**Acceptance**: `npm test` runs and reports correctly

### Database Schema Design
- [ ] Design conversations table schema
- [ ] Design messages table schema
- [ ] Design learnings table schema
- [ ] Design job_queue table schema
- [ ] Design sanitization_log table schema
- [ ] Design uploads table schema

**Subagent**: `database-schema-agent`

**Acceptance**: Schema documented with all tables, columns, indexes, foreign keys

### Migration System
- [ ] Install migration library (kysely or knex)
- [ ] Create migration runner
- [ ] Implement up/down migration functions
- [ ] Create initial migration (001_create_tables.ts)
- [ ] Test migrations are reversible

**Acceptance**: Migrations run forward and backward successfully

### Test Utilities
- [ ] Create test database factory
- [ ] Create test data builders
- [ ] Create assertion helpers
- [ ] Create mock factories
- [ ] Document test utilities

**Acceptance**: Test utilities available and documented

## Dependencies

None (this is the foundation phase)

## Deliverables

1. `tsconfig.json` with strict settings
2. `vitest.config.ts` configured
3. `.eslintrc.json` and `.prettierrc`
4. Database schema documentation
5. Migration system with initial migration
6. Test utilities in `/tests/utils/`

## Success Criteria

- ✅ TypeScript compiles with no errors
- ✅ Vitest runs and reports coverage
- ✅ ESLint passes with strict rules
- ✅ Database migrations work bidirectionally
- ✅ Test utilities ready for use
- ✅ Documentation complete

## Testing Strategy

- Unit tests for migration runner
- Integration tests for database setup
- Test coverage for utilities

## Related Documents

- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
- [Database Schema Architecture](../architecture/architecture-database-schema-2025-01-16.md)
````

## File: plans/plan-phase-1-tasks-2025-01-16.md
````markdown
# Phase 1: Event Capture Tasks

> Hook implementation, event collection, and persistent queue

---
title: Phase 1 Event Capture Tasks
category: plan
date: 2025-01-16
status: active
tags: [phase-1, hooks, events, queue]
---

## Goal

Capture all Claude Code interactions via hooks without blocking the user, with fast pre-sanitization before persistence.

## Duration

3-4 days

## Tasks

### Event Schema Design
- [ ] Define event payload schema (correlation IDs, timestamps, versioning)
- [ ] Define message ordering strategy
- [ ] Define idempotency keys (dedupe strategy)
- [ ] Design event versioning for future compatibility
- [ ] Document schema with TypeScript types

**Acceptance**: Schema documented and typed

### UserPromptSubmit Hook
- [ ] Create `.claude/hooks/userPromptSubmit.sh` script
- [ ] Implement fast pre-sanitizer (< 50ms, removes obvious PII)
- [ ] Capture user prompt with correlation ID
- [ ] Implement error handling (never block user)
- [ ] Add performance monitoring
- [ ] Test hook execution time < 100ms (p95)

**Subagent**: `hook-developer-agent`

**Acceptance**: Hook runs < 100ms, captures prompts, never blocks UI

### Stop Hook
- [ ] Create `.claude/hooks/stop.sh` script
- [ ] Implement fast pre-sanitizer
- [ ] Capture agent response with correlation ID
- [ ] Handle partial responses and tool calls
- [ ] Add error handling
- [ ] Test hook execution time < 100ms (p95)

**Acceptance**: Hook runs < 100ms, captures responses, never blocks UI

### Fast Pre-Sanitizer
- [ ] Implement rule-based PII removal (API keys, obvious paths, emails)
- [ ] Ensure < 50ms execution for typical payloads
- [ ] Redact with [REDACTED_TYPE] format
- [ ] Log what was pre-sanitized (audit trail)
- [ ] Unit test with common PII patterns

**Acceptance**: Pre-sanitization < 50ms, removes obvious PII before persistence

### Event Collector
- [ ] Aggregate events into conversation sessions
- [ ] Implement correlation ID tracking
- [ ] Handle out-of-order messages
- [ ] Deduplicate events by idempotency key
- [ ] Handle conversation boundaries

**Subagent**: `event-collector-agent`

**Acceptance**: Events properly grouped into conversations

### Persistent Queue
- [ ] Implement SQLite-based event queue
- [ ] Store pre-sanitized events only (no raw data persisted)
- [ ] Implement queue operations (enqueue, dequeue, peek)
- [ ] Add backpressure handling (disk full, DB unavailable)
- [ ] Implement graceful degradation (in-memory fallback)
- [ ] Test persistence across restarts

**Subagent**: `queue-system-agent`

**Acceptance**: Queue persists pre-sanitized events, handles backpressure

### Cross-Platform Support
- [ ] Test hooks on macOS
- [ ] Test hooks on Linux (if applicable)
- [ ] Test hooks on Windows (if applicable)
- [ ] Document installation steps per OS
- [ ] Handle permission requirements

**Acceptance**: Hooks work on primary development OS

### Hook Failure Modes
- [ ] Implement circuit-breaker for persistent failures
- [ ] Add silent retry for transient errors
- [ ] Create offline mode (queue in memory until reconnected)
- [ ] Log failures without blocking
- [ ] Test failure scenarios

**Acceptance**: Failures handled gracefully, never block user

## Dependencies

- Phase 0: Database schema, TypeScript setup

## Deliverables

1. `.claude/hooks/userPromptSubmit.sh`
2. `.claude/hooks/stop.sh`
3. Fast pre-sanitizer implementation
4. Event collector implementation
5. Persistent queue implementation
6. Hook performance benchmarks
7. Installation guide for hooks

## Success Criteria

- ✅ Hooks execute < 100ms (p95)
- ✅ Pre-sanitization removes obvious PII < 50ms
- ✅ Events persisted with correlation IDs
- ✅ No user-blocking errors
- ✅ Complete conversation capture
- ✅ Queue handles backpressure
- ✅ Idempotent event processing (no duplicates)
- ✅ No plaintext raw data persisted on disk

## Testing Strategy

- Unit tests for event collector
- Unit tests for queue operations
- Performance tests for hooks (p95, p99 latency)
- Integration tests for end-to-end flow
- Chaos tests (kill process mid-write)

## Related Documents

- [Hooks & Event Capture Architecture](../architecture/architecture-hooks-event-capture-2025-01-16.md)
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
````

## File: plans/plan-phase-2-tasks-2025-01-16.md
````markdown
# Phase 2: Sanitization Pipeline Tasks

> PII detection and removal with rule-based + AI hybrid approach

---
title: Phase 2 Sanitization Pipeline Tasks
category: plan
date: 2025-01-16
status: active
tags: [phase-2, sanitization, pii, privacy, security]
---

## Goal

Remove ALL PII before final database storage using hybrid sanitization (rules + AI) with measurable precision/recall metrics.

## Duration

7-10 days (includes gold dataset creation)

## Tasks

### Gold Dataset Creation
- [ ] Build or adopt labeled PII dataset (1000+ examples)
- [ ] Include categories: email, phone, IP, path, key, URL, JWT, name
- [ ] Cover multiple OS path patterns (macOS, Linux, Windows)
- [ ] Include international formats (phone, addresses)
- [ ] Include adversarial examples (bypass attempts)
- [ ] Label each example with PII type

**Subagent**: `dataset-builder-agent`

**Acceptance**: 1000+ labeled examples across 8+ PII categories

### Rule-Based PII Detector
- [ ] Implement regex patterns for emails
- [ ] Implement regex for phone numbers (multiple formats)
- [ ] Implement regex for IPv4 and IPv6 addresses
- [ ] Implement regex for file paths (OS-specific patterns)
- [ ] Implement regex for API keys (AWS, GCP, OpenAI, GitHub, Slack)
- [ ] Implement regex for JWTs and URL tokens
- [ ] Implement regex for common environment variable patterns
- [ ] Define redaction format: [REDACTED_EMAIL], [REDACTED_PATH], etc.
- [ ] Optimize for < 10ms execution

**Subagent**: `rule-sanitizer-agent`

**Acceptance**:
- Precision ≥ 98% per category on gold dataset
- Execution < 10ms for typical payloads

### AI-Powered Sanitizer
- [ ] Integrate Claude API for context-aware sanitization
- [ ] Define prompt templates (temperature=0 for determinism)
- [ ] Implement fallback to rules-only if API unavailable
- [ ] Distinguish names from variable names
- [ ] Handle company-specific terminology
- [ ] Set max tokens and timeout limits
- [ ] Cache API responses for identical inputs

**Subagent**: `ai-sanitizer-agent`

**Acceptance**:
- Recall ≥ 95% per category on gold dataset
- Handles context ambiguity
- Fallback working

### Hybrid Validation Pipeline
- [ ] Combine rule-based + AI results
- [ ] Implement consensus logic (union of redactions)
- [ ] Create audit log of all redactions
- [ ] Track which method detected each PII item
- [ ] Implement override mechanism for false positives
- [ ] Ensure deterministic output

**Subagent**: `sanitization-pipeline-agent`

**Acceptance**: Combined precision ≥ 98%, recall ≥ 95%

### Scoring Harness
- [ ] Implement precision calculation per PII category
- [ ] Implement recall calculation per PII category
- [ ] Implement F1 score per category
- [ ] Generate per-category reports
- [ ] Set acceptance thresholds per category
- [ ] Create continuous evaluation suite

**Acceptance**: Automated scoring against gold dataset

### Chain-of-Thought Exclusion
- [ ] Identify and exclude chain-of-thought from all storage
- [ ] Document policy: never store thinking processes
- [ ] Implement detection of thinking tags
- [ ] Test exclusion is complete
- [ ] Update schema to not include thinking fields

**Acceptance**: Zero chain-of-thought stored

### Adversarial Testing
- [ ] Create adversarial test suite (prompt injection, evasion)
- [ ] Test obfuscated PII (base64, hex, URL-encoded)
- [ ] Test context manipulation attempts
- [ ] Test delimiter confusion
- [ ] Document vulnerabilities found

**Acceptance**: Adversarial suite passes, vulnerabilities documented

### Logging Redaction
- [ ] Implement log redaction middleware
- [ ] Ensure no PII in debug/error logs
- [ ] Test log output for PII leaks
- [ ] Document safe logging practices

**Acceptance**: No PII in any log outputs

## Dependencies

- Phase 1: Events to sanitize

## Deliverables

1. Gold PII dataset (1000+ labeled examples)
2. Rule-based sanitizer with 20+ patterns
3. AI-powered sanitizer with prompt templates
4. Hybrid pipeline orchestrator
5. Scoring harness with per-category metrics
6. Adversarial test suite
7. Audit logging system

## Success Criteria

- ✅ Precision ≥ 98% per PII category
- ✅ Recall ≥ 95% per PII category
- ✅ Sanitization < 2s per conversation
- ✅ No plaintext raw data on disk (pre-sanitized before persistence)
- ✅ Chain-of-thought excluded from all storage
- ✅ Audit log tracking all redactions
- ✅ Adversarial test suite passes
- ✅ No PII in logs

## PII Categories Covered

1. **Emails**: Standard formats, obfuscated variants
2. **Phone Numbers**: US, international, with/without formatting
3. **IP Addresses**: IPv4, IPv6, with ports
4. **File Paths**: Absolute paths with usernames (all OSes)
5. **API Keys**: AWS, GCP, OpenAI, GitHub, Slack, generic patterns
6. **URLs with Tokens**: Query params, bearer tokens, JWTs
7. **Environment Variables**: With secret values
8. **Names**: Person names (context-aware, not variable names)

## Testing Strategy

- Unit tests for each regex pattern
- Unit tests for AI sanitizer with mocked API
- Integration tests for hybrid pipeline
- Performance tests (< 2s per conversation)
- Precision/recall tests against gold dataset
- Adversarial tests for evasion attempts
- Chaos tests (API unavailable, rate limited)

## Related Documents

- [Sanitization Pipeline Architecture](../architecture/architecture-sanitization-pipeline-2025-01-16.md)
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
- [Privacy ADR](../decisions/decision-sanitize-before-storage-2025-01-16.md)
````

## File: plans/plan-phase-3-tasks-2025-01-16.md
````markdown
# Phase 3: Database & Storage Tasks

> Repository pattern, query optimization, and data persistence

---
title: Phase 3 Database & Storage Tasks
category: plan
date: 2025-01-16
status: active
tags: [phase-3, database, storage, sqlite, migrations]
---

## Goal

Persist sanitized data with ACID guarantees, fast queries, and reversible migrations.

## Duration

2-3 days

## Tasks

### Repository Pattern Implementation
- [ ] Create ConversationRepository
- [ ] Create MessageRepository
- [ ] Create LearningRepository
- [ ] Create JobQueueRepository
- [ ] Create UploadRepository
- [ ] Implement common CRUD operations
- [ ] Add transaction support

**Subagent**: `repository-agent`

**Acceptance**: All repositories implemented with typed interfaces

### Conversation Table
- [ ] Implement create conversation
- [ ] Implement find conversation by ID
- [ ] Implement list conversations with pagination
- [ ] Implement update conversation metadata
- [ ] Implement soft delete (if needed)
- [ ] Add indexes for common queries

**Acceptance**: All operations < 100ms

### Messages Table
- [ ] Implement create message
- [ ] Implement find messages by conversation ID
- [ ] Implement message ordering (by timestamp)
- [ ] Implement foreign key constraints to conversations
- [ ] Add indexes for conversation_id, timestamp

**Acceptance**: Foreign keys enforced, queries < 100ms

### Learnings Table
- [ ] Implement create learning
- [ ] Implement find learning by ID
- [ ] Implement search learnings (text, category, tags)
- [ ] Implement quality scoring filter (confidence ≥ threshold)
- [ ] Add full-text search indexes
- [ ] Add category and tag indexes

**Acceptance**: Search queries < 100ms

### Job Queue Table
- [ ] Implement enqueue job
- [ ] Implement dequeue job (with locking)
- [ ] Implement update job status
- [ ] Implement retry tracking
- [ ] Add indexes for status, priority, created_at

**Acceptance**: Queue operations atomic and < 50ms

### Migration Versioning
- [ ] Implement migration version tracking table
- [ ] Create migration 001: initial tables
- [ ] Create migration 002: indexes
- [ ] Create migration 003: audit tables
- [ ] Test rollback for each migration
- [ ] Document migration process

**Acceptance**: Migrations reversible, version tracked

### Query Optimization
- [ ] Enable WAL mode for SQLite
- [ ] Create indexes for all foreign keys
- [ ] Create indexes for common filters (status, category, created_at)
- [ ] Add full-text search index for learnings
- [ ] Test query performance with realistic dataset (10k+ rows)
- [ ] Benchmark and document query times

**Subagent**: `query-optimization-agent`

**Acceptance**: All queries < 100ms on 10k+ dataset

### ACID Compliance Testing
- [ ] Test concurrent writers don't corrupt data
- [ ] Test foreign key constraints enforced
- [ ] Test transactions rollback on error
- [ ] Test no orphan messages after conversation delete
- [ ] Test isolation levels

**Acceptance**: All ACID tests pass

### Data Retention Policies
- [ ] Implement purge old conversations (configurable retention)
- [ ] Implement delete conversation with cascade
- [ ] Implement right-to-delete support
- [ ] Implement vacuum schedule for SQLite
- [ ] Document retention policies

**Acceptance**: Deletion works correctly, vacuum reduces DB size

## Dependencies

- Phase 0: Database schema
- Phase 2: Sanitized data to store

## Deliverables

1. Repository implementations for all tables
2. Migration scripts (001-003+)
3. Query optimization indexes
4. ACID compliance test suite
5. Data retention utilities
6. Performance benchmark results

## Success Criteria

- ✅ All queries < 100ms
- ✅ ACID compliance verified (concurrent writes, FK constraints)
- ✅ Migrations reversible
- ✅ WAL mode enabled
- ✅ No orphan records
- ✅ Full-text search working for learnings

## Testing Strategy

- Unit tests for each repository method
- Integration tests for cross-table operations
- Concurrent writer tests
- Foreign key constraint tests
- Performance tests with 10k+ rows
- Migration rollback tests

## Related Documents

- [Database Schema Architecture](../architecture/architecture-database-schema-2025-01-16.md)
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
````

## File: plans/plan-phase-4-tasks-2025-01-16.md
````markdown
# Phase 4: Async Processing Tasks

> Job queue, workers, idempotency, and crash recovery

---
title: Phase 4 Async Processing Tasks
category: plan
date: 2025-01-16
status: active
tags: [phase-4, async, queue, workers, reliability]
---

## Goal

Implement reliable async processing with idempotent jobs, retry logic, and crash recovery.

## Duration

5-7 days

## Tasks

### Job Queue Implementation
- [ ] Implement SQLite-based job queue
- [ ] Add job types (sanitize, extract_learning, mine_upload)
- [ ] Implement priority levels
- [ ] Add job status tracking (pending, in_progress, completed, failed)
- [ ] Implement advisory locks for job claiming
- [ ] Add job visibility timeout

**Subagent**: `job-queue-agent`

**Acceptance**: Jobs enqueued atomically, no duplicate processing

### Job Idempotency
- [ ] Design dedupe key strategy (conversation_id + job_type)
- [ ] Implement idempotency checks before execution
- [ ] Test duplicate job submissions are ignored
- [ ] Implement outbox pattern for uploads
- [ ] Document idempotency guarantees

**Acceptance**: Duplicate jobs don't execute twice

### Worker Process Architecture
- [ ] Create worker process framework
- [ ] Implement graceful startup
- [ ] Implement graceful shutdown (wait for in-flight jobs)
- [ ] Add job checkpoint/resume on shutdown
- [ ] Implement worker heartbeat
- [ ] Support multiple worker instances

**Subagent**: `worker-agent`

**Acceptance**: Workers start/stop cleanly, no job loss

### Retry Logic
- [ ] Implement exponential backoff (1s, 2s, 4s, 8s, ...)
- [ ] Set max retry count (3 attempts)
- [ ] Add jitter to prevent retry storms
- [ ] Track retry count per job
- [ ] Implement retry cap per time window

**Acceptance**: Failed jobs retry with backoff, max 3 attempts

### Dead Letter Queue
- [ ] Implement DLQ for permanently failed jobs
- [ ] Add failure reason tracking
- [ ] Implement manual review workflow
- [ ] Add re-queue from DLQ capability
- [ ] Monitor DLQ size

**Acceptance**: Failed jobs moved to DLQ after max retries

### Crash Recovery
- [ ] Test worker crash mid-job
- [ ] Implement job timeout and re-queue
- [ ] Test DB locked scenario
- [ ] Test disk full scenario
- [ ] Test power loss scenario (via simulation)
- [ ] Verify no data loss on crash

**Acceptance**: All crash scenarios recover gracefully

### Concurrency Control
- [ ] Implement advisory locks in SQLite
- [ ] Test concurrent workers don't claim same job
- [ ] Implement worker registration/deregistration
- [ ] Add worker health checks
- [ ] Test scaling to N workers

**Acceptance**: Concurrent workers safe, no double-processing

### Job Monitoring
- [ ] Track jobs in each status
- [ ] Track average job duration by type
- [ ] Track retry rate
- [ ] Track DLQ size
- [ ] Expose metrics endpoint (simple JSON)

**Acceptance**: Metrics available for monitoring

## Dependencies

- Phase 3: Database for job storage

## Deliverables

1. Job queue implementation
2. Worker process framework
3. Retry logic with backoff
4. Dead letter queue
5. Crash recovery tests
6. Concurrency control system
7. Monitoring/metrics

## Success Criteria

- ✅ Jobs never lost (even on crash)
- ✅ Idempotent processing verified
- ✅ Graceful shutdown within 30s
- ✅ Exponential backoff working
- ✅ DLQ captures failed jobs
- ✅ Concurrent workers safe
- ✅ Crash recovery E2E tests pass

## Job Types

### sanitize_conversation
- Input: conversation_id, raw_events
- Output: sanitized_conversation_id
- Retry: Yes (max 3)
- Timeout: 30s

### extract_learning
- Input: sanitized_conversation_id
- Output: learning_ids[]
- Retry: Yes (max 3)
- Timeout: 60s

### mine_upload
- Input: learning_id
- Output: ipfs_cid, blockchain_tx_id
- Retry: Yes (max 3)
- Timeout: 120s

## Testing Strategy

- Unit tests for queue operations
- Unit tests for retry logic
- Integration tests for worker lifecycle
- Chaos tests (kill worker, DB unavailable, disk full)
- Concurrency tests (multiple workers)
- Idempotency tests (duplicate submissions)
- Load tests (1000+ jobs)

## Related Documents

- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
````

## File: plans/plan-phase-5-tasks-2025-01-16.md
````markdown
# Phase 5: Learning Extraction Tasks

> Extract valuable learnings with quality scoring and deduplication

---
title: Phase 5 Learning Extraction Tasks
category: plan
date: 2025-01-16
status: active
tags: [phase-5, learning, extraction, quality, dedup]
---

## Goal

Extract high-quality, reusable learnings from sanitized conversations with automated quality scoring and deduplication.

## Duration

6-8 days

## Tasks

### Conversation Analyzer
- [ ] Implement value detection (is conversation worth extracting from?)
- [ ] Analyze conversation length, complexity
- [ ] Detect problem-solving patterns
- [ ] Identify code implementations
- [ ] Filter trivial conversations
- [ ] Score conversation value (0-1)

**Subagent**: `learning-extractor-agent`

**Acceptance**: Accurate filtering of trivial vs valuable conversations

### Extraction Approach Design
- [ ] Design hybrid approach (rules + LLM + embeddings)
- [ ] Create extraction prompt templates
- [ ] Define metadata schema (tags, category, provenance, version)
- [ ] Set temperature=0 for deterministic extraction
- [ ] Implement chunking for long conversations

**Acceptance**: Extraction approach documented and implemented

### Category Extractors
- [ ] Implement pattern extractor (code patterns, architectures)
- [ ] Implement best_practice extractor
- [ ] Implement anti_pattern extractor
- [ ] Implement bug_fix extractor
- [ ] Implement optimization extractor
- [ ] Implement tool_usage extractor
- [ ] Implement workflow extractor
- [ ] Implement decision extractor

**Acceptance**: All 8 category extractors working

### Quality Scoring Algorithm
- [ ] Implement confidence scoring (0-1 scale)
- [ ] Define minimum information gain metric
- [ ] Filter learnings below confidence threshold (≥ 0.6)
- [ ] Ensure learning length ≥ 100 characters
- [ ] Validate learning has actionable content
- [ ] Score uniqueness vs existing learnings

**Subagent**: `quality-filter-agent`

**Acceptance**: 90% of high-scoring learnings rated "useful" by human reviewer

### Deduplication System
- [ ] Implement lexical similarity check
- [ ] Implement embedding-based similarity (cosine)
- [ ] Set similarity threshold (< 0.85 to keep)
- [ ] Implement windowed comparison (recent N learnings)
- [ ] Provide rejection reasons (duplicate of learning_id X)
- [ ] Test edge cases (near-duplicates, rephrasing)

**Acceptance**: No duplicate learnings (verified by similarity tests)

### Metadata & Provenance
- [ ] Capture conversation_id source
- [ ] Record timestamp of extraction
- [ ] Record sanitizer_version used
- [ ] Record extractor_version used
- [ ] Capture tags (auto-generated + manual)
- [ ] Record category

**Acceptance**: All metadata fields populated

### Negative Set (Anti-Patterns)
- [ ] Define examples of trivial learnings to reject
- [ ] Build negative test dataset
- [ ] Test extractor rejects trivial/generic content
- [ ] Document criteria for rejection
- [ ] Calibrate confidence threshold using negative set

**Acceptance**: Trivial learnings filtered out

### Human-in-the-Loop Review
- [ ] Create sample review interface (CLI or simple UI)
- [ ] Review random sample of extracted learnings (N=50)
- [ ] Rate each learning as useful/not useful
- [ ] Calculate approval rate (target ≥ 90%)
- [ ] Use feedback to tune thresholds

**Acceptance**: ≥ 90% approval rate on random sample

## Dependencies

- Phase 4: Async processing to run extraction jobs

## Deliverables

1. Conversation analyzer
2. 8 category-specific extractors
3. Quality scoring algorithm
4. Deduplication system
5. Metadata schema implementation
6. Negative test dataset
7. Human review process and results

## Success Criteria

- ✅ Confidence scores ≥ 0.6 for all extracted learnings
- ✅ No duplicate learnings (similarity < 0.85)
- ✅ Proper categorization (8 categories)
- ✅ Complete metadata (provenance, version, tags)
- ✅ 90% human approval rate on sample
- ✅ Trivial learnings filtered out
- ✅ Extraction < 5s per conversation

## Learning Categories

1. **pattern**: Code patterns, architectures, design patterns
2. **best_practice**: Recommended approaches, standards
3. **anti_pattern**: What to avoid, known pitfalls
4. **bug_fix**: Problem-solving strategies, debugging
5. **optimization**: Performance improvements, efficiency
6. **tool_usage**: How to use libraries, frameworks, tools
7. **workflow**: Development workflows, processes
8. **decision**: Architecture decisions, trade-offs

## Testing Strategy

- Unit tests for each category extractor
- Integration tests for end-to-end extraction
- Quality tests with known good/bad examples
- Deduplication tests with similar content
- Performance tests (< 5s per conversation)
- Human review on random sample (N=50)

## Related Documents

- [Learning Extraction Architecture](../architecture/architecture-learning-extraction-2025-01-16.md)
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
````

## File: plans/plan-phase-6-tasks-2025-01-16.md
````markdown
# Phase 6: MCP Server Tasks

> Model Context Protocol server for agent queries

---
title: Phase 6 MCP Server Tasks
category: plan
date: 2025-01-16
status: active
tags: [phase-6, mcp, server, query-interface]
---

## Goal

Implement MCP protocol server enabling Claude Code and other agents to query learnings with < 200ms latency.

## Duration

3-4 days

## Tasks

### MCP SDK Integration
- [ ] Install @modelcontextprotocol/sdk
- [ ] Create server configuration
- [ ] Implement server initialization
- [ ] Configure loopback binding (127.0.0.1 only)
- [ ] Add optional API key authentication
- [ ] Test server starts and responds to ping

**Subagent**: `mcp-protocol-agent`

**Acceptance**: MCP server starts and Claude Code can connect

### search_learnings Tool
- [ ] Implement text search across learning content
- [ ] Add category filter (pattern, best_practice, etc.)
- [ ] Add tag filter (multiple tags support)
- [ ] Add confidence threshold filter
- [ ] Implement pagination (limit, offset)
- [ ] Implement sorting (by date, confidence, relevance)
- [ ] Validate input parameters and bounds

**Acceptance**: Search returns relevant learnings < 200ms

### get_learning_by_id Tool
- [ ] Implement fetch learning by ID
- [ ] Return full learning with metadata
- [ ] Handle not found errors gracefully
- [ ] Validate ID format

**Acceptance**: Fetch by ID < 50ms

### get_learning_context Tool
- [ ] Fetch full conversation for a learning
- [ ] Return sanitized conversation messages
- [ ] Include provenance information
- [ ] Handle privacy (only sanitized data)

**Acceptance**: Context retrieval < 200ms

### Resource Endpoints
- [ ] Implement `context://learnings/recent` (latest N learnings)
- [ ] Implement `context://learnings/top-rated` (highest confidence)
- [ ] Implement `context://stats` (total learnings, categories, etc.)
- [ ] Add caching for resource endpoints
- [ ] Document resource schemas

**Acceptance**: Resources load < 100ms

### Authentication & Authorization
- [ ] Implement optional API key authentication
- [ ] Default to localhost-only binding (no auth needed)
- [ ] Add config for enabling auth
- [ ] Reject requests without valid auth (if enabled)
- [ ] Document security model

**Acceptance**: Auth enforced when enabled, bypassed on localhost

### Input Validation
- [ ] Validate all tool parameters
- [ ] Enforce bounds (max limit, offset ranges)
- [ ] Sanitize search queries (prevent injection)
- [ ] Reject malformed requests
- [ ] Return clear error messages

**Acceptance**: Invalid requests rejected gracefully

### Rate Limiting
- [ ] Implement simple rate limiter (N requests per minute)
- [ ] Prevent local DoS scenarios
- [ ] Return 429 Too Many Requests when exceeded
- [ ] Make limits configurable

**Acceptance**: Rate limiting prevents abuse

### Query Optimization
- [ ] Use database indexes for filters
- [ ] Implement result caching for common queries
- [ ] Optimize full-text search queries
- [ ] Test performance with 10k+ learnings
- [ ] Profile slow queries

**Subagent**: `query-optimization-agent`

**Acceptance**: All queries < 200ms on 10k+ dataset

### MCP Protocol Conformance
- [ ] Test against MCP SDK examples
- [ ] Verify tool schemas are valid
- [ ] Verify resource schemas are valid
- [ ] Test error handling conforms to spec
- [ ] Test streaming responses (if applicable)

**Acceptance**: MCP conformance tests pass

## Dependencies

- Phase 5: Learnings to query

## Deliverables

1. MCP server implementation
2. Three query tools (search, get by ID, get context)
3. Three resource endpoints (recent, top-rated, stats)
4. Authentication layer (optional)
5. Rate limiting system
6. Performance benchmarks

## Success Criteria

- ✅ MCP protocol conformance tests pass
- ✅ All queries < 200ms
- ✅ Invalid queries rejected gracefully
- ✅ Auth enforced when enabled
- ✅ Rate limiting prevents abuse
- ✅ Claude Code integration working
- ✅ No data leaks via MCP

## MCP Tools

### search_learnings
```typescript
{
  name: "search_learnings",
  description: "Search for learnings by text, category, or tags",
  inputSchema: {
    query: "string (optional)",
    category: "string (optional)",
    tags: "string[] (optional)",
    minConfidence: "number (optional, default 0.6)",
    limit: "number (optional, default 10, max 100)",
    offset: "number (optional, default 0)"
  }
}
```

### get_learning_by_id
```typescript
{
  name: "get_learning_by_id",
  description: "Fetch a specific learning by ID",
  inputSchema: {
    id: "string (required)"
  }
}
```

### get_learning_context
```typescript
{
  name: "get_learning_context",
  description: "Get full conversation context for a learning",
  inputSchema: {
    learningId: "string (required)"
  }
}
```

## MCP Resources

- `context://learnings/recent`: Latest 50 learnings
- `context://learnings/top-rated`: Top 50 by confidence
- `context://stats`: Network statistics (total, categories, etc.)

## Testing Strategy

- Unit tests for each tool
- Integration tests for MCP protocol
- Performance tests (concurrent queries)
- Security tests (injection, auth bypass)
- Load tests (N concurrent clients)
- Conformance tests against MCP spec

## Related Documents

- [MCP Server Architecture](../architecture/architecture-mcp-server-2025-01-16.md)
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
````

## File: plans/plan-phase-7-tasks-2025-01-16.md
````markdown
# Phase 7: Mining & Upload Tasks (MVP+)

> IPFS upload and blockchain integration with token tracking

---
title: Phase 7 Mining & Upload Tasks (MVP+)
category: plan
date: 2025-01-16
status: active
tags: [phase-7, ipfs, blockchain, mining, upload, mvp-plus]
---

## Goal

Upload quality learnings to IPFS and anchor CIDs on blockchain with token reward tracking.

## Duration

4-10 days (depending on blockchain complexity)

**Note**: This phase is marked as MVP+ and can be deferred or simplified for initial MVP release.

## Tasks

### IPFS Integration Decision
- [ ] Choose: self-hosted IPFS node vs pinning provider
- [ ] If provider: select service (Pinata, web3.storage, etc.)
- [ ] If self-hosted: set up IPFS node
- [ ] Configure pinning strategy and SLA
- [ ] Test IPFS node connectivity
- [ ] Document IPFS setup

**Acceptance**: IPFS endpoint available and tested

### IPFS Client Implementation
- [ ] Install IPFS client library
- [ ] Implement content upload function
- [ ] Implement CID generation
- [ ] Implement content retrieval (verify upload)
- [ ] Add retry logic for failed uploads
- [ ] Test upload/retrieval flow

**Subagent**: `ipfs-integration-agent`

**Acceptance**: Content uploads to IPFS and is retrievable

### Content Preparation
- [ ] Format learning for IPFS (JSON structure)
- [ ] Include metadata (category, tags, confidence, timestamp)
- [ ] Exclude any remaining sensitive data
- [ ] Add license information
- [ ] Add provenance data (sanitizer version, etc.)
- [ ] Compress content if needed

**Acceptance**: Content properly formatted for public sharing

### Blockchain Network Selection
- [ ] Choose blockchain (Ethereum testnet, Celestia, etc.)
- [ ] Set up RPC provider access
- [ ] Get testnet tokens from faucet
- [ ] Document network choice and rationale

**Acceptance**: Blockchain network decided and access configured

### Wallet & Key Management
- [ ] Implement local wallet generation
- [ ] Store keys securely (OS keychain)
- [ ] Implement backup/export functionality
- [ ] Add passphrase protection
- [ ] Document key management security
- [ ] Require explicit user action to enable

**Subagent**: `blockchain-agent`

**Acceptance**: Keys generated, stored securely, backupable

### CID Anchoring Strategy
- [ ] Decide: simple CID registry vs full smart contract
- [ ] If registry: implement off-chain registry with on-chain hashes
- [ ] If contract: design contract, deploy to testnet
- [ ] Implement transaction submission
- [ ] Implement confirmation polling
- [ ] Add gas estimation

**Acceptance**: CIDs anchored on-chain with confirmation

### Smart Contract (Optional - if time permits)
- [ ] Design learning registry contract
- [ ] Implement reward calculation logic
- [ ] Add quality score submission
- [ ] Deploy to testnet
- [ ] Audit contract (basic security review)
- [ ] Document contract interface

**Acceptance**: Contract deployed, tested, and documented

### Token Reward Tracking
- [ ] Implement reward calculation (placeholder or actual)
- [ ] Track upload status (pending, confirmed, rewarded)
- [ ] Store transaction IDs
- [ ] Query reward balance
- [ ] Document tokenomics (even if simplified)

**Acceptance**: Rewards tracked in database

### User Controls
- [ ] Implement opt-in for uploads (default off)
- [ ] Add manual approval gate before each upload
- [ ] Implement "local-only" mode
- [ ] Add upload pause/resume
- [ ] Document user privacy controls

**Acceptance**: User has full control over uploads

### Upload Verification
- [ ] Verify CID retrievable via 2+ IPFS gateways
- [ ] Verify on-chain record exists
- [ ] Verify N confirmed blocks
- [ ] Implement status monitoring
- [ ] Add failure alerts

**Acceptance**: Uploads verified across infrastructure

### Retry & Error Handling
- [ ] Implement retry for IPFS failures
- [ ] Implement retry for blockchain failures
- [ ] Handle insufficient gas scenarios
- [ ] Handle rate limiting
- [ ] Add exponential backoff
- [ ] Move to DLQ after max retries

**Acceptance**: Failures handled gracefully with retries

## Dependencies

- Phase 5: Quality learnings to upload

## Deliverables

1. IPFS client integration
2. Blockchain integration
3. Wallet/key management system
4. CID anchoring implementation
5. Token reward tracking
6. User control interface
7. Upload verification system

## Success Criteria

- ✅ Content uploads to IPFS successfully
- ✅ CID retrievable via 2+ gateways
- ✅ On-chain record confirmed (N blocks)
- ✅ Keys stored securely (OS keychain)
- ✅ Manual approval gate working
- ✅ User controls for opt-in/out
- ✅ Retry logic handles failures

## MVP Simplification Options

If timeline is tight, simplify Phase 7 to:

### Minimal MVP+
- IPFS upload only (via pinning provider)
- Off-chain CID registry (no blockchain)
- Placeholder token tracking
- Defer smart contracts to post-MVP

### Core MVP (No Phase 7)
- Skip network upload entirely
- Focus on local capture, sanitization, learning extraction, MCP
- Add Phase 7 in Month 2

## Testing Strategy

- Integration tests for IPFS upload/retrieval
- Integration tests for blockchain transactions
- Security tests for key management
- Chaos tests (IPFS unavailable, blockchain RPC down)
- Manual approval flow tests
- Retry logic tests

## Related Documents

- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
- [Original User Vision](./plan-original-user-vision-2025-01-16.md)
````

## File: plans/plan-sanitization-architecture-doc-2025-01-16.md
````markdown
# Plan: Sanitization Pipeline Architecture Document

**Date**: 2025-01-16
**Status**: Ready for GPT-5 Review
**Priority**: CRITICAL - Blocks implementation
**Target Doc**: `docs/architecture/architecture-sanitization-pipeline-2025-01-16.md`

---

## Context

This plan outlines the complete architecture document for the 2-stage sanitization pipeline. This is identified as the **MOST CRITICAL MISSING DOCUMENT** by external reviews (GPT-5 holistic + Gemini 2.5 Pro).

**Why This Matters**:
- Privacy is the #1 architectural principle
- Zero-trust PII handling requires perfect sanitization
- Contradiction exists: some docs show raw persistence vs STANDARDS.md mandate
- Implementation cannot start without this spec

**Critical Requirements from STANDARDS.md**:
1. Privacy flow: Pre-sanitize in hook (<50ms) → Persist ONLY sanitized → Optional AI validation (async)
2. NEVER persist raw content to disk
3. Schema: Write to `messages` table (no events/event_queue tables)
4. Redaction format: `[REDACTED_API_KEY]`, `[REDACTED_EMAIL]`, etc.
5. Performance: Fast sanitization <50ms, AI validation <2s

---

## Document Structure

### 1. Overview Section
**Purpose**: Executive summary and architectural principles

**Contents**:
- System purpose: Zero-trust PII protection before storage
- Core principle: "PII never touches disk"
- Privacy guarantee: If raw data never persists, no PII leaks possible
- Performance targets: <50ms synchronous, <2s async
- References to ADR-004 and STANDARDS.md

**ASCII Diagram - High-Level Flow**:
```
User Input → Hook Receives Event (in-memory)
           ↓
           Fast Pre-Sanitization (<50ms, rule-based)
           ↓
           Persist ONLY Sanitized Content (messages table)
           ↓
           Optional AI Validation (async, <2s, context-aware)
```

### 2. Architecture Components

#### 2.1 Stage 1: Fast Pre-Sanitization (Synchronous)

**Purpose**: Lightning-fast rule-based redaction in hook before disk write

**Performance Budget**:
- Target: <50ms p95
- Maximum: <80ms p99
- Method: Compiled regex patterns only
- Constraint: Must complete before database write

**Component Diagram**:
```
┌─────────────────────────────────────────────────┐
│          UserPromptSubmit Hook                   │
│  ┌───────────────────────────────────────────┐  │
│  │ 1. Receive event (stdin JSON)             │  │
│  │ 2. Extract content string                 │  │
│  │ 3. Run fastSanitize(content)              │  │
│  │    - Apply regex patterns sequentially    │  │
│  │    - Replace matches with redaction tags  │  │
│  │    - Track detections for audit           │  │
│  │ 4. Write sanitized to messages table      │  │
│  │ 5. Enqueue AI validation job (async)      │  │
│  │ 6. Return success (<50ms total)           │  │
│  └───────────────────────────────────────────┘  │
└─────────────────────────────────────────────────┘
```

**PII Taxonomy - Stage 1 (Rule-Based)**:
| Category | Pattern | Redaction Format | Example |
|----------|---------|------------------|---------|
| API Keys | `sk_live_`, `pk_live_`, `api_key_`, `AKIA`, etc. | `[REDACTED_API_KEY]` | `sk_live_abc123` → `[REDACTED_API_KEY]` |
| Email | RFC 5322 compliant regex | `[REDACTED_EMAIL]` | `user@example.com` → `[REDACTED_EMAIL]` |
| Phone | E.164 + common formats | `[REDACTED_PHONE]` | `+1-555-123-4567` → `[REDACTED_PHONE]` |
| IP Address | IPv4/IPv6 patterns | `[REDACTED_IP]` | `192.168.1.1` → `[REDACTED_IP]` |
| File Paths | `/Users/`, `/home/`, `C:\Users\` | `[REDACTED_PATH]` | `/Users/john/file.txt` → `[REDACTED_PATH]` |
| URLs with Tokens | `?token=`, `?api_key=`, `?auth=` | `[REDACTED_URL]` | `https://api.com?token=abc` → `[REDACTED_URL]` |
| JWT Tokens | `eyJ...` pattern | `[REDACTED_JWT]` | `eyJhbGciOi...` → `[REDACTED_JWT]` |
| SSH/PEM Keys | `-----BEGIN` blocks | `[REDACTED_PRIVATE_KEY]` | `-----BEGIN RSA...` → `[REDACTED_PRIVATE_KEY]` |
| Credit Cards | Luhn algorithm + common patterns | `[REDACTED_CREDIT_CARD]` | `4532-1234-5678-9010` → `[REDACTED_CREDIT_CARD]` |
| SSN | `XXX-XX-XXXX` pattern | `[REDACTED_SSN]` | `123-45-6789` → `[REDACTED_SSN]` |

**Regex Pattern Library** (TypeScript):
```typescript
// Fast, compiled patterns for Stage 1
export const FAST_PATTERNS = {
  API_KEY: /\b(sk_live_|pk_live_|sk_test_|pk_test_|api_key_|apikey=)[A-Za-z0-9_-]{20,}\b/gi,
  AWS_KEY: /\b(AKIA[0-9A-Z]{16})\b/g,
  EMAIL: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g,
  PHONE: /\b(\+\d{1,3}[-.]?)?\(?\d{3}\)?[-.]?\d{3}[-.]?\d{4}\b/g,
  IP_V4: /\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b/g,
  FILE_PATH_UNIX: /(?:\/Users\/[^\/\s]+\/[^\s]*|\/home\/[^\/\s]+\/[^\s]*)/g,
  FILE_PATH_WIN: /C:\\Users\\[^\\]+\\[^\s]*/g,
  URL_WITH_TOKEN: /https?:\/\/[^\s]+[?&](token|key|auth|api_key|access_token)=[^\s&]+/gi,
  JWT: /eyJ[A-Za-z0-9_-]{10,}\.[A-Za-z0-9_-]{10,}\.[A-Za-z0-9_-]{10,}/g,
  SSH_KEY: /-----BEGIN\s+(?:RSA\s+)?PRIVATE\s+KEY-----[\s\S]+?-----END\s+(?:RSA\s+)?PRIVATE\s+KEY-----/g,
  CREDIT_CARD: /\b(?:\d{4}[-\s]?){3}\d{4}\b/g,
  SSN: /\b\d{3}-\d{2}-\d{4}\b/g
} as const;
```

**Implementation - fastSanitize()**:
```typescript
import { performance } from 'node:perf_hooks';

interface SanitizationResult {
  sanitized: string;
  detections: Detection[];
  durationMs: number;
}

interface Detection {
  category: string;
  originalLength: number;
  position: number;
}

function fastSanitize(content: string): SanitizationResult {
  const start = performance.now();
  const detections: Detection[] = [];
  let sanitized = content;

  // Apply patterns in priority order (most critical first)
  const patterns = [
    { name: 'API_KEY', regex: FAST_PATTERNS.API_KEY, format: '[REDACTED_API_KEY]' },
    { name: 'AWS_KEY', regex: FAST_PATTERNS.AWS_KEY, format: '[REDACTED_AWS_KEY]' },
    { name: 'JWT', regex: FAST_PATTERNS.JWT, format: '[REDACTED_JWT]' },
    { name: 'SSH_KEY', regex: FAST_PATTERNS.SSH_KEY, format: '[REDACTED_PRIVATE_KEY]' },
    { name: 'CREDIT_CARD', regex: FAST_PATTERNS.CREDIT_CARD, format: '[REDACTED_CREDIT_CARD]' },
    { name: 'SSN', regex: FAST_PATTERNS.SSN, format: '[REDACTED_SSN]' },
    { name: 'EMAIL', regex: FAST_PATTERNS.EMAIL, format: '[REDACTED_EMAIL]' },
    { name: 'PHONE', regex: FAST_PATTERNS.PHONE, format: '[REDACTED_PHONE]' },
    { name: 'IP_V4', regex: FAST_PATTERNS.IP_V4, format: '[REDACTED_IP]' },
    { name: 'FILE_PATH_UNIX', regex: FAST_PATTERNS.FILE_PATH_UNIX, format: '[REDACTED_PATH]' },
    { name: 'FILE_PATH_WIN', regex: FAST_PATTERNS.FILE_PATH_WIN, format: '[REDACTED_PATH]' },
    { name: 'URL_WITH_TOKEN', regex: FAST_PATTERNS.URL_WITH_TOKEN, format: '[REDACTED_URL]' }
  ];

  for (const pattern of patterns) {
    sanitized = sanitized.replace(pattern.regex, (match, ...args) => {
      const offset = args[args.length - 2]; // Regex match offset
      detections.push({
        category: pattern.name,
        originalLength: match.length,
        position: offset
      });
      return pattern.format;
    });
  }

  const duration = performance.now() - start;

  // Log performance warning if over budget
  if (duration > 50) {
    console.warn(`[SANITIZE] Fast sanitization took ${duration}ms (budget: 50ms)`);
  }

  return {
    sanitized,
    detections,
    durationMs: duration
  };
}
```

**Error Handling**:
- If sanitization throws: Log error, write empty string to DB (fail-safe)
- If over time budget: Complete anyway, log warning, investigate pattern optimization
- Never block user: Hook must return within 100ms total (50ms sanitization + 20ms DB write + 30ms buffer)

#### 2.2 Stage 2: AI-Powered Validation (Asynchronous)

**Purpose**: Context-aware detection of PII that regex cannot catch

**Performance Budget**:
- Target: <2s p95 per conversation
- Method: Claude API call with specialized prompt
- Timing: Runs AFTER sanitized content persisted
- Non-blocking: User never waits for this

**Component Diagram**:
```
┌─────────────────────────────────────────────────┐
│          Async Worker Process                    │
│  ┌───────────────────────────────────────────┐  │
│  │ 1. Poll job_queue for sanitization jobs  │  │
│  │ 2. Fetch sanitized message from DB        │  │
│  │ 3. Call Claude API with validation prompt │  │
│  │ 4. Parse AI response for detections       │  │
│  │ 5. If PII found:                          │  │
│  │    - Re-sanitize with AI suggestions      │  │
│  │    - Update message in DB                 │  │
│  │    - Log to sanitization_log              │  │
│  │ 6. Mark job completed                     │  │
│  └───────────────────────────────────────────┘  │
└─────────────────────────────────────────────────┘
```

**PII Taxonomy - Stage 2 (AI-Detected)**:
| Category | Detection Method | Example |
|----------|------------------|---------|
| Person Names | NER + context | "John Smith shared his code" → "[REDACTED_PERSON] shared his code" |
| Organization Names | Context-aware | "Acme Corp's API key" → "[REDACTED_ORG]'s API key" |
| Location/Address | Geographic context | "123 Main St, Springfield" → "[REDACTED_ADDRESS]" |
| Custom Identifiers | Heuristic patterns | "employee-id-12345" → "[REDACTED_IDENTIFIER]" |
| Contextual Secrets | Semantic analysis | "my password is correcthorsebattery" → "my password is [REDACTED_PASSWORD]" |

**AI Validation Prompt Template**:
```
You are a privacy protection system. Analyze the following text for personally identifiable information (PII) that was not caught by rule-based sanitization.

Text (already pre-sanitized with [REDACTED_*] tags):
---
{sanitized_content}
---

Identify any remaining PII in these categories:
1. Person names (not variable names or code identifiers)
2. Organization/company names
3. Physical addresses
4. Custom identifiers (employee IDs, account numbers)
5. Contextual secrets (passwords mentioned in prose)
6. Any other sensitive information

For each detection, provide:
- Category
- Exact text to redact
- Start/end position
- Confidence (0-1)
- Reasoning (why this is PII vs legitimate content)

Output JSON:
{
  "detections": [
    {
      "category": "PERSON_NAME",
      "text": "John Smith",
      "start": 45,
      "end": 55,
      "confidence": 0.95,
      "reasoning": "Proper name in context 'John Smith shared', not a code variable"
    }
  ]
}
```

**AI Response Processing**:
```typescript
interface AIDetection {
  category: string;
  text: string;
  start: number;
  end: number;
  confidence: number;
  reasoning: string;
}

async function aiValidate(messageId: string, sanitizedContent: string): Promise<void> {
  const start = performance.now();

  try {
    // Call Claude API
    const response = await claudeClient.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 2000,
      temperature: 0, // Deterministic for privacy
      messages: [
        {
          role: 'user',
          content: AI_VALIDATION_PROMPT.replace('{sanitized_content}', sanitizedContent)
        }
      ]
    });

    // Parse detections
    const result = JSON.parse(response.content[0].text);
    const detections: AIDetection[] = result.detections || [];

    // Filter by confidence threshold
    const highConfidence = detections.filter(d => d.confidence >= 0.8);

    if (highConfidence.length > 0) {
      // Re-sanitize with AI detections
      let reSanitized = sanitizedContent;
      for (const detection of highConfidence.reverse()) { // Reverse to maintain positions
        const redactionTag = `[REDACTED_${detection.category}]`;
        reSanitized =
          reSanitized.slice(0, detection.start) +
          redactionTag +
          reSanitized.slice(detection.end);
      }

      // Update message in database
      await db.prepare(`
        UPDATE messages
        SET content = ?, ai_validated = 1, ai_detections = ?
        WHERE id = ?
      `).run(reSanitized, JSON.stringify(highConfidence), messageId);

      // Log to audit trail
      await db.prepare(`
        INSERT INTO sanitization_log (message_id, stage, detections, timestamp)
        VALUES (?, 'ai_validation', ?, ?)
      `).run(messageId, JSON.stringify(highConfidence), new Date().toISOString());
    } else {
      // No additional PII found
      await db.prepare(`
        UPDATE messages SET ai_validated = 1 WHERE id = ?
      `).run(messageId);
    }

    const duration = performance.now() - start;
    console.log(`[AI VALIDATE] Completed in ${duration}ms, found ${highConfidence.length} issues`);

  } catch (error) {
    console.error('[AI VALIDATE] Failed:', error);
    // Don't fail the message - it's already pre-sanitized
    await db.prepare(`
      UPDATE messages SET ai_validated = 0, ai_error = ? WHERE id = ?
    `).run(error.message, messageId);
  }
}
```

**Cost Management**:
- Only validate new conversations (not every message)
- Batch multiple messages per API call when possible
- Set max tokens to control cost
- Track spending via job_queue metadata

### 3. Data Flow & Integration

#### 3.1 Complete Privacy Flow

**End-to-End Diagram**:
```
┌─────────────────────────────────────────────────────────────────────┐
│ 1. User submits prompt in Claude Code                               │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 2. UserPromptSubmit Hook triggered                                  │
│    - Receives event via stdin (JSON)                                │
│    - Event contains: { role: 'user', content: 'raw text...' }       │
│    - Raw content ONLY in memory (never written to disk)             │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 3. Fast Pre-Sanitization (Stage 1)                                  │
│    const { sanitized, detections } = fastSanitize(event.content)    │
│    - Applies regex patterns                                         │
│    - Returns sanitized string + detection metadata                  │
│    - Duration: <50ms                                                │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 4. Persist ONLY Sanitized Content                                   │
│    INSERT INTO messages (id, conversation_id, role, content, ...)   │
│    VALUES (?, ?, 'user', sanitized, ...)                            │
│    - Raw content NEVER written to disk                              │
│    - Only sanitized string persisted                                │
│    - Duration: <20ms (WAL mode)                                     │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 5. Enqueue AI Validation Job (Async)                                │
│    INSERT INTO job_queue (type, payload, status)                    │
│    VALUES ('ai_sanitization_validation',                            │
│            JSON({ message_id }), 'queued')                          │
│    - Non-blocking                                                   │
│    - User interaction continues                                     │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 6. Hook Returns Success                                             │
│    - Total time: <100ms (50ms sanitize + 20ms DB + 30ms overhead)   │
│    - User never blocked                                             │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             │ (Meanwhile, async worker runs...)
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 7. Async Worker: AI Validation (Stage 2)                            │
│    - Polls job_queue for 'queued' jobs                              │
│    - Fetches sanitized message from messages table                  │
│    - Calls Claude API with validation prompt                        │
│    - If additional PII found: re-sanitize and update DB             │
│    - Marks job 'completed'                                          │
│    - Duration: <2s (non-blocking to user)                           │
└────────────────────────────┬────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 8. Final State: Fully Sanitized Message in DB                       │
│    - messages.content = fully sanitized text                        │
│    - messages.ai_validated = 1                                      │
│    - sanitization_log = audit trail                                 │
│    - GUARANTEE: No PII on disk                                      │
└─────────────────────────────────────────────────────────────────────┘
```

#### 3.2 Schema Integration (Per STANDARDS.md)

**Messages Table** (sanitized content only):
```sql
CREATE TABLE messages (
  id TEXT PRIMARY KEY,                 -- ULID
  conversation_id TEXT NOT NULL,       -- ULID
  role TEXT NOT NULL CHECK(role IN ('user', 'assistant')),
  content TEXT NOT NULL,               -- SANITIZED content (never raw)
  sequence INTEGER NOT NULL,

  -- Sanitization metadata
  pre_sanitized INTEGER DEFAULT 1,     -- Always 1 (all messages pre-sanitized)
  ai_validated INTEGER DEFAULT 0,      -- 1 after Stage 2 validation
  ai_detections TEXT,                  -- JSON array of AI-found issues
  ai_error TEXT,                       -- Error message if AI validation failed

  -- Timestamps
  created_at TEXT NOT NULL,            -- ISO-8601 UTC

  -- Constraints
  FOREIGN KEY (conversation_id) REFERENCES conversations(id)
);

CREATE INDEX idx_messages_conversation ON messages(conversation_id, sequence);
CREATE INDEX idx_messages_ai_pending ON messages(ai_validated) WHERE ai_validated = 0;
```

**Sanitization Log Table** (audit trail):
```sql
CREATE TABLE sanitization_log (
  id TEXT PRIMARY KEY,                 -- ULID
  message_id TEXT NOT NULL,
  stage TEXT NOT NULL CHECK(stage IN ('pre_sanitization', 'ai_validation')),
  detections TEXT NOT NULL,            -- JSON array of Detection objects
  timestamp TEXT NOT NULL,             -- ISO-8601 UTC

  FOREIGN KEY (message_id) REFERENCES messages(id)
);

CREATE INDEX idx_sanitization_log_message ON sanitization_log(message_id);
CREATE INDEX idx_sanitization_log_stage ON sanitization_log(stage, timestamp);
```

**Job Queue Table** (async processing):
```sql
CREATE TABLE job_queue (
  id TEXT PRIMARY KEY,                 -- ULID
  type TEXT NOT NULL CHECK(type IN ('ai_sanitization_validation', 'learning_extraction', 'network_upload')),
  payload TEXT NOT NULL,               -- JSON with job-specific data
  status TEXT NOT NULL CHECK(status IN ('queued', 'in_progress', 'completed', 'failed', 'dead_letter')),

  -- Retry handling
  attempts INTEGER DEFAULT 0,
  max_attempts INTEGER DEFAULT 3,
  error TEXT,

  -- Timestamps
  created_at TEXT NOT NULL,            -- ISO-8601 UTC
  started_at TEXT,
  completed_at TEXT,

  -- Scheduling
  scheduled_for TEXT,                  -- ISO-8601 UTC (for delayed jobs)
  priority INTEGER DEFAULT 5           -- 1 (high) to 10 (low)
);

CREATE INDEX idx_job_queue_status ON job_queue(status, scheduled_for, priority);
CREATE INDEX idx_job_queue_type ON job_queue(type, status);
```

**NO events or event_queue tables** - eliminated per STANDARDS.md

### 4. Performance Characteristics

#### 4.1 Performance Budgets

| Component | Budget | Measurement | Notes |
|-----------|--------|-------------|-------|
| Hook execution (total) | <100ms p95 | End-to-end UserPromptSubmit hook | Includes all stages |
| Fast sanitization | <50ms p95 | fastSanitize() function | Rule-based regex only |
| Database write | <20ms p95 | INSERT into messages | WAL mode enabled |
| AI validation | <2s p95 | Claude API call + processing | Async, non-blocking |
| Job queue poll | <100ms p95 | SELECT pending jobs | Worker loop |

#### 4.2 Benchmarking Strategy

**Unit Benchmarks**:
```typescript
// Benchmark each regex pattern individually
for (const [name, pattern] of Object.entries(FAST_PATTERNS)) {
  const iterations = 10000;
  const start = performance.now();

  for (let i = 0; i < iterations; i++) {
    TEST_CORPUS.replace(pattern, '[REDACTED]');
  }

  const avgMs = (performance.now() - start) / iterations;
  console.log(`Pattern ${name}: ${avgMs}ms per iteration`);

  assert(avgMs < 0.005, `Pattern ${name} too slow: ${avgMs}ms`); // 5μs max per pattern
}
```

**Integration Benchmarks**:
```typescript
// Benchmark full fastSanitize() with realistic data
const testCases = [
  { name: 'Small (100 chars)', content: '...' },
  { name: 'Medium (1KB)', content: '...' },
  { name: 'Large (10KB)', content: '...' },
  { name: 'Mixed PII', content: '...' }
];

for (const testCase of testCases) {
  const iterations = 1000;
  const durations: number[] = [];

  for (let i = 0; i < iterations; i++) {
    const { durationMs } = fastSanitize(testCase.content);
    durations.push(durationMs);
  }

  const p50 = percentile(durations, 0.5);
  const p95 = percentile(durations, 0.95);
  const p99 = percentile(durations, 0.99);

  console.log(`${testCase.name}: p50=${p50}ms, p95=${p95}ms, p99=${p99}ms`);

  assert(p95 < 50, `${testCase.name} p95 over budget: ${p95}ms`);
}
```

**End-to-End Benchmarks**:
```typescript
// Benchmark full hook execution (sanitize + DB write)
async function benchmarkHook(eventPayload: HookEvent) {
  const start = performance.now();

  // Simulate hook execution
  const { sanitized, detections } = fastSanitize(eventPayload.content);

  await db.prepare(`
    INSERT INTO messages (id, conversation_id, role, content, sequence, created_at, pre_sanitized)
    VALUES (?, ?, ?, ?, ?, ?, 1)
  `).run(
    ulid(),
    eventPayload.conversation_id,
    eventPayload.role,
    sanitized,
    eventPayload.sequence,
    new Date().toISOString()
  );

  const totalMs = performance.now() - start;
  return totalMs;
}

// Run 1000 iterations
const durations = await Promise.all(
  Array(1000).fill(0).map(() => benchmarkHook(generateTestEvent()))
);

const p95 = percentile(durations, 0.95);
console.log(`Hook p95: ${p95}ms`);
assert(p95 < 100, `Hook p95 over budget: ${p95}ms`);
```

### 5. Security & Privacy Guarantees

#### 5.1 Privacy Invariants

**Invariant 1: Raw Content Never Persists**
```
∀ event ∈ EventStream:
  IF event.content contains PII
  THEN messages.content = sanitize(event.content)
  AND sanitize(x) contains NO PII
```

**Verification**:
```typescript
// Privacy canary test
async function testPrivacyInvariant() {
  const piiContent = "My email is test@example.com and API key is sk_live_abc123xyz";

  // Simulate hook processing
  const { sanitized } = fastSanitize(piiContent);

  // Persist (simulating hook behavior)
  const messageId = ulid();
  await db.prepare(`
    INSERT INTO messages (id, conversation_id, role, content, sequence, created_at, pre_sanitized)
    VALUES (?, ?, ?, ?, ?, ?, 1)
  `).run(messageId, ulid(), 'user', sanitized, 1, new Date().toISOString());

  // Verify: Fetch from DB and check for PII
  const stored = await db.prepare(`SELECT content FROM messages WHERE id = ?`).get(messageId);

  assert(!stored.content.includes('test@example.com'), 'Email leaked to database!');
  assert(!stored.content.includes('sk_live_abc123xyz'), 'API key leaked to database!');
  assert(stored.content.includes('[REDACTED_EMAIL]'), 'Email not redacted');
  assert(stored.content.includes('[REDACTED_API_KEY]'), 'API key not redacted');
}
```

**Invariant 2: In-Memory Only for Raw Content**
```
∀ event ∈ EventStream:
  raw_content ∈ Memory(Hook Process)
  AND raw_content ∉ Disk(Database)
  AND raw_content ∉ Disk(Logs)
  AND raw_content ∉ Disk(Temporary Files)
```

**Verification**:
- No code writes raw content to SQLite
- No logs include raw content (only metadata)
- No temporary files created with raw content
- Hook process memory clears on exit

**Invariant 3: Sanitization Before Any Persistence**
```
∀ write_operation ∈ DatabaseWrites:
  write_operation MUST be preceded by sanitize_operation
  AND sanitize_operation completes before write_operation
```

**Verification**:
```typescript
// Code review enforcement: All INSERT/UPDATE to messages must use sanitized content
// Static analysis rule: Grep for "INSERT INTO messages" and verify preceding sanitize() call

// Example enforcement in schema:
CREATE TRIGGER enforce_sanitization
BEFORE INSERT ON messages
FOR EACH ROW
WHEN NEW.pre_sanitized != 1
BEGIN
  SELECT RAISE(ABORT, 'Cannot insert unsanitized message');
END;
```

#### 5.2 Audit Trail

**Every sanitization operation logged**:
```typescript
async function logSanitization(
  messageId: string,
  stage: 'pre_sanitization' | 'ai_validation',
  detections: Detection[]
) {
  await db.prepare(`
    INSERT INTO sanitization_log (id, message_id, stage, detections, timestamp)
    VALUES (?, ?, ?, ?, ?)
  `).run(
    ulid(),
    messageId,
    stage,
    JSON.stringify(detections),
    new Date().toISOString()
  );
}
```

**Audit queries**:
```sql
-- How many PII instances detected in last 24 hours?
SELECT
  stage,
  json_each.value->>'category' as category,
  COUNT(*) as count
FROM sanitization_log,
     json_each(detections)
WHERE timestamp > datetime('now', '-1 day')
GROUP BY stage, category;

-- Which messages had most PII?
SELECT
  message_id,
  COUNT(*) as detection_count
FROM sanitization_log,
     json_each(detections)
GROUP BY message_id
ORDER BY detection_count DESC
LIMIT 10;

-- AI validation coverage
SELECT
  COUNT(*) FILTER (WHERE ai_validated = 1) * 100.0 / COUNT(*) as pct_validated
FROM messages;
```

### 6. Error Handling & Edge Cases

#### 6.1 Sanitization Failures

**Scenario 1: Regex compilation error**
```typescript
// Graceful fallback if pattern fails
try {
  sanitized = content.replace(FAST_PATTERNS.API_KEY, '[REDACTED_API_KEY]');
} catch (error) {
  console.error('[SANITIZE] Pattern failed:', error);
  // Fail-safe: Redact entire message rather than risk PII leak
  sanitized = '[ERROR: Message blocked for safety]';
  await logError('regex_failure', { pattern: 'API_KEY', error });
}
```

**Scenario 2: Performance timeout**
```typescript
// If sanitization takes >80ms, log warning but complete anyway
if (durationMs > 80) {
  console.warn('[SANITIZE] Over budget:', { durationMs, contentLength: content.length });
  await metrics.increment('sanitization.timeout', { durationMs });
}
// Don't throw - complete the operation
```

**Scenario 3: Out of memory**
```typescript
// For extremely large content (>1MB), reject
if (content.length > 1_000_000) {
  throw new Error('Content too large for sanitization (>1MB)');
  // Hook will catch this and log error, not persist message
}
```

#### 6.2 AI Validation Failures

**Scenario 1: Claude API timeout**
```typescript
try {
  const response = await claudeClient.messages.create({
    // ... params
    timeout: 10000 // 10 second timeout
  });
} catch (error) {
  if (error instanceof TimeoutError) {
    console.error('[AI VALIDATE] Timeout:', error);
    // Message already pre-sanitized, so this is enhancement only
    // Mark as failed validation, retry later
    await updateJob(jobId, { status: 'failed', error: error.message, attempts: attempts + 1 });
    return;
  }
  throw error;
}
```

**Scenario 2: API rate limit**
```typescript
// Exponential backoff with max 3 retries
const maxRetries = 3;
for (let attempt = 1; attempt <= maxRetries; attempt++) {
  try {
    return await claudeClient.messages.create({ /* ... */ });
  } catch (error) {
    if (error.status === 429) { // Rate limit
      const delayMs = Math.pow(2, attempt) * 1000; // 2s, 4s, 8s
      console.warn(`[AI VALIDATE] Rate limited, retry in ${delayMs}ms`);
      await sleep(delayMs);
      continue;
    }
    throw error;
  }
}
// After 3 retries, move to dead_letter queue
await updateJob(jobId, { status: 'dead_letter', error: 'Rate limit exceeded after retries' });
```

**Scenario 3: AI returns invalid JSON**
```typescript
try {
  const result = JSON.parse(response.content[0].text);
} catch (error) {
  console.error('[AI VALIDATE] Invalid JSON response:', error);
  // Log the raw response for debugging
  await logError('ai_invalid_json', { response: response.content[0].text, error });
  // Skip AI detections, message already pre-sanitized
  await updateMessage(messageId, { ai_validated: 0, ai_error: 'Invalid AI response' });
}
```

### 7. Testing Strategy

#### 7.1 Unit Tests

**Fast Sanitization Tests**:
```typescript
describe('fastSanitize', () => {
  it('should redact API keys', () => {
    const input = 'My key is sk_live_abc123xyz456789012345';
    const { sanitized, detections } = fastSanitize(input);

    expect(sanitized).toBe('My key is [REDACTED_API_KEY]');
    expect(detections).toHaveLength(1);
    expect(detections[0].category).toBe('API_KEY');
  });

  it('should redact multiple PII types', () => {
    const input = 'Email: test@example.com, Phone: 555-123-4567, IP: 192.168.1.1';
    const { sanitized } = fastSanitize(input);

    expect(sanitized).toBe('Email: [REDACTED_EMAIL], Phone: [REDACTED_PHONE], IP: [REDACTED_IP]');
  });

  it('should complete within 50ms for 10KB content', () => {
    const input = generateLargeContent(10_000); // 10KB
    const { durationMs } = fastSanitize(input);

    expect(durationMs).toBeLessThan(50);
  });

  it('should handle edge cases without crashing', () => {
    const edgeCases = [
      '',                          // Empty string
      'a'.repeat(100_000),        // Large content
      '🔑 sk_live_émoji123',      // Unicode
      null,                        // Null (should throw)
      undefined                    // Undefined (should throw)
    ];

    for (const testCase of edgeCases) {
      expect(() => fastSanitize(testCase)).not.toThrow();
    }
  });
});
```

**AI Validation Tests**:
```typescript
describe('aiValidate', () => {
  it('should detect person names missed by regex', async () => {
    const sanitized = 'John Smith shared his API credentials'; // Name not in regex
    const messageId = await createTestMessage(sanitized);

    await aiValidate(messageId, sanitized);

    const updated = await getMessage(messageId);
    expect(updated.content).toContain('[REDACTED_PERSON]');
    expect(updated.ai_validated).toBe(1);
  });

  it('should not re-sanitize if no additional PII found', async () => {
    const sanitized = 'The function returns [REDACTED_API_KEY] as output';
    const messageId = await createTestMessage(sanitized);

    await aiValidate(messageId, sanitized);

    const updated = await getMessage(messageId);
    expect(updated.content).toBe(sanitized); // Unchanged
    expect(updated.ai_validated).toBe(1);
  });

  it('should handle API failures gracefully', async () => {
    // Mock API to throw error
    claudeClient.messages.create.mockRejectedValue(new Error('API timeout'));

    const messageId = await createTestMessage('test content');
    await aiValidate(messageId, 'test content');

    const updated = await getMessage(messageId);
    expect(updated.ai_validated).toBe(0);
    expect(updated.ai_error).toContain('API timeout');
  });
});
```

#### 7.2 Integration Tests

**End-to-End Privacy Test**:
```typescript
describe('Privacy Guarantee', () => {
  it('should never persist raw PII to database', async () => {
    const rawContent = `
      User details:
      Email: user@example.com
      Phone: +1-555-123-4567
      API Key: sk_live_abc123xyz456
      AWS Key: AKIAIOSFODNN7EXAMPLE
      File: /Users/john/secret.txt
    `;

    // Simulate hook execution
    const event = {
      role: 'user',
      content: rawContent,
      conversation_id: ulid(),
      sequence: 1
    };

    // Process through hook
    await userPromptSubmitHook(event);

    // Verify: Check database for any PII
    const messages = await db.prepare(`
      SELECT content FROM messages WHERE conversation_id = ?
    `).all(event.conversation_id);

    for (const msg of messages) {
      // Assert NO raw PII in database
      expect(msg.content).not.toContain('user@example.com');
      expect(msg.content).not.toContain('555-123-4567');
      expect(msg.content).not.toContain('sk_live_abc123xyz456');
      expect(msg.content).not.toContain('AKIAIOSFODNN7EXAMPLE');
      expect(msg.content).not.toContain('/Users/john/secret.txt');

      // Assert redaction tags present
      expect(msg.content).toContain('[REDACTED_EMAIL]');
      expect(msg.content).toContain('[REDACTED_PHONE]');
      expect(msg.content).toContain('[REDACTED_API_KEY]');
      expect(msg.content).toContain('[REDACTED_AWS_KEY]');
      expect(msg.content).toContain('[REDACTED_PATH]');
    }
  });
});
```

**Performance Integration Test**:
```typescript
describe('Performance Budgets', () => {
  it('should complete hook execution in <100ms', async () => {
    const durations: number[] = [];

    for (let i = 0; i < 100; i++) {
      const event = generateTestEvent();
      const start = performance.now();

      await userPromptSubmitHook(event);

      const duration = performance.now() - start;
      durations.push(duration);
    }

    const p95 = percentile(durations, 0.95);
    expect(p95).toBeLessThan(100);
  });
});
```

#### 7.3 Adversarial Tests

**Obfuscated PII**:
```typescript
describe('Adversarial Sanitization', () => {
  it('should catch obfuscated email addresses', () => {
    const inputs = [
      'user@example.com',           // Normal
      'user [at] example [dot] com', // Obfuscated
      'user(at)example(dot)com',    // Alternative
      'user AT example DOT com'     // Caps
    ];

    for (const input of inputs) {
      const { sanitized } = fastSanitize(input);
      // First pattern should catch, others may need AI
      expect(sanitized).toContain('[REDACTED');
    }
  });

  it('should catch API keys with mixed case', () => {
    const inputs = [
      'sk_live_abc123',
      'SK_LIVE_ABC123',
      'Sk_Live_Abc123'
    ];

    for (const input of inputs) {
      const { sanitized } = fastSanitize(input);
      expect(sanitized).toContain('[REDACTED_API_KEY]');
    }
  });
});
```

### 8. Operational Considerations

#### 8.1 Monitoring & Alerts

**Key Metrics**:
```typescript
// Performance metrics
metrics.timing('sanitization.fast.duration_ms', durationMs);
metrics.timing('sanitization.ai.duration_ms', durationMs);
metrics.timing('hook.total_duration_ms', totalMs);

// Detection metrics
metrics.increment('sanitization.detections', { category: detection.category });
metrics.increment('sanitization.ai.additional_detections');

// Error metrics
metrics.increment('sanitization.errors', { error_type: 'regex_failure' });
metrics.increment('sanitization.ai.errors', { error_type: 'api_timeout' });

// Coverage metrics
metrics.gauge('sanitization.ai_validation_rate', validatedCount / totalCount);
```

**Alerts**:
- Hook p95 latency > 100ms → Page on-call
- Fast sanitization p95 > 50ms → Warning
- AI validation failure rate > 10% → Investigation
- Zero detections for 1 hour → Potential sanitization bypass

#### 8.2 Maintenance & Updates

**Pattern Updates**:
```typescript
// Versioned pattern library
export const PATTERN_VERSIONS = {
  'v1.0': { /* initial patterns */ },
  'v1.1': { /* added AWS keys */ },
  'v1.2': { /* improved email regex */ }
} as const;

// Current version
export const CURRENT_PATTERN_VERSION = 'v1.2';

// Migration path
async function migratePatterns(fromVersion: string, toVersion: string) {
  // Re-sanitize messages that used old patterns
  const messages = await db.prepare(`
    SELECT id, content FROM messages
    WHERE pattern_version = ?
  `).all(fromVersion);

  for (const msg of messages) {
    const { sanitized } = fastSanitize(msg.content); // Uses new patterns
    await db.prepare(`
      UPDATE messages
      SET content = ?, pattern_version = ?
      WHERE id = ?
    `).run(sanitized, toVersion, msg.id);
  }
}
```

**AI Prompt Updates**:
```typescript
// Versioned prompts
export const AI_PROMPT_VERSIONS = {
  'v1.0': { /* initial prompt */ },
  'v1.1': { /* improved reasoning instructions */ }
} as const;

// A/B testing new prompts
async function abTestPrompt(messageId: string, content: string) {
  const useNewPrompt = Math.random() < 0.1; // 10% traffic
  const promptVersion = useNewPrompt ? 'v1.1' : 'v1.0';

  const result = await aiValidate(messageId, content, promptVersion);

  // Track metrics by version
  metrics.increment('ai.detections', { version: promptVersion, count: result.detections.length });
}
```

### 9. Future Enhancements

#### 9.1 Machine Learning NER (Post-MVP)

**Add Transformer-based NER** for better person/org detection:
```
Stage 1.5: ML NER (between regex and AI)
- Use spaCy or Hugging Face model
- Detect: PERSON, ORG, GPE (location)
- Budget: <200ms (still synchronous)
- Trade-off: Better accuracy vs added latency
```

#### 9.2 Pseudonymization (Optional)

**Within-session linking** while preserving privacy:
```typescript
// Map PII to consistent placeholders per session
const sessionMapping = new Map<string, string>();

function pseudonymize(pii: string, category: string): string {
  if (sessionMapping.has(pii)) {
    return sessionMapping.get(pii)!;
  }

  const placeholder = `<${category}_${sessionMapping.size + 1}>`;
  sessionMapping.set(pii, placeholder);
  return placeholder;
}

// Example:
// "John emailed john@example.com" → "<PERSON_1> emailed <EMAIL_1>"
// "John called Mary" → "<PERSON_1> called <PERSON_2>"
```

#### 9.3 Differential Privacy (Research)

**Add noise to prevent re-identification**:
- Aggregate statistics only (no individual queries)
- K-anonymity guarantees
- Research application for learning extraction

---

## Success Criteria

This architecture document will be considered complete when:

1. **Alignment with STANDARDS.md**: 100% consistency with canonical flow, schema, enums
2. **Privacy Guarantee**: Clear proof that raw PII never persists
3. **Implementation Ready**: Developers can implement both stages from this spec
4. **Performance Targets**: Explicit budgets with measurement strategy
5. **Testing Coverage**: Comprehensive test strategy for privacy and performance
6. **Integration Clear**: Shows how sanitization fits with hooks, DB, job queue
7. **GPT-5 Validated**: External review confirms no contradictions or gaps

---

## Next Steps

1. **Get GPT-5 Review** (Now):
   - Share this plan + repomix project context
   - Ask: "Does this architecture align with STANDARDS.md and ADR-004?"
   - Ask: "Are there any gaps, contradictions, or missing components?"
   - Ask: "Will this provide zero-PII-leakage guarantee?"

2. **Incorporate Feedback**:
   - Address any issues raised
   - Refine sections as needed

3. **Create Final Document**:
   - Write `docs/architecture/architecture-sanitization-pipeline-2025-01-16.md`
   - Include all diagrams, code samples, and specifications
   - Cross-link to STANDARDS.md, ADR-004, and related docs

4. **Update INDEX**:
   - Add to `docs/architecture/INDEX.md`
   - Update references in other docs

---

## GPT-5 Review Request

**Prompt for GPT-5**:

```
You are reviewing a sanitization pipeline architecture plan for a privacy-first system that captures AI conversations.

CONTEXT:
- Full project context: [attach project-context-repomix-2025-01-16.txt]
- STANDARDS.md: Canonical source of truth for all standards
- ADR-004: Privacy decision - sanitize before storage
- Critical issue: Some docs show raw persistence (contradiction)

REQUIREMENTS:
1. Privacy flow MUST be: Pre-sanitize (<50ms) → Persist ONLY sanitized → Optional AI validation (async)
2. Raw content NEVER persists to disk
3. Schema uses `messages` table (no events/event_queue)
4. Performance: <50ms pre-sanitization, <2s AI validation
5. Redaction format: [REDACTED_API_KEY], [REDACTED_EMAIL], etc.

REVIEW THIS PLAN:
[attach this plan document]

QUESTIONS:
1. Does this architecture align 100% with STANDARDS.md and ADR-004?
2. Are there any contradictions with the canonical privacy flow?
3. Will this guarantee zero PII leakage to disk?
4. Are there any missing components or edge cases?
5. Is the performance budget realistic and achievable?
6. Are the integration points with hooks/DB/job_queue clear?
7. Is the testing strategy sufficient to prove privacy guarantees?

Please provide:
- Alignment assessment (pass/fail with specific issues)
- Gap analysis (what's missing)
- Risk assessment (what could go wrong)
- Recommendations for improvements
```

---

**Document Status**: Ready for GPT-5 review
**Created**: 2025-01-16
**Author**: Claude (based on STANDARDS.md, ADR-004, and external reviews)
````

## File: plans/plan-subagent-workflow-2025-01-16.md
````markdown
# Subagent Workflow - How Subagents Work Together

> Detailed workflow for using specialized subagents to implement the Global Context Network

---
title: Subagent-Driven Development Workflow
category: plan
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [subagents, workflow, tdd, development-process]
---

## Overview

This document describes how specialized subagents collaborate to implement features using Test-Driven Development (TDD) and automated quality gates.

**Core Principle**: Never implement directly - always delegate to specialized subagents.

## Subagent Roles

### Implementation Subagents
Build features and components following TDD
- Read requirements from plans
- Write tests first (Red phase)
- Implement minimal code (Green phase)
- Refactor for quality (Refactor phase)

### Test Generation Subagents
Create comprehensive test suites
- Generate unit tests (70% coverage target)
- Generate integration tests (20% coverage target)
- Generate E2E tests (10% coverage target)
- Validate test quality

### Validation Subagents
Enforce quality gates
- Test quality validator
- Coverage validator
- Implementation validator
- Code quality validator
- Security validator
- Performance validator

## TDD Workflow with Subagents

### Step 1: Test Generation (RED Phase)

**Subagent**: `unit-test-generator`

**Inputs**:
- Feature requirements
- Interface/API design
- Edge cases to cover

**Outputs**:
- Failing test suite
- Test quality score

**Process**:
```typescript
// Main orchestrator delegates to test generator
const testResult = await runSubagent("unit-test-generator", {
  feature: "Event sanitization",
  requirements: phaseRequirements,
  coverage: "edges, errors, boundaries"
});

// Validate test quality
const qualityCheck = await runSubagent("test-quality-validator", {
  tests: testResult.tests
});

// Require quality score ≥ 0.8
if (qualityCheck.score < 0.8) {
  throw new Error("Test quality insufficient");
}

// Run tests - confirm they FAIL
const testRun = await runTests(testResult.tests);
if (!testRun.allFailed) {
  throw new Error("Tests should fail before implementation");
}
```

### Step 2: Implementation (GREEN Phase)

**Subagent**: `implementation-agent` (phase-specific)

**Inputs**:
- Failing tests
- Requirements
- Architecture guidelines

**Outputs**:
- Implementation code
- All tests passing

**Process**:
```typescript
// Main orchestrator delegates to implementation agent
const implResult = await runSubagent("hook-developer-agent", {
  tests: testResult.tests,
  requirements: phaseRequirements,
  constraints: "< 100ms execution time"
});

// Run tests - confirm they PASS
const testRun = await runTests(testResult.tests);
if (!testRun.allPassed) {
  throw new Error("Implementation must pass all tests");
}

// Validate implementation
const implCheck = await runSubagent("implementation-validator", {
  code: implResult.code,
  tests: testResult.tests
});
```

### Step 3: Refactor (REFACTOR Phase)

**Subagent**: `refactor-agent`

**Inputs**:
- Working implementation
- Passing tests
- Code quality standards

**Outputs**:
- Refactored code
- Still passing tests
- Improved quality scores

**Process**:
```typescript
// Main orchestrator delegates to refactor agent
const refactorResult = await runSubagent("refactor-agent", {
  code: implResult.code,
  tests: testResult.tests,
  focus: "DRY, clear naming, SOLID principles"
});

// Run tests - confirm still PASS
const testRun = await runTests(testResult.tests);
if (!testRun.allPassed) {
  throw new Error("Refactoring broke tests");
}
```

### Step 4: Quality Gates (VALIDATION Phase)

**Subagents**: Multiple validators in parallel

**Process**:
```typescript
// Run all validators in parallel
const [coverageCheck, codeQualityCheck, securityCheck, perfCheck] = await Promise.all([
  runSubagent("coverage-validator", { tests: testResult.tests }),
  runSubagent("code-quality-validator", { code: refactorResult.code }),
  runSubagent("security-validator", { code: refactorResult.code }),
  runSubagent("performance-validator", { code: refactorResult.code })
]);

// All gates must pass
const allGatesPassed =
  coverageCheck.coverage >= 0.85 &&
  codeQualityCheck.passed &&
  securityCheck.issues.length === 0 &&
  perfCheck.acceptable;

if (!allGatesPassed) {
  throw new Error("Quality gates failed");
}
```

## Phase-Specific Workflows

### Phase 0: Foundation

**Subagents**:
- `foundation-setup-agent`: TypeScript + Vitest setup
- `database-schema-agent`: Schema design
- `test-infrastructure-agent`: Test utilities

**Workflow**:
1. Foundation agent sets up project
2. Database agent creates schema
3. Test infrastructure agent creates helpers
4. All work in parallel (independent)

**Coordination**:
```typescript
// Parallel execution
const [foundation, database, testInfra] = await Promise.all([
  runSubagent("foundation-setup-agent", {...}),
  runSubagent("database-schema-agent", {...}),
  runSubagent("test-infrastructure-agent", {...})
]);
```

### Phase 1: Event Capture

**Subagents**:
- `hook-developer-agent`: Hook implementations
- `event-collector-agent`: Event aggregation
- `queue-system-agent`: Persistent queue

**Workflow**:
1. Test generator creates hook tests
2. Hook developer implements hooks
3. Event collector aggregates events
4. Queue system persists events
5. Integration testing validates flow

**Coordination**:
```typescript
// Sequential with dependencies
const hookTests = await runSubagent("unit-test-generator", { feature: "hooks" });
const hooks = await runSubagent("hook-developer-agent", { tests: hookTests });

// Event collector needs hooks
const collectorTests = await runSubagent("unit-test-generator", { feature: "event-collector" });
const collector = await runSubagent("event-collector-agent", {
  tests: collectorTests,
  hooks: hooks
});

// Queue system needs collector schema
const queueTests = await runSubagent("unit-test-generator", { feature: "queue" });
const queue = await runSubagent("queue-system-agent", {
  tests: queueTests,
  schema: collector.eventSchema
});
```

### Phase 2: Sanitization

**Subagents**:
- `rule-sanitizer-agent`: Regex patterns
- `ai-sanitizer-agent`: LLM sanitization
- `sanitization-pipeline-agent`: Orchestration

**Workflow**:
1. Build gold dataset first
2. Rule sanitizer in parallel with AI sanitizer
3. Pipeline orchestrator combines both
4. Validation with precision/recall metrics

**Coordination**:
```typescript
// Create gold dataset first
const goldDataset = await runSubagent("dataset-builder-agent", {
  categories: ["email", "phone", "ip", "path", "key", "url"],
  size: 1000
});

// Parallel sanitizer development
const [ruleTests, aiTests] = await Promise.all([
  runSubagent("unit-test-generator", { feature: "rule-sanitizer", dataset: goldDataset }),
  runSubagent("unit-test-generator", { feature: "ai-sanitizer", dataset: goldDataset })
]);

const [ruleSanitizer, aiSanitizer] = await Promise.all([
  runSubagent("rule-sanitizer-agent", { tests: ruleTests, dataset: goldDataset }),
  runSubagent("ai-sanitizer-agent", { tests: aiTests, dataset: goldDataset })
]);

// Pipeline combines both
const pipelineTests = await runSubagent("integration-test-generator", { feature: "sanitization-pipeline" });
const pipeline = await runSubagent("sanitization-pipeline-agent", {
  tests: pipelineTests,
  ruleSanitizer,
  aiSanitizer,
  dataset: goldDataset
});
```

## Quality Gate Details

### Coverage Gate

**Validator**: `coverage-validator`

**Requirements**:
- Line coverage ≥ 85%
- Statement coverage ≥ 85%
- Function coverage ≥ 85%
- Branch coverage ≥ 80%

**Process**:
```bash
vitest run --coverage
```

**Gate**:
```typescript
if (coverage.lines < 0.85 || coverage.statements < 0.85 ||
    coverage.functions < 0.85 || coverage.branches < 0.80) {
  throw new Error(`Coverage insufficient: ${JSON.stringify(coverage)}`);
}
```

### Code Quality Gate

**Validator**: `code-quality-validator`

**Requirements**:
- ESLint clean
- Prettier formatted
- TypeScript strict mode compliant
- No `any` types
- Clear naming conventions

**Process**:
```bash
npm run lint
npm run type-check
```

**Gate**: All checks must pass

### Security Gate

**Validator**: `security-validator`

**Requirements**:
- No SQL injection vectors
- No command injection
- No path traversal
- No hardcoded secrets
- No insecure dependencies

**Process**:
```bash
npm audit
grep -r "apiKey.*=" src/  # Example check
```

**Gate**: Zero critical issues

### Performance Gate

**Validator**: `performance-validator`

**Requirements**:
- Hooks < 100ms (p95)
- DB queries < 100ms
- MCP queries < 200ms
- Sanitization < 2s per conversation

**Process**: Benchmark tests with realistic data

**Gate**: All SLOs met

## Error Handling in Workflows

### Subagent Failure

**Strategy**: Retry with exponential backoff

```typescript
async function runWithRetry(agentName: string, input: any, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await runSubagent(agentName, input);
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await sleep(2 ** i * 1000);
    }
  }
}
```

### Quality Gate Failure

**Strategy**: Block and report

```typescript
if (!qualityGatePassed) {
  // Log detailed failure reasons
  console.error("Quality gate failed:", {
    coverage: coverageCheck,
    codeQuality: codeQualityCheck,
    security: securityCheck,
    performance: perfCheck
  });

  // Block merge/commit
  throw new Error("Quality gates must pass before proceeding");
}
```

### Test Flakiness

**Strategy**: Deterministic tests only

- Pin LLM model/version
- Use temperature=0 for AI calls
- Snapshot test outputs
- Avoid time-dependent assertions
- Mock external dependencies

## Monitoring Subagent Performance

### Metrics to Track

- Subagent invocation count
- Success rate per subagent
- Average execution time
- Error types and frequency
- Quality gate pass rate

### Dashboard (Future)

```typescript
{
  "unit-test-generator": {
    "invocations": 150,
    "successRate": 0.98,
    "avgDuration": "45s",
    "qualityScore": 0.87
  },
  "hook-developer-agent": {
    "invocations": 12,
    "successRate": 1.0,
    "avgDuration": "3m 20s",
    "testsPass": true
  }
}
```

## Related Documents

- [Subagent System Architecture](../architecture/architecture-subagent-system-2025-01-16.md)
- [Implementation Roadmap](./plan-implementation-roadmap-2025-01-16.md)
- [TDD Workflow Guide](../guides/guide-tdd-workflow-2025-01-16.md)
- [Using Subagents Guide](../guides/guide-using-subagents-2025-01-16.md)
````

## File: reference/INDEX.md
````markdown
# Reference Documentation

> Last updated: 2025-01-16

## Overview

This directory contains technical reference materials, API documentation, schemas, and specifications for the Global Context Network MVP. Reference docs are designed for quick lookup during implementation.

## Documents

### Active Documents

| Date | Document | Description |
|------|----------|-------------|
| 2025-01-16 | [reference-testing-strategy-2025-01-16.md](./reference-testing-strategy-2025-01-16.md) | Complete testing strategy with TDD, coverage requirements, and test organization |
| 2025-01-16 | [reference-subagent-types-2025-01-16.md](./reference-subagent-types-2025-01-16.md) | Catalog of all 22 subagent types with configurations and prompts |
| 2025-01-16 | [reference-database-schema-2025-01-16.md](./reference-database-schema-2025-01-16.md) | Complete SQLite schema with tables, indexes, migrations, and query patterns |
| 2025-01-16 | [reference-claude-agent-sdk-api-2025-01-16.md](./reference-claude-agent-sdk-api-2025-01-16.md) | Claude Agent SDK API reference with subagent patterns and MCP integration |
| 2025-01-16 | [reference-event-schema-2025-01-16.md](./reference-event-schema-2025-01-16.md) | Complete event schemas for Claude Code hooks with ULID IDs and ISO-8601 timestamps |
| 2025-01-16 | [reference-hook-configuration-2025-01-16.md](./reference-hook-configuration-2025-01-16.md) | Canonical .claude/hooks.json configuration with compiled .js hooks and stdin IO |

## Quick Reference

### Testing Strategy

**Coverage Requirements**:
- Global: ≥85% lines, ≥70% branches
- Critical path: 100% lines, 90% branches
- Test pyramid: 70% unit, 20% integration, 10% E2E

**Key Commands**:
```bash
pnpm test:unit           # Unit tests
pnpm test:integration    # Integration tests
pnpm test:e2e           # End-to-end tests
pnpm test:coverage      # Coverage report
pnpm test:watch         # Watch mode
```

**TDD Cycle**: Red (failing test) → Green (minimal implementation) → Refactor (improve quality)

### Subagent Types

**Total**: 22 subagents
- **14 Implementation**: foundation-setup, database-schema, hook-developer, etc.
- **3 Test Generation**: unit-test-generator, integration-test-generator, e2e-test-generator
- **3 Test Validation**: test-quality-validator, coverage-validator, implementation-validator
- **3 Quality Gates**: code-quality-validator, security-validator, performance-validator

**Model Selection**:
- **Sonnet**: Complex reasoning, code generation, validation
- **Haiku**: Simple tasks, quick validations, high-volume operations

### Database Schema

**Tables**: 6 core tables
- `conversations` - Sanitized conversation metadata
- `messages` - Individual sanitized messages
- `learnings` - Extracted insights with FTS5
- `job_queue` - Async job processing
- `uploads` - Network upload tracking
- `sanitization_log` - PII detection audit trail

**Critical**: NEVER store unsanitized data. `sanitized` column always = 1.

**Required PRAGMAs**:
```sql
PRAGMA foreign_keys = ON;
PRAGMA journal_mode = WAL;
PRAGMA synchronous = FULL;
```

### Claude Agent SDK API

**Main Function**: `query(options: QueryOptions)`

**Agent Definition**:
```typescript
{
  description: string;
  model: 'sonnet' | 'haiku';
  tools: string[];
  prompt: string;
  output_schema?: JSONSchema;
  timeout_ms?: number;
}
```

**Execution Patterns**:
- **Parallel**: Independent subagents run simultaneously
- **Sequential**: Dependent subagents run in order
- **Fan-out/Fan-in**: Multiple validators, merge results

## Document Usage

### By Role

**For Developers**:
1. Start with Testing Strategy for TDD workflow
2. Reference Database Schema for queries and repositories
3. Use Claude Agent SDK API for subagent integration
4. Consult Subagent Types for specific agent configs

**For Testing**:
1. Testing Strategy for coverage requirements
2. Subagent Types for test generator configs
3. Database Schema for test database setup
4. Claude Agent SDK API for mocking

**For Architecture Review**:
1. Database Schema for data model validation
2. Subagent Types for agent responsibilities
3. Claude Agent SDK API for integration patterns
4. Testing Strategy for quality gates

### By Phase

**Phase 0 (Foundation)**:
- Testing Strategy: Vitest configuration
- Database Schema: Initial migration
- Subagent Types: foundation-setup, database-schema, test-infrastructure

**Phase 1 (Event Capture)**:
- Subagent Types: hook-developer, event-collector, queue-system
- Database Schema: conversations, messages, job_queue tables
- Testing Strategy: Hook performance tests

**Phase 2 (Sanitization)**:
- Subagent Types: rule-sanitizer, ai-sanitizer, sanitization-pipeline
- Database Schema: sanitization_log table
- Testing Strategy: Security testing, PII corpus

**Phase 3-7 (Later Phases)**:
- Refer to respective subagents
- Database Schema: learnings, uploads tables
- Testing Strategy: Integration and E2E tests

## Cross-References

### Related Categories

- **[Architecture](../architecture/INDEX.md)**: System design and component architecture
- **[Guides](../guides/INDEX.md)**: Step-by-step how-to guides
- **[Decisions](../decisions/INDEX.md)**: ADRs explaining technical choices
- **[Plans](../plans/INDEX.md)**: Implementation roadmaps and task breakdowns

### Key Cross-Links

**Testing Strategy** ↔ **Subagent Types**:
- Test generator subagents (unit, integration, E2E)
- Validation subagents (quality, coverage, implementation)

**Database Schema** ↔ **Testing Strategy**:
- In-memory test database setup
- Fixture factories
- Repository testing patterns

**Claude Agent SDK API** ↔ **Subagent Types**:
- Agent configuration examples
- Orchestration patterns
- MCP integration

**All References** ↔ **Architecture**:
- Architecture provides context
- References provide implementation details

## Version Information

### Schema Version
- **Database**: 1.0.0
- **Sanitization**: 1.0.0
- **API**: Claude Agent SDK 1.x

### Applies To
- **SQLite**: 3.40+
- **Node.js**: 20+
- **TypeScript**: 5.x
- **Vitest**: 1.x
- **Claude Models**: Sonnet 4.5, Haiku 3.5

## Quick Lookup Tables

### Coverage Thresholds

| Scope | Lines | Statements | Branches | Functions |
|-------|-------|------------|----------|-----------|
| Global | 85% | 85% | 70% | 85% |
| Critical Path | 100% | 100% | 90% | 100% |
| Infrastructure | 50% | 50% | 30% | 50% |

### Performance SLAs

| Component | P50 | P95 | P99 | Max |
|-----------|-----|-----|-----|-----|
| Hooks | <50ms | <100ms | <150ms | 200ms |
| Sanitization | <1s | <2s | <3s | 5s |
| DB Queries | <50ms | <100ms | <200ms | 500ms |
| MCP Queries | <100ms | <200ms | <500ms | 1s |

### Subagent Models

| Task Type | Model | Rationale |
|-----------|-------|-----------|
| Complex reasoning | Sonnet | Better quality |
| Code generation | Sonnet | Type-safe code |
| Simple validation | Haiku | Fast, cost-effective |
| High-volume ops | Haiku | Throughput |

### Database Indexes

| Table | Index | Purpose |
|-------|-------|---------|
| conversations | idx_conversations_session | Session queries |
| conversations | idx_conversations_created | Recent conversations |
| messages | idx_messages_conversation | Conversation messages |
| learnings | idx_learnings_category | Category filtering |
| learnings_fts | FTS5 | Full-text search |
| job_queue | idx_job_queue_dequeue | Worker queries |

## Maintenance

### Updating References

When updating reference docs:

1. **Update version metadata** in frontmatter
2. **Add changelog entry** if schema/API changes
3. **Update this INDEX** with changes
4. **Cross-link** with related architecture docs
5. **Verify code examples** still work

### Schema Evolution

When database schema changes:

1. Create new migration file
2. Update reference-database-schema with new DDL
3. Update repository examples
4. Note breaking changes in changelog
5. Bump schema version

### API Changes

When Claude Agent SDK API changes:

1. Update reference-claude-agent-sdk-api
2. Update code examples
3. Test against new SDK version
4. Update "applies_to" metadata
5. Note deprecations

## Contributing

### Reference Doc Template

```markdown
# [Topic] Reference

> Brief description

---
title: [Topic] Reference
category: reference
date: YYYY-MM-DD
status: active
authors: Claude + Dennison
tags: [relevant, tags]
applies_to: Versions/Tools
---

## Overview
[What this reference covers]

## [Section 1]
[Technical details, code examples, tables]

## [Section 2]
[More technical content]

## Related Documents
[Cross-links to architecture, guides, etc.]
```

### Quality Checklist

- ✅ Clear, concise descriptions
- ✅ Complete code examples (runnable)
- ✅ Type signatures and schemas
- ✅ Performance characteristics noted
- ✅ Common pitfalls documented
- ✅ Cross-references complete
- ✅ Version information current
- ✅ Tables formatted consistently

---

*This index provides quick access to all technical reference materials for the Global Context Network MVP.*
````

## File: reference/reference-claude-agent-sdk-api-2025-01-16.md
````markdown
# Claude Agent SDK API Reference

> Complete API reference for Claude Agent SDK with subagent patterns and MCP integration

---
title: Claude Agent SDK API Reference
category: reference
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [claude-agent-sdk, api, subagents, mcp, typescript]
applies_to: Claude Agent SDK 1.x, Sonnet 4.5, Haiku 3.5
---

## Overview

The Claude Agent SDK enables building AI agents with specialized subagents, MCP tool integration, and structured workflows. This document provides complete API reference and best practices.

**Package**: `@anthropic-ai/claude-agent-sdk` (verify actual package name)
**Models**: `claude-sonnet-4-5-20250929`, `claude-haiku-3-5-20250318`

---

## Core API

### query()

Main entry point for delegating tasks to subagents.

**Signature**:
```typescript
function query(options: QueryOptions): AsyncIterable<Message>;

interface QueryOptions {
  prompt: string;
  options?: {
    model?: string;
    agents?: Record<string, AgentDefinition>;
    mcpServers?: Record<string, MCPServerConfig>;
    maxTokens?: number;
    temperature?: number;
    abortSignal?: AbortSignal;
  };
}
```

**Parameters**:
- `prompt` (required): Main task description
- `options.model`: Model ID (default: `claude-sonnet-4-5-20250929`)
- `options.agents`: Subagent definitions (see AgentDefinition)
- `options.mcpServers`: MCP server configurations
- `options.maxTokens`: Token budget (default: 4096)
- `options.temperature`: Randomness 0-1 (default: 1.0)
- `options.abortSignal`: Cancellation signal

**Returns**: Async iterable of messages

**Example**:
```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';

const response = query({
  prompt: 'Implement user authentication',
  options: {
    model: 'claude-sonnet-4-5-20250929',
    agents: {
      'implementation-agent': {
        description: 'Implements authentication logic',
        model: 'sonnet',
        tools: ['Write', 'Read', 'Bash'],
        prompt: 'You are an authentication expert...'
      }
    }
  }
});

for await (const message of response) {
  if (message.type === 'text') {
    console.log(message.content);
  }
}
```

---

### Message Types

**Event Stream Format**:
```typescript
type Message =
  | TextMessage
  | ToolCallMessage
  | ToolResultMessage
  | SubagentStartMessage
  | SubagentProgressMessage
  | SubagentEndMessage
  | ErrorMessage
  | LogMessage;

interface TextMessage {
  type: 'text';
  content: string;
  role: 'user' | 'assistant';
  correlation_id?: string;
}

interface ToolCallMessage {
  type: 'tool_call';
  tool_name: string;
  tool_input: any;
  tool_call_id: string;
  correlation_id?: string;
}

interface ToolResultMessage {
  type: 'tool_result';
  tool_call_id: string;
  result: any;
  error?: string;
  correlation_id?: string;
}

interface SubagentStartMessage {
  type: 'system';
  subtype: 'subagent_start';
  agent_name: string;
  agent_description: string;
  correlation_id: string;
  timestamp: string;
}

interface SubagentProgressMessage {
  type: 'system';
  subtype: 'subagent_progress';
  agent_name: string;
  progress: number; // 0-1
  message: string;
  correlation_id: string;
  timestamp: string;
}

interface SubagentEndMessage {
  type: 'system';
  subtype: 'subagent_end';
  agent_name: string;
  result: any;
  success: boolean;
  error?: string;
  correlation_id: string;
  timestamp: string;
  duration_ms: number;
}

interface ErrorMessage {
  type: 'error';
  error_type: string; // 'api_error' | 'tool_error' | 'timeout' | 'rate_limit'
  error_message: string;
  error_code?: string;
  correlation_id?: string;
  retry_after_ms?: number; // For rate limits
}

interface LogMessage {
  type: 'log';
  level: 'debug' | 'info' | 'warn' | 'error';
  message: string;
  correlation_id?: string;
  metadata?: any;
}
```

---

## Subagent Configuration

### AgentDefinition

```typescript
interface AgentDefinition {
  name?: string; // Auto-set from key if not provided
  description: string; // What this agent does
  model: 'sonnet' | 'haiku' | string; // Model selection
  tools: string[]; // Available tool names
  resources?: string[]; // MCP resources
  prompt: string; // System prompt for agent
  output_schema?: JSONSchema; // Expected output format
  temperature?: number; // 0-1, default 1.0
  max_tokens?: number; // Token budget
  stop_sequences?: string[]; // Stop generation at these
  timeout_ms?: number; // Max execution time
  retry_policy?: RetryPolicy;
  metadata?: any; // Custom metadata
}

interface RetryPolicy {
  max_retries: number; // Default 3
  initial_delay_ms: number; // Default 1000
  max_delay_ms: number; // Default 60000
  backoff_multiplier: number; // Default 2 (exponential)
}

interface JSONSchema {
  type: 'object' | 'array' | 'string' | 'number' | 'boolean';
  properties?: Record<string, JSONSchema>;
  items?: JSONSchema;
  required?: string[];
  enum?: any[];
  // ... standard JSON Schema fields
}
```

**Example: Implementation Agent**:
```typescript
const implementationAgent: AgentDefinition = {
  description: 'Implements TypeScript functions following TDD',
  model: 'sonnet',
  tools: ['Write', 'Read', 'Bash'],
  prompt: `You are a TypeScript implementation expert.

REQUIREMENTS:
- Write type-safe code with strict mode
- Never use "any" types
- Follow TDD: tests exist before implementation
- Handle errors gracefully
- Add JSDoc comments

OUTPUT FORMAT:
Return JSON with:
- files_created: string[]
- tests_passing: boolean
- coverage_percentage: number
`,
  output_schema: {
    type: 'object',
    properties: {
      files_created: { type: 'array', items: { type: 'string' } },
      tests_passing: { type: 'boolean' },
      coverage_percentage: { type: 'number' }
    },
    required: ['files_created', 'tests_passing']
  },
  temperature: 0.7, // Slightly more focused
  max_tokens: 8192,
  timeout_ms: 120000, // 2 minutes
  retry_policy: {
    max_retries: 3,
    initial_delay_ms: 1000,
    max_delay_ms: 30000,
    backoff_multiplier: 2
  }
};
```

**Example: Test Generator**:
```typescript
const testGenerator: AgentDefinition = {
  description: 'Generates comprehensive unit tests',
  model: 'sonnet',
  tools: ['Write', 'Read', 'mcp__test-runner__run_unit_tests'],
  prompt: `You are a test generation expert.

Generate unit tests with:
- Arrange-Act-Assert pattern
- Edge cases (null, undefined, empty, boundary values)
- Error conditions
- Clear test names
- Proper mocking

TARGET: >85% coverage with high-quality tests
`,
  output_schema: {
    type: 'object',
    properties: {
      test_files: { type: 'array', items: { type: 'string' } },
      coverage: { type: 'number' },
      test_count: { type: 'number' }
    },
    required: ['test_files', 'coverage']
  },
  max_tokens: 16384, // More for comprehensive tests
  timeout_ms: 180000 // 3 minutes
};
```

---

## MCP Integration

### MCPServerConfig

```typescript
interface MCPServerConfig {
  name: string;
  command: string; // Executable path
  args?: string[]; // Command arguments
  env?: Record<string, string>; // Environment variables
  capabilities?: {
    tools?: boolean;
    resources?: boolean;
    prompts?: boolean;
  };
  timeout_ms?: number;
}
```

**Example: Test Runner MCP Server**:
```typescript
const testRunnerServer: MCPServerConfig = {
  name: 'test-runner',
  command: 'node',
  args: ['./mcp-servers/test-runner/index.js'],
  env: {
    NODE_ENV: 'test'
  },
  capabilities: {
    tools: true,
    resources: true
  },
  timeout_ms: 30000
};

// Use in query
const response = query({
  prompt: 'Run unit tests',
  options: {
    mcpServers: {
      'test-runner': testRunnerServer
    }
  }
});
```

### MCP Tool Definition

```typescript
// In your MCP server (test-runner/index.js)
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';

const server = new Server({
  name: 'test-runner',
  version: '1.0.0'
}, {
  capabilities: {
    tools: {},
    resources: {}
  }
});

// Define tools
server.setRequestHandler('tools/list', async () => {
  return {
    tools: [
      {
        name: 'run_unit_tests',
        description: 'Runs unit tests and returns results',
        inputSchema: {
          type: 'object',
          properties: {
            test_pattern: {
              type: 'string',
              description: 'Test file pattern (e.g., "**/*.test.ts")'
            },
            watch: {
              type: 'boolean',
              description: 'Run in watch mode',
              default: false
            }
          }
        }
      },
      {
        name: 'get_coverage_report',
        description: 'Returns coverage report',
        inputSchema: {
          type: 'object',
          properties: {}
        }
      }
    ]
  };
});

// Implement tool calls
server.setRequestHandler('tools/call', async (request) => {
  const { name, arguments: args } = request.params;

  if (name === 'run_unit_tests') {
    const { test_pattern = '**/*.test.ts', watch = false } = args;

    // Run tests
    const result = await runVitest({ pattern: test_pattern, watch });

    return {
      content: [
        {
          type: 'text',
          text: JSON.stringify({
            total: result.total,
            passed: result.passed,
            failed: result.failed,
            duration_ms: result.duration,
            coverage: result.coverage
          }, null, 2)
        }
      ]
    };
  }

  throw new Error(`Unknown tool: ${name}`);
});

// Start server
const transport = new StdioServerTransport();
await server.connect(transport);
```

### MCP Resource Definition

```typescript
// Define resources
server.setRequestHandler('resources/list', async () => {
  return {
    resources: [
      {
        uri: 'test://coverage/summary',
        name: 'Coverage Summary',
        description: 'Current test coverage summary',
        mimeType: 'application/json'
      },
      {
        uri: 'test://results/latest',
        name: 'Latest Test Results',
        description: 'Most recent test run results',
        mimeType: 'application/json'
      }
    ]
  };
});

// Implement resource reads
server.setRequestHandler('resources/read', async (request) => {
  const { uri } = request.params;

  if (uri === 'test://coverage/summary') {
    const coverage = await getCoverageSummary();

    return {
      contents: [
        {
          uri,
          mimeType: 'application/json',
          text: JSON.stringify(coverage, null, 2)
        }
      ]
    };
  }

  throw new Error(`Unknown resource: ${uri}`);
});
```

---

## Execution Patterns

### Parallel Execution

When subagents have no dependencies, run them in parallel:

```typescript
const response = query({
  prompt: 'Implement Phase 0 Foundation',
  options: {
    agents: {
      'foundation-setup': foundationSetupAgent,
      'database-schema': databaseSchemaAgent,
      'test-infrastructure': testInfraAgent
    }
  }
});

const results: Record<string, any> = {};

for await (const message of response) {
  if (message.type === 'system' && message.subtype === 'subagent_end') {
    results[message.agent_name] = message.result;
    console.log(`✅ ${message.agent_name} completed in ${message.duration_ms}ms`);
  }
}

// All three agents ran in parallel
console.log('All agents complete:', results);
```

### Sequential with Dependencies

When subagents depend on each other's outputs:

```typescript
// Step 1: Generate tests
const testResult = await runSubagent('test-generator', {
  function_name: 'detectAPIKeys',
  requirements: 'Detect AWS, OpenAI, GitHub API keys'
});

// Step 2: Implement (needs test output)
const implResult = await runSubagent('implementation-agent', {
  tests: testResult.test_files,
  requirements: 'Implement to pass tests'
});

// Step 3: Validate (needs both)
const validationResult = await runSubagent('implementation-validator', {
  implementation: implResult.files_created,
  tests: testResult.test_files
});

// Helper function
async function runSubagent(agentName: string, input: any): Promise<any> {
  const response = query({
    prompt: JSON.stringify(input),
    options: {
      agents: {
        [agentName]: agentConfigs[agentName]
      }
    }
  });

  let result: any = null;

  for await (const message of response) {
    if (message.type === 'system' && message.subtype === 'subagent_end') {
      result = message.result;
    }
  }

  return result;
}
```

### Fan-Out / Fan-In

Run multiple subagents, then merge results:

```typescript
async function runValidators(code: string): Promise<ValidationReport> {
  const response = query({
    prompt: `Validate this code:\n${code}`,
    options: {
      agents: {
        'code-quality': codeQualityValidator,
        'security': securityValidator,
        'performance': performanceValidator
      }
    }
  });

  const validationResults: Record<string, any> = {};

  for await (const message of response) {
    if (message.type === 'system' && message.subtype === 'subagent_end') {
      validationResults[message.agent_name] = message.result;
    }
  }

  // Merge results
  return {
    code_quality: validationResults['code-quality'],
    security: validationResults['security'],
    performance: validationResults['performance'],
    overall_pass: Object.values(validationResults).every(r => r.passed)
  };
}
```

---

## Error Handling

### Error Types

```typescript
class APIError extends Error {
  constructor(
    message: string,
    public code: string,
    public status?: number
  ) {
    super(message);
    this.name = 'APIError';
  }
}

class ToolError extends Error {
  constructor(
    message: string,
    public tool_name: string,
    public tool_input: any
  ) {
    super(message);
    this.name = 'ToolError';
  }
}

class TimeoutError extends Error {
  constructor(
    message: string,
    public timeout_ms: number
  ) {
    super(message);
    this.name = 'TimeoutError';
  }
}

class RateLimitError extends Error {
  constructor(
    message: string,
    public retry_after_ms: number
  ) {
    super(message);
    this.name = 'RateLimitError';
  }
}
```

### Retry Logic

```typescript
async function queryWithRetry(
  options: QueryOptions,
  maxRetries = 3
): Promise<any> {
  let lastError: Error | null = null;

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      const response = query(options);
      let result: any = null;

      for await (const message of response) {
        if (message.type === 'error') {
          throw new APIError(
            message.error_message,
            message.error_code || 'unknown',
            message.retry_after_ms
          );
        }

        if (message.type === 'system' && message.subtype === 'subagent_end') {
          if (!message.success) {
            throw new Error(message.error || 'Subagent failed');
          }
          result = message.result;
        }
      }

      return result;

    } catch (error) {
      lastError = error as Error;

      // Don't retry on validation errors
      if (error instanceof Error && error.message.includes('validation')) {
        throw error;
      }

      // Rate limit: wait before retry
      if (error instanceof RateLimitError) {
        await sleep(error.retry_after_ms);
        continue;
      }

      // Exponential backoff
      const backoffMs = Math.min(1000 * Math.pow(2, attempt), 60000);
      await sleep(backoffMs);
    }
  }

  throw lastError || new Error('Max retries exceeded');
}

function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}
```

### Graceful Degradation

```typescript
async function sanitizeWithFallback(content: string): Promise<string> {
  try {
    // Try AI sanitization first
    const result = await runSubagent('ai-sanitizer', { content });
    return result.sanitized_content;

  } catch (error) {
    console.warn('AI sanitization failed, falling back to rules:', error);

    // Fallback to rule-based
    const result = await runSubagent('rule-sanitizer', { content });
    return result.sanitized_content;
  }
}
```

---

## Best Practices

### When to Use Subagents

**✅ Use subagents for**:
- Complex, multi-step tasks
- Tasks requiring specialized expertise
- Tasks that can run in parallel
- Tasks requiring different model sizes (Sonnet vs Haiku)
- Tasks needing different tool sets

**❌ Don't use subagents for**:
- Simple, single-step operations
- Tasks where delegation overhead > task complexity
- Tightly coupled operations better done in one context

### Model Selection

**Sonnet (claude-sonnet-4-5)**:
- Complex reasoning
- Code generation
- Architecture decisions
- Creative tasks
- Quality validation

**Haiku (claude-haiku-3-5)**:
- Simple transformations
- Format conversions
- Quick validations
- High-volume operations
- Cost-sensitive tasks

**Example**:
```typescript
const agents = {
  // Sonnet for complex implementation
  'implementation': {
    model: 'claude-sonnet-4-5-20250929',
    description: 'Implements complex authentication logic',
    // ...
  },

  // Haiku for simple validation
  'coverage-checker': {
    model: 'claude-haiku-3-5-20250318',
    description: 'Checks if coverage >= 85%',
    // ...
  }
};
```

### Output Schema Validation

Always validate subagent outputs:

```typescript
import Ajv from 'ajv';

const ajv = new Ajv();

async function runSubagentWithValidation(
  agentName: string,
  input: any,
  expectedSchema: JSONSchema
): Promise<any> {
  const result = await runSubagent(agentName, input);

  const validate = ajv.compile(expectedSchema);
  if (!validate(result)) {
    throw new Error(`Invalid output from ${agentName}: ${JSON.stringify(validate.errors)}`);
  }

  return result;
}

// Usage
const result = await runSubagentWithValidation(
  'implementation-agent',
  { function: 'detectAPIKeys' },
  {
    type: 'object',
    properties: {
      files_created: { type: 'array', items: { type: 'string' } },
      tests_passing: { type: 'boolean' }
    },
    required: ['files_created', 'tests_passing']
  }
);
```

### Correlation IDs

Track execution across subagents:

```typescript
import { randomUUID } from 'crypto';

async function runWithCorrelation(taskName: string, fn: () => Promise<any>): Promise<any> {
  const correlationId = randomUUID();

  console.log(`[${correlationId}] Starting: ${taskName}`);

  try {
    const result = await fn();
    console.log(`[${correlationId}] Success: ${taskName}`);
    return result;
  } catch (error) {
    console.error(`[${correlationId}] Failed: ${taskName}`, error);
    throw error;
  }
}
```

### Cost Tracking

Monitor token usage and costs:

```typescript
interface UsageStats {
  total_tokens: number;
  input_tokens: number;
  output_tokens: number;
  cost_usd: number;
}

const PRICING = {
  'claude-sonnet-4-5': {
    input: 0.003 / 1000,  // $3 per 1M input tokens
    output: 0.015 / 1000  // $15 per 1M output tokens
  },
  'claude-haiku-3-5': {
    input: 0.00025 / 1000,  // $0.25 per 1M input tokens
    output: 0.00125 / 1000  // $1.25 per 1M output tokens
  }
};

function calculateCost(usage: UsageStats, model: string): number {
  const pricing = PRICING[model as keyof typeof PRICING];
  if (!pricing) return 0;

  return (
    usage.input_tokens * pricing.input +
    usage.output_tokens * pricing.output
  );
}

async function runWithCostTracking(options: QueryOptions): Promise<{ result: any; cost: number }> {
  let totalInputTokens = 0;
  let totalOutputTokens = 0;

  const response = query(options);
  let result: any = null;

  for await (const message of response) {
    // Track usage from messages
    if ('usage' in message) {
      totalInputTokens += message.usage?.input_tokens || 0;
      totalOutputTokens += message.usage?.output_tokens || 0;
    }

    if (message.type === 'system' && message.subtype === 'subagent_end') {
      result = message.result;
    }
  }

  const cost = calculateCost(
    {
      total_tokens: totalInputTokens + totalOutputTokens,
      input_tokens: totalInputTokens,
      output_tokens: totalOutputTokens,
      cost_usd: 0
    },
    options.options?.model || 'claude-sonnet-4-5-20250929'
  );

  return { result, cost };
}
```

---

## Testing with SDK

### Mocking the SDK

```typescript
import { vi } from 'vitest';

export function mockQuery(responses: Array<Message[]>) {
  let callCount = 0;

  return vi.fn((options: QueryOptions) => {
    const messages = responses[callCount] || [];
    callCount++;

    return (async function* () {
      for (const message of messages) {
        yield message;
      }
    })();
  });
}

// Usage in tests
it('should handle subagent success', async () => {
  const mockQueryFn = mockQuery([
    [
      {
        type: 'system',
        subtype: 'subagent_start',
        agent_name: 'test-agent',
        correlation_id: '123',
        timestamp: new Date().toISOString()
      },
      {
        type: 'system',
        subtype: 'subagent_end',
        agent_name: 'test-agent',
        result: { files_created: ['test.ts'] },
        success: true,
        correlation_id: '123',
        timestamp: new Date().toISOString(),
        duration_ms: 1000
      }
    ]
  ]);

  // Test your function that uses query()
  const result = await myFunction(mockQueryFn);

  expect(result).toEqual({ files_created: ['test.ts'] });
});
```

### Testing MCP Servers

```typescript
import { createMockMCPServer } from '../helpers/mock-mcp-server';

it('should call MCP tool correctly', async () => {
  const mockServer = createMockMCPServer();

  mockServer.addTool('run_unit_tests', async (args) => {
    return {
      total: 10,
      passed: 10,
      failed: 0
    };
  });

  const response = query({
    prompt: 'Run tests',
    options: {
      mcpServers: {
        'test-runner': mockServer.config
      }
    }
  });

  // Assert tool was called
  expect(mockServer.toolCalls['run_unit_tests']).toHaveLength(1);
});
```

---

## Complete Example

### TDD Workflow with Subagents

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';

interface TDDResult {
  tests_created: string[];
  implementation_created: string[];
  all_tests_passing: boolean;
  coverage: number;
}

async function implementFeatureTDD(
  featureName: string,
  requirements: string
): Promise<TDDResult> {
  console.log(`\n🔴 RED: Generating failing tests for ${featureName}...`);

  // Step 1: Generate tests
  const testGenResult = await runSubagent('test-generator', {
    feature: featureName,
    requirements
  });

  console.log(`✅ Tests created: ${testGenResult.test_files.join(', ')}`);

  console.log(`\n🟢 GREEN: Implementing ${featureName}...`);

  // Step 2: Implement
  const implResult = await runSubagent('implementation-agent', {
    feature: featureName,
    requirements,
    tests: testGenResult.test_files
  });

  console.log(`✅ Implementation created: ${implResult.files_created.join(', ')}`);

  console.log(`\n🔵 REFACTOR: Validating quality...`);

  // Step 3: Validate quality (parallel)
  const validationResponse = query({
    prompt: `Validate ${featureName} implementation`,
    options: {
      agents: {
        'code-quality': codeQualityValidator,
        'security': securityValidator,
        'coverage': coverageValidator
      }
    }
  });

  const validations: Record<string, any> = {};

  for await (const message of validationResponse) {
    if (message.type === 'system' && message.subtype === 'subagent_end') {
      validations[message.agent_name] = message.result;

      if (!message.success) {
        throw new Error(`${message.agent_name} failed: ${message.error}`);
      }
    }
  }

  console.log(`\n✅ All quality gates passed!`);

  return {
    tests_created: testGenResult.test_files,
    implementation_created: implResult.files_created,
    all_tests_passing: implResult.tests_passing,
    coverage: validations['coverage'].percentage
  };
}

// Agent configurations
const testGenerator = {
  description: 'Generates comprehensive unit tests',
  model: 'sonnet',
  tools: ['Write', 'Read'],
  prompt: `Generate unit tests following TDD best practices...`,
  output_schema: {
    type: 'object',
    properties: {
      test_files: { type: 'array', items: { type: 'string' } },
      test_count: { type: 'number' }
    },
    required: ['test_files']
  }
};

const implementationAgent = {
  description: 'Implements features to pass tests',
  model: 'sonnet',
  tools: ['Write', 'Read', 'Bash'],
  prompt: `Implement features following TDD...`,
  output_schema: {
    type: 'object',
    properties: {
      files_created: { type: 'array', items: { type: 'string' } },
      tests_passing: { type: 'boolean' }
    },
    required: ['files_created', 'tests_passing']
  }
};

const codeQualityValidator = {
  description: 'Validates code quality',
  model: 'sonnet',
  tools: ['Read', 'Bash'],
  prompt: `Check code quality standards...`
};

const securityValidator = {
  description: 'Scans for security issues',
  model: 'sonnet',
  tools: ['Read', 'Bash'],
  prompt: `Scan for security vulnerabilities...`
};

const coverageValidator = {
  description: 'Checks test coverage',
  model: 'haiku', // Simple task
  tools: ['Bash', 'Read'],
  prompt: `Calculate test coverage percentage...`
};

// Usage
const result = await implementFeatureTDD(
  'API Key Detection',
  'Detect AWS, OpenAI, and GitHub API keys in content'
);

console.log(JSON.stringify(result, null, 2));
```

---

## Related Documents

### Architecture
- [Subagent System Architecture](../architecture/architecture-subagent-system-2025-01-16.md)
- [Global Context Network](../architecture/architecture-global-context-network-2025-01-16.md)

### Reference
- [Subagent Types](./reference-subagent-types-2025-01-16.md)
- [Testing Strategy](./reference-testing-strategy-2025-01-16.md)

### Guides
- [Using Subagents](../guides/guide-using-subagents-2025-01-16.md)
- [TDD Workflow](../guides/guide-tdd-workflow-2025-01-16.md)
````

## File: reference/reference-database-schema-2025-01-16.md
````markdown
# Database Schema Reference

> Complete SQLite database schema with tables, indexes, migrations, and query patterns

---
title: Database Schema Reference
category: reference
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [database, sqlite, schema, migrations, sql]
applies_to: SQLite 3.40+, better-sqlite3 9.x
schema_version: 1.0.0
---

## Overview

The Global Context Network uses SQLite with WAL mode for local persistence. All data MUST be sanitized before insertion - there are NO raw content columns.

**Core Principle**: Never store unsanitized data. Sanitization happens BEFORE database insertion.

### Database Configuration

```sql
-- Required PRAGMAs (set on every connection)
PRAGMA foreign_keys = ON;
PRAGMA journal_mode = WAL;
PRAGMA synchronous = FULL;
PRAGMA busy_timeout = 5000;
PRAGMA page_size = 8192;
```

### Connection Setup

```typescript
import Database from 'better-sqlite3';

export function createDatabase(path: string): Database.Database {
  const db = new Database(path);

  // Enable required PRAGMAs
  db.pragma('foreign_keys = ON');
  db.pragma('journal_mode = WAL');
  db.pragma('synchronous = FULL');
  db.pragma('busy_timeout = 5000');

  return db;
}
```

---

## Tables

### conversations

Stores sanitized conversation metadata.

```sql
CREATE TABLE IF NOT EXISTS conversations (
  id TEXT PRIMARY KEY, -- ULID or UUID
  session_id TEXT NOT NULL, -- Claude Code session identifier
  correlation_id TEXT NOT NULL UNIQUE, -- For tracking conversation flow
  created_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  updated_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  completed_at TEXT, -- When conversation ended
  sanitized BOOLEAN NOT NULL DEFAULT 1 CHECK (sanitized = 1), -- ALWAYS true
  sanitization_version TEXT NOT NULL, -- e.g., "1.0.0"
  message_count INTEGER NOT NULL DEFAULT 0,
  metadata JSON -- Additional context (project path, user settings, etc.)
);

-- Indexes
CREATE INDEX idx_conversations_session ON conversations(session_id);
CREATE INDEX idx_conversations_created ON conversations(created_at DESC);
CREATE INDEX idx_conversations_correlation ON conversations(correlation_id);

-- Triggers for updated_at
CREATE TRIGGER conversations_updated_at
AFTER UPDATE ON conversations
FOR EACH ROW
BEGIN
  UPDATE conversations SET updated_at = datetime('now') WHERE id = NEW.id;
END;
```

**Column Descriptions**:
- `id`: Unique conversation identifier (ULID recommended for sortability)
- `session_id`: Claude Code session ID for grouping
- `correlation_id`: Unique ID for tracking across systems
- `created_at`: When conversation started (auto-set)
- `updated_at`: Last modification (auto-updated via trigger)
- `completed_at`: When conversation ended (NULL if ongoing)
- `sanitized`: MUST always be 1 (enforced by CHECK constraint)
- `sanitization_version`: Version of sanitization rules applied
- `message_count`: Cached count (updated via trigger)
- `metadata`: JSON for flexible additional data

**Performance Considerations**:
- `session_id` index for session queries
- `created_at DESC` index for recent conversations
- `correlation_id` unique index for lookups

---

### messages

Stores individual sanitized messages within conversations.

```sql
CREATE TABLE IF NOT EXISTS messages (
  id TEXT PRIMARY KEY, -- ULID or UUID
  conversation_id TEXT NOT NULL,
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
  content TEXT NOT NULL, -- SANITIZED content only
  content_hash TEXT NOT NULL, -- SHA-256 hash for deduplication
  sequence INTEGER NOT NULL, -- Order within conversation (0, 1, 2, ...)
  created_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  token_count INTEGER, -- Approximate token count
  metadata JSON, -- Thinking, tool calls, etc.

  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE,
  UNIQUE (conversation_id, sequence)
);

-- Indexes
CREATE INDEX idx_messages_conversation ON messages(conversation_id, sequence);
CREATE INDEX idx_messages_created ON messages(created_at DESC);
CREATE INDEX idx_messages_hash ON messages(content_hash);

-- Trigger to update conversation.message_count
CREATE TRIGGER messages_after_insert
AFTER INSERT ON messages
FOR EACH ROW
BEGIN
  UPDATE conversations
  SET message_count = message_count + 1
  WHERE id = NEW.conversation_id;
END;

CREATE TRIGGER messages_after_delete
AFTER DELETE ON messages
FOR EACH ROW
BEGIN
  UPDATE conversations
  SET message_count = message_count - 1
  WHERE id = OLD.conversation_id;
END;
```

**Column Descriptions**:
- `id`: Unique message identifier
- `conversation_id`: Parent conversation (CASCADE delete)
- `role`: Message sender (user/assistant/system)
- `content`: SANITIZED message content (NO PII)
- `content_hash`: For detecting duplicate messages
- `sequence`: Order within conversation (0-indexed)
- `created_at`: When message was created
- `token_count`: Approximate tokens (for cost tracking)
- `metadata`: JSON for thinking, tool calls, attachments

**CRITICAL**: This table has NO `raw_content` or `unsanitized_content` column. Sanitization MUST happen before insertion.

---

### learnings

Stores extracted learnings with full-text search.

```sql
CREATE TABLE IF NOT EXISTS learnings (
  id TEXT PRIMARY KEY, -- ULID or UUID
  conversation_id TEXT NOT NULL,
  source_message_ids JSON NOT NULL, -- Array of message IDs that produced this learning
  category TEXT NOT NULL CHECK (
    category IN (
      'pattern',
      'best_practice',
      'anti_pattern',
      'bug_fix',
      'optimization',
      'tool_usage',
      'workflow',
      'decision'
    )
  ),
  title TEXT NOT NULL, -- Short summary
  content TEXT NOT NULL, -- Detailed learning (SANITIZED)
  confidence REAL NOT NULL CHECK (confidence >= 0.0 AND confidence <= 1.0),
  tags JSON NOT NULL DEFAULT '[]', -- Array of strings
  dedupe_hash TEXT NOT NULL UNIQUE, -- For preventing duplicates
  created_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  metadata JSON, -- Additional context

  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- Indexes
CREATE INDEX idx_learnings_conversation ON learnings(conversation_id);
CREATE INDEX idx_learnings_category ON learnings(category, confidence DESC);
CREATE INDEX idx_learnings_confidence ON learnings(confidence DESC);
CREATE INDEX idx_learnings_created ON learnings(created_at DESC);
CREATE INDEX idx_learnings_dedupe ON learnings(dedupe_hash);

-- Full-text search (FTS5)
CREATE VIRTUAL TABLE learnings_fts USING fts5(
  learning_id UNINDEXED,
  title,
  content,
  tags,
  content='learnings',
  content_rowid='rowid'
);

-- Triggers to keep FTS in sync
CREATE TRIGGER learnings_fts_insert
AFTER INSERT ON learnings
BEGIN
  INSERT INTO learnings_fts(rowid, learning_id, title, content, tags)
  VALUES (NEW.rowid, NEW.id, NEW.title, NEW.content, NEW.tags);
END;

CREATE TRIGGER learnings_fts_delete
AFTER DELETE ON learnings
BEGIN
  DELETE FROM learnings_fts WHERE rowid = OLD.rowid;
END;

CREATE TRIGGER learnings_fts_update
AFTER UPDATE ON learnings
BEGIN
  DELETE FROM learnings_fts WHERE rowid = OLD.rowid;
  INSERT INTO learnings_fts(rowid, learning_id, title, content, tags)
  VALUES (NEW.rowid, NEW.id, NEW.title, NEW.content, NEW.tags);
END;
```

**Column Descriptions**:
- `id`: Unique learning identifier
- `conversation_id`: Source conversation
- `source_message_ids`: JSON array of message IDs
- `category`: Type of learning (CHECK constraint enforced)
- `title`: Short summary (used in lists)
- `content`: Detailed learning text
- `confidence`: Quality score 0.0-1.0
- `tags`: JSON array of topic tags
- `dedupe_hash`: Prevents duplicate learnings
- `created_at`: When learning was extracted
- `metadata`: Additional context

**FTS5 Full-Text Search**:
- Searches across `title`, `content`, and `tags`
- BM25 ranking
- Supports phrase queries, AND/OR, NEAR

---

### job_queue

Persistent queue for async job processing.

```sql
CREATE TABLE IF NOT EXISTS job_queue (
  id TEXT PRIMARY KEY, -- ULID (sortable)
  type TEXT NOT NULL, -- 'sanitize', 'extract_learning', 'upload'
  status TEXT NOT NULL DEFAULT 'queued' CHECK (
    status IN ('queued', 'running', 'succeeded', 'failed', 'quarantined')
  ),
  priority INTEGER NOT NULL DEFAULT 5 CHECK (priority >= 1 AND priority <= 10), -- 1=highest
  run_at TEXT NOT NULL DEFAULT (datetime('now')), -- When to run (for delayed jobs)
  locked_at TEXT, -- When job was claimed by worker
  locked_by TEXT, -- Worker ID that claimed job
  payload JSON NOT NULL, -- Job-specific data
  attempts INTEGER NOT NULL DEFAULT 0,
  max_retries INTEGER NOT NULL DEFAULT 3,
  last_error TEXT, -- Error message from last failure
  created_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  updated_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  completed_at TEXT -- When job finished (success or quarantine)
);

-- Indexes for worker queries
CREATE INDEX idx_job_queue_dequeue ON job_queue(status, priority, run_at)
  WHERE status = 'queued';
CREATE INDEX idx_job_queue_type ON job_queue(type, status);
CREATE INDEX idx_job_queue_created ON job_queue(created_at DESC);

-- Trigger for updated_at
CREATE TRIGGER job_queue_updated_at
AFTER UPDATE ON job_queue
FOR EACH ROW
BEGIN
  UPDATE job_queue SET updated_at = datetime('now') WHERE id = NEW.id;
END;
```

**Column Descriptions**:
- `id`: ULID for time-sortable IDs
- `type`: Job type for worker routing
- `status`: Current state (queued/running/succeeded/failed/quarantined)
- `priority`: 1-10, where 1 is highest priority
- `run_at`: Delayed job support (run after this time)
- `locked_at`: Optimistic locking timestamp
- `locked_by`: Worker identifier (hostname + PID)
- `payload`: JSON with job-specific parameters
- `attempts`: Retry counter
- `max_retries`: Max attempts before quarantine
- `last_error`: Last failure reason
- `completed_at`: When job finished

**Worker Query Pattern** (Optimistic Locking):
```sql
UPDATE job_queue
SET
  status = 'running',
  locked_at = datetime('now'),
  locked_by = :worker_id,
  attempts = attempts + 1
WHERE id = (
  SELECT id
  FROM job_queue
  WHERE status = 'queued'
    AND run_at <= datetime('now')
  ORDER BY priority ASC, run_at ASC
  LIMIT 1
)
RETURNING *;
```

---

### uploads

Tracks uploads to global network (IPFS + blockchain).

```sql
CREATE TABLE IF NOT EXISTS uploads (
  id TEXT PRIMARY KEY, -- ULID or UUID
  learning_id TEXT NOT NULL UNIQUE,
  ipfs_cid TEXT UNIQUE, -- Content Identifier from IPFS
  chain_tx_hash TEXT UNIQUE, -- Blockchain transaction hash
  status TEXT NOT NULL DEFAULT 'pending' CHECK (
    status IN ('pending', 'ipfs_uploaded', 'tx_submitted', 'confirmed', 'failed')
  ),
  retries INTEGER NOT NULL DEFAULT 0,
  max_retries INTEGER NOT NULL DEFAULT 5,
  last_error TEXT, -- Error from last attempt
  tokens_earned REAL, -- Reward amount (if confirmed)
  created_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  updated_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  uploaded_at TEXT, -- When IPFS upload succeeded
  confirmed_at TEXT, -- When blockchain tx confirmed

  FOREIGN KEY (learning_id) REFERENCES learnings(id) ON DELETE CASCADE
);

-- Indexes
CREATE INDEX idx_uploads_learning ON uploads(learning_id);
CREATE INDEX idx_uploads_status ON uploads(status);
CREATE INDEX idx_uploads_created ON uploads(created_at DESC);
CREATE UNIQUE INDEX idx_uploads_ipfs_cid ON uploads(ipfs_cid) WHERE ipfs_cid IS NOT NULL;
CREATE UNIQUE INDEX idx_uploads_tx_hash ON uploads(chain_tx_hash) WHERE chain_tx_hash IS NOT NULL;

-- Trigger for updated_at
CREATE TRIGGER uploads_updated_at
AFTER UPDATE ON uploads
FOR EACH ROW
BEGIN
  UPDATE uploads SET updated_at = datetime('now') WHERE id = NEW.id;
END;
```

**Column Descriptions**:
- `id`: Unique upload identifier
- `learning_id`: Source learning (UNIQUE - one upload per learning)
- `ipfs_cid`: Content identifier from IPFS
- `chain_tx_hash`: Blockchain transaction hash
- `status`: Upload lifecycle state
- `retries`: Attempt counter
- `max_retries`: Max attempts before giving up
- `last_error`: Last failure reason
- `tokens_earned`: Reward if confirmed
- `uploaded_at`: IPFS upload timestamp
- `confirmed_at`: Blockchain confirmation timestamp

**Upload States**:
1. `pending` → Initial state
2. `ipfs_uploaded` → Content in IPFS, have CID
3. `tx_submitted` → Blockchain tx sent
4. `confirmed` → Tx confirmed, tokens earned
5. `failed` → Max retries exceeded

---

### sanitization_log

Audit trail of all PII detections and redactions.

```sql
CREATE TABLE IF NOT EXISTS sanitization_log (
  id TEXT PRIMARY KEY, -- ULID for time ordering
  conversation_id TEXT NOT NULL,
  message_id TEXT, -- NULL if conversation-level sanitization
  category TEXT NOT NULL, -- PII type (api_key, email, file_path, etc.)
  rule_id TEXT, -- Which rule detected it (for rule-based)
  original_snippet_hash TEXT NOT NULL, -- SHA-256 of original text
  replacement TEXT NOT NULL, -- What it was replaced with
  detector TEXT NOT NULL CHECK (detector IN ('rule', 'ai', 'hybrid')),
  confidence REAL NOT NULL CHECK (confidence >= 0.0 AND confidence <= 1.0),
  created_at TEXT NOT NULL DEFAULT (datetime('now')), -- ISO 8601
  metadata JSON, -- Additional context

  FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE,
  FOREIGN KEY (message_id) REFERENCES messages(id) ON DELETE CASCADE
);

-- Indexes
CREATE INDEX idx_sanitization_log_conversation ON sanitization_log(conversation_id);
CREATE INDEX idx_sanitization_log_message ON sanitization_log(message_id);
CREATE INDEX idx_sanitization_log_category ON sanitization_log(category);
CREATE INDEX idx_sanitization_log_created ON sanitization_log(created_at DESC);
```

**Column Descriptions**:
- `id`: ULID for time-ordered audit trail
- `conversation_id`: Parent conversation
- `message_id`: Specific message (NULL for conversation-wide)
- `category`: Type of PII detected
- `rule_id`: Identifier of detection rule
- `original_snippet_hash`: Hash of PII (NEVER store actual PII)
- `replacement`: Replacement text used
- `detector`: Which system detected it
- `confidence`: Detection confidence score
- `created_at`: When detection occurred
- `metadata`: Additional context (position, surrounding text hash)

**CRITICAL**: NEVER store actual PII in this table. Use `original_snippet_hash` only.

---

## Migrations

### Migration System

```typescript
// src/database/migrations/runner.ts
import Database from 'better-sqlite3';
import fs from 'fs';
import path from 'path';

interface Migration {
  version: number;
  name: string;
  up: string;
  down: string;
}

// Migrations table
function createMigrationsTable(db: Database.Database): void {
  db.exec(`
    CREATE TABLE IF NOT EXISTS _migrations (
      version INTEGER PRIMARY KEY,
      name TEXT NOT NULL,
      applied_at TEXT NOT NULL DEFAULT (datetime('now'))
    );
  `);
}

// Get current version
function getCurrentVersion(db: Database.Database): number {
  const row = db.prepare('SELECT MAX(version) as version FROM _migrations').get() as { version: number | null };
  return row.version ?? 0;
}

// Load migration files
function loadMigrations(dir: string): Migration[] {
  const files = fs.readdirSync(dir).sort();
  const migrations: Migration[] = [];

  for (const file of files) {
    if (!file.endsWith('.sql')) continue;

    const match = file.match(/^(\d+)_(.+)\.sql$/);
    if (!match) continue;

    const version = parseInt(match[1], 10);
    const name = match[2];
    const content = fs.readFileSync(path.join(dir, file), 'utf8');

    // Split on -- UP / -- DOWN markers
    const [up, down] = content.split(/--\s*DOWN/i);
    const upSql = up.replace(/--\s*UP/i, '').trim();
    const downSql = down?.trim() || '';

    migrations.push({ version, name, up: upSql, down: downSql });
  }

  return migrations;
}

// Run migrations
export function runMigrations(db: Database.Database, targetVersion?: number): void {
  createMigrationsTable(db);

  const currentVersion = getCurrentVersion(db);
  const migrations = loadMigrations(path.join(__dirname, 'sql'));

  const toApply = migrations.filter(m =>
    m.version > currentVersion && (!targetVersion || m.version <= targetVersion)
  );

  if (toApply.length === 0) {
    console.log('No migrations to apply');
    return;
  }

  for (const migration of toApply) {
    console.log(`Applying migration ${migration.version}: ${migration.name}`);

    const applyMigration = db.transaction(() => {
      db.exec(migration.up);
      db.prepare('INSERT INTO _migrations (version, name) VALUES (?, ?)').run(migration.version, migration.name);
    });

    applyMigration();
  }

  console.log(`Migrated to version ${toApply[toApply.length - 1].version}`);
}

// Rollback migrations
export function rollbackMigrations(db: Database.Database, targetVersion: number): void {
  const currentVersion = getCurrentVersion(db);
  const migrations = loadMigrations(path.join(__dirname, 'sql'));

  const toRollback = migrations
    .filter(m => m.version > targetVersion && m.version <= currentVersion)
    .reverse();

  if (toRollback.length === 0) {
    console.log('No migrations to rollback');
    return;
  }

  for (const migration of toRollback) {
    console.log(`Rolling back migration ${migration.version}: ${migration.name}`);

    const rollback = db.transaction(() => {
      db.exec(migration.down);
      db.prepare('DELETE FROM _migrations WHERE version = ?').run(migration.version);
    });

    rollback();
  }

  console.log(`Rolled back to version ${targetVersion}`);
}
```

### Example Migration File

```sql
-- migrations/001_initial.sql

-- UP
CREATE TABLE IF NOT EXISTS conversations (
  id TEXT PRIMARY KEY,
  session_id TEXT NOT NULL,
  correlation_id TEXT NOT NULL UNIQUE,
  created_at TEXT NOT NULL DEFAULT (datetime('now')),
  updated_at TEXT NOT NULL DEFAULT (datetime('now')),
  completed_at TEXT,
  sanitized BOOLEAN NOT NULL DEFAULT 1 CHECK (sanitized = 1),
  sanitization_version TEXT NOT NULL,
  message_count INTEGER NOT NULL DEFAULT 0,
  metadata JSON
);

CREATE INDEX idx_conversations_session ON conversations(session_id);
CREATE INDEX idx_conversations_created ON conversations(created_at DESC);

CREATE TRIGGER conversations_updated_at
AFTER UPDATE ON conversations
FOR EACH ROW
BEGIN
  UPDATE conversations SET updated_at = datetime('now') WHERE id = NEW.id;
END;

-- DOWN
DROP TRIGGER IF EXISTS conversations_updated_at;
DROP INDEX IF EXISTS idx_conversations_created;
DROP INDEX IF EXISTS idx_conversations_session;
DROP TABLE IF EXISTS conversations;
```

---

## Query Patterns

### Repository Base Class

```typescript
// src/database/repositories/base-repository.ts
import Database from 'better-sqlite3';

export abstract class BaseRepository<T> {
  constructor(protected db: Database.Database) {}

  protected transaction<R>(fn: () => R): R {
    const trans = this.db.transaction(fn);
    return trans();
  }

  protected prepare(sql: string): Database.Statement {
    return this.db.prepare(sql);
  }

  abstract create(data: Partial<T>): T;
  abstract findById(id: string): T | null;
  abstract update(id: string, data: Partial<T>): T;
  abstract delete(id: string): void;
}
```

### Conversation Repository

```typescript
// src/database/repositories/conversation-repository.ts
import { BaseRepository } from './base-repository';
import { ulid } from 'ulid';

export interface Conversation {
  id: string;
  session_id: string;
  correlation_id: string;
  created_at: string;
  updated_at: string;
  completed_at: string | null;
  sanitized: boolean;
  sanitization_version: string;
  message_count: number;
  metadata: any;
}

export class ConversationRepository extends BaseRepository<Conversation> {
  create(data: Partial<Conversation>): Conversation {
    const id = data.id || ulid();
    const correlation_id = data.correlation_id || ulid();
    const sanitization_version = data.sanitization_version || '1.0.0';

    this.prepare(`
      INSERT INTO conversations (id, session_id, correlation_id, sanitization_version, metadata)
      VALUES (?, ?, ?, ?, ?)
    `).run(
      id,
      data.session_id,
      correlation_id,
      sanitization_version,
      JSON.stringify(data.metadata || {})
    );

    return this.findById(id)!;
  }

  findById(id: string): Conversation | null {
    const row = this.prepare('SELECT * FROM conversations WHERE id = ?').get(id);
    return row ? this.deserialize(row as any) : null;
  }

  findByCorrelationId(correlationId: string): Conversation | null {
    const row = this.prepare('SELECT * FROM conversations WHERE correlation_id = ?').get(correlationId);
    return row ? this.deserialize(row as any) : null;
  }

  findBySession(sessionId: string, limit = 10): Conversation[] {
    const rows = this.prepare(`
      SELECT * FROM conversations
      WHERE session_id = ?
      ORDER BY created_at DESC
      LIMIT ?
    `).all(sessionId, limit);

    return rows.map(r => this.deserialize(r as any));
  }

  update(id: string, data: Partial<Conversation>): Conversation {
    const updates: string[] = [];
    const values: any[] = [];

    if (data.completed_at !== undefined) {
      updates.push('completed_at = ?');
      values.push(data.completed_at);
    }
    if (data.metadata !== undefined) {
      updates.push('metadata = ?');
      values.push(JSON.stringify(data.metadata));
    }

    if (updates.length === 0) {
      return this.findById(id)!;
    }

    values.push(id);

    this.prepare(`
      UPDATE conversations
      SET ${updates.join(', ')}
      WHERE id = ?
    `).run(...values);

    return this.findById(id)!;
  }

  delete(id: string): void {
    this.prepare('DELETE FROM conversations WHERE id = ?').run(id);
  }

  private deserialize(row: any): Conversation {
    return {
      ...row,
      sanitized: Boolean(row.sanitized),
      metadata: row.metadata ? JSON.parse(row.metadata) : {}
    };
  }
}
```

### Learning Repository with FTS

```typescript
// src/database/repositories/learning-repository.ts
import { BaseRepository } from './base-repository';
import { ulid } from 'ulid';
import crypto from 'crypto';

export interface Learning {
  id: string;
  conversation_id: string;
  source_message_ids: string[];
  category: string;
  title: string;
  content: string;
  confidence: number;
  tags: string[];
  dedupe_hash: string;
  created_at: string;
  metadata: any;
}

export class LearningRepository extends BaseRepository<Learning> {
  create(data: Partial<Learning>): Learning {
    const id = data.id || ulid();
    const dedupe_hash = this.generateDedupeHash(data.content!, data.category!);

    this.prepare(`
      INSERT INTO learnings (
        id, conversation_id, source_message_ids, category, title,
        content, confidence, tags, dedupe_hash, metadata
      )
      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `).run(
      id,
      data.conversation_id,
      JSON.stringify(data.source_message_ids || []),
      data.category,
      data.title,
      data.content,
      data.confidence,
      JSON.stringify(data.tags || []),
      dedupe_hash,
      JSON.stringify(data.metadata || {})
    );

    return this.findById(id)!;
  }

  findById(id: string): Learning | null {
    const row = this.prepare('SELECT * FROM learnings WHERE id = ?').get(id);
    return row ? this.deserialize(row as any) : null;
  }

  search(query: string, options: {
    category?: string;
    minConfidence?: number;
    limit?: number;
  } = {}): Learning[] {
    const limit = options.limit || 10;
    const minConfidence = options.minConfidence || 0.0;

    let sql = `
      SELECT l.*
      FROM learnings l
      JOIN learnings_fts fts ON l.rowid = fts.rowid
      WHERE fts MATCH ?
        AND l.confidence >= ?
    `;

    const params: any[] = [query, minConfidence];

    if (options.category) {
      sql += ' AND l.category = ?';
      params.push(options.category);
    }

    sql += ' ORDER BY bm25(fts), l.confidence DESC LIMIT ?';
    params.push(limit);

    const rows = this.prepare(sql).all(...params);
    return rows.map(r => this.deserialize(r as any));
  }

  findByCategory(category: string, limit = 10): Learning[] {
    const rows = this.prepare(`
      SELECT * FROM learnings
      WHERE category = ?
      ORDER BY confidence DESC, created_at DESC
      LIMIT ?
    `).all(category, limit);

    return rows.map(r => this.deserialize(r as any));
  }

  findRecent(limit = 10): Learning[] {
    const rows = this.prepare(`
      SELECT * FROM learnings
      ORDER BY created_at DESC
      LIMIT ?
    `).all(limit);

    return rows.map(r => this.deserialize(r as any));
  }

  findTopRated(limit = 10): Learning[] {
    const rows = this.prepare(`
      SELECT * FROM learnings
      ORDER BY confidence DESC, created_at DESC
      LIMIT ?
    `).all(limit);

    return rows.map(r => this.deserialize(r as any));
  }

  update(id: string, data: Partial<Learning>): Learning {
    // Learnings are generally immutable, but allow confidence updates
    if (data.confidence !== undefined) {
      this.prepare('UPDATE learnings SET confidence = ? WHERE id = ?').run(data.confidence, id);
    }
    return this.findById(id)!;
  }

  delete(id: string): void {
    this.prepare('DELETE FROM learnings WHERE id = ?').run(id);
  }

  private generateDedupeHash(content: string, category: string): string {
    return crypto.createHash('sha256').update(`${category}:${content}`).digest('hex');
  }

  private deserialize(row: any): Learning {
    return {
      ...row,
      source_message_ids: JSON.parse(row.source_message_ids),
      tags: JSON.parse(row.tags),
      metadata: row.metadata ? JSON.parse(row.metadata) : {}
    };
  }
}
```

### Job Queue Repository

```typescript
// src/database/repositories/job-queue-repository.ts
import { BaseRepository } from './base-repository';
import { ulid } from 'ulid';
import os from 'os';

export interface Job {
  id: string;
  type: string;
  status: 'queued' | 'running' | 'succeeded' | 'failed' | 'quarantined';
  priority: number;
  run_at: string;
  locked_at: string | null;
  locked_by: string | null;
  payload: any;
  attempts: number;
  max_retries: number;
  last_error: string | null;
  created_at: string;
  updated_at: string;
  completed_at: string | null;
}

export class JobQueueRepository extends BaseRepository<Job> {
  private workerId = `${os.hostname()}-${process.pid}`;

  create(data: Partial<Job>): Job {
    const id = data.id || ulid();
    const priority = data.priority || 5;
    const max_retries = data.max_retries || 3;
    const run_at = data.run_at || new Date().toISOString();

    this.prepare(`
      INSERT INTO job_queue (id, type, priority, run_at, payload, max_retries)
      VALUES (?, ?, ?, ?, ?, ?)
    `).run(
      id,
      data.type,
      priority,
      run_at,
      JSON.stringify(data.payload || {}),
      max_retries
    );

    return this.findById(id)!;
  }

  // Optimistic locking: claim next job
  dequeue(): Job | null {
    const updated = this.prepare(`
      UPDATE job_queue
      SET
        status = 'running',
        locked_at = datetime('now'),
        locked_by = ?,
        attempts = attempts + 1
      WHERE id = (
        SELECT id
        FROM job_queue
        WHERE status = 'queued'
          AND run_at <= datetime('now')
        ORDER BY priority ASC, run_at ASC
        LIMIT 1
      )
      RETURNING *
    `).get(this.workerId);

    return updated ? this.deserialize(updated as any) : null;
  }

  markSucceeded(id: string): void {
    this.prepare(`
      UPDATE job_queue
      SET status = 'succeeded', completed_at = datetime('now')
      WHERE id = ?
    `).run(id);
  }

  markFailed(id: string, error: string): void {
    const job = this.findById(id);
    if (!job) return;

    if (job.attempts >= job.max_retries) {
      // Quarantine
      this.prepare(`
        UPDATE job_queue
        SET status = 'quarantined', last_error = ?, completed_at = datetime('now')
        WHERE id = ?
      `).run(error, id);
    } else {
      // Requeue with backoff
      const backoffMs = Math.pow(2, job.attempts) * 1000;
      const runAt = new Date(Date.now() + backoffMs).toISOString();

      this.prepare(`
        UPDATE job_queue
        SET status = 'queued', last_error = ?, run_at = ?, locked_at = NULL, locked_by = NULL
        WHERE id = ?
      `).run(error, runAt, id);
    }
  }

  findById(id: string): Job | null {
    const row = this.prepare('SELECT * FROM job_queue WHERE id = ?').get(id);
    return row ? this.deserialize(row as any) : null;
  }

  // Clean up stale locks (workers that crashed)
  releaseStaleJobs(timeout_ms = 300000): number {
    const staleTime = new Date(Date.now() - timeout_ms).toISOString();

    const result = this.prepare(`
      UPDATE job_queue
      SET status = 'queued', locked_at = NULL, locked_by = NULL
      WHERE status = 'running'
        AND locked_at < ?
    `).run(staleTime);

    return result.changes;
  }

  update(id: string, data: Partial<Job>): Job {
    throw new Error('Use specific methods (markSucceeded, markFailed)');
  }

  delete(id: string): void {
    this.prepare('DELETE FROM job_queue WHERE id = ?').run(id);
  }

  private deserialize(row: any): Job {
    return {
      ...row,
      payload: JSON.parse(row.payload)
    };
  }
}
```

---

## Performance Optimization

### Query Performance Tips

1. **Always use indexes for foreign keys**
2. **Add covering indexes for frequent queries**
3. **Use EXPLAIN QUERY PLAN to verify index usage**
4. **Keep transactions short**
5. **Use prepared statements (auto-cached)**

### EXPLAIN Example

```sql
EXPLAIN QUERY PLAN
SELECT l.*
FROM learnings l
JOIN learnings_fts fts ON l.rowid = fts.rowid
WHERE fts MATCH 'typescript testing'
  AND l.category = 'pattern'
ORDER BY bm25(fts), l.confidence DESC
LIMIT 10;

-- Should use:
-- - FTS index for MATCH
-- - idx_learnings_category for category filter
```

### Maintenance

```typescript
// Run periodically
export function optimizeDatabase(db: Database.Database): void {
  // Rebuild FTS index
  db.exec('INSERT INTO learnings_fts(learnings_fts) VALUES("rebuild")');

  // Update statistics
  db.exec('ANALYZE');

  // Vacuum (compact database)
  db.exec('VACUUM');
}
```

---

## Backup & Restore

### Backup

```typescript
import Database from 'better-sqlite3';

export async function backupDatabase(sourceDb: Database.Database, destPath: string): Promise<void> {
  return new Promise((resolve, reject) => {
    const backup = sourceDb.backup(destPath);

    const doBackup = () => {
      const remaining = backup.step(100); // Pages per step
      if (remaining === 0) {
        backup.close();
        resolve();
      } else {
        setImmediate(doBackup);
      }
    };

    try {
      doBackup();
    } catch (error) {
      backup.close();
      reject(error);
    }
  });
}
```

### Restore

```typescript
export async function restoreDatabase(sourceDb: Database.Database, destPath: string): Promise<void> {
  const destDb = new Database(destPath);

  try {
    await backupDatabase(sourceDb, destPath);
    console.log(`Restored to ${destPath}`);
  } finally {
    destDb.close();
  }
}
```

### Verification

```typescript
export function verifyDatabase(db: Database.Database): boolean {
  const result = db.pragma('integrity_check', { simple: true });
  return result === 'ok';
}
```

---

## Related Documents

### Architecture
- [Global Context Network](../architecture/architecture-global-context-network-2025-01-16.md)
- [Database Schema Architecture](../architecture/architecture-database-schema-2025-01-16.md)

### Reference
- [Testing Strategy](./reference-testing-strategy-2025-01-16.md)
- [Subagent Types](./reference-subagent-types-2025-01-16.md)

### Guides
- [Database Setup Guide](../guides/guide-database-setup-2025-01-16.md)
````

## File: reference/reference-hook-configuration-2025-01-16.md
````markdown
# Hook Configuration Reference

> Complete reference for configuring Claude Code hooks with compiled JavaScript execution

---
title: Hook Configuration Reference
category: reference
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [reference, hooks, configuration, performance, build]
---

## Overview

This document provides the complete reference for configuring Claude Code hooks in the Global Context Network. It covers the canonical `.claude/hooks.json` format, TypeScript compilation requirements, performance budgets, environment variables, and cross-platform considerations.

**Critical Requirements**:
- Configuration path: `.claude/hooks.json` (canonical)
- Hook scripts: Compiled `.js` files (NOT `.ts`, NO `ts-node`)
- Input method: `stdin` JSON (canonical)
- Performance: <100ms p95 latency

## Table of Contents

1. [Configuration File Format](#configuration-file-format)
2. [Hook Types](#hook-types)
3. [Build Process](#build-process)
4. [Performance Budgets](#performance-budgets)
5. [Environment Variables](#environment-variables)
6. [Error Handling Configuration](#error-handling-configuration)
7. [Cross-Platform Considerations](#cross-platform-considerations)
8. [Complete Examples](#complete-examples)
9. [Troubleshooting](#troubleshooting)

## Configuration File Format

### Location

**Canonical path**: `.claude/hooks.json`

```
project-root/
├── .claude/
│   ├── hooks.json          # ← Configuration file (CANONICAL)
│   └── hooks/
│       ├── src/            # TypeScript source
│       │   ├── userPromptSubmit.ts
│       │   └── stop.ts
│       ├── dist/           # Compiled JavaScript (referenced by hooks.json)
│       │   ├── userPromptSubmit.js
│       │   └── stop.js
│       ├── tsconfig.json
│       └── package.json
```

**NOT**:
- ❌ `.claude/hooks/hooks.json` (incorrect path)
- ❌ `.claude/config.json` (wrong filename)
- ❌ `hooks.json` (missing `.claude/` directory)

### Schema

```typescript
interface HooksConfiguration {
  hooks: {
    [hookName: string]: string;  // Path to compiled .js file
  };
  config?: {
    eventQueuePath?: string;
    maxBufferSize?: number;
    fallbackToSampling?: boolean;
    samplingRate?: number;
    performanceBudgetMs?: number;
  };
}
```

### Minimal Configuration

```json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/dist/userPromptSubmit.js",
    "Stop": ".claude/hooks/dist/stop.js"
  }
}
```

### Full Configuration

```json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/dist/userPromptSubmit.js",
    "Stop": ".claude/hooks/dist/stop.js"
  },
  "config": {
    "eventQueuePath": "${PROJECT_ROOT}/.data/context.db",
    "maxBufferSize": 1000,
    "fallbackToSampling": true,
    "samplingRate": 0.1,
    "performanceBudgetMs": 100
  }
}
```

### Configuration Options

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `hooks` | object | `{}` | Map of hook names to script paths |
| `config.eventQueuePath` | string | `"${PROJECT_ROOT}/.data/context.db"` | SQLite database path |
| `config.maxBufferSize` | number | `1000` | Max events in queue before sampling |
| `config.fallbackToSampling` | boolean | `true` | Enable sampling under load |
| `config.samplingRate` | number | `0.1` | Sample rate when buffer full (0.1 = 10%) |
| `config.performanceBudgetMs` | number | `100` | p95 latency budget in milliseconds |

### Path Variables

Supported environment variable expansions:

| Variable | Expands To | Example |
|----------|------------|---------|
| `${PROJECT_ROOT}` | Absolute project directory | `/Users/alice/my-project` |
| `${HOME}` | User home directory | `/Users/alice` |
| `${TMPDIR}` | System temp directory | `/tmp` |

## Hook Types

### UserPromptSubmit

**Fires**: Before Claude processes user input

**Purpose**: Capture user prompts for context building

**Event Payload** (via stdin):
```json
{
  "prompt": "User's message text",
  "conversation_id": "uuid-v4-string",
  "session_id": "session-uuid",
  "project_id": "project-identifier",
  "timestamp": 1705401234567,
  "attachments": [
    {
      "name": "file.txt",
      "path": "/absolute/path/to/file.txt",
      "mime_type": "text/plain",
      "size": 1024
    }
  ]
}
```

**Configuration Example**:
```json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/dist/userPromptSubmit.js"
  }
}
```

### Stop

**Fires**: After Claude completes response generation

**Purpose**: Capture agent responses and tool usage

**Event Payload** (via stdin):
```json
{
  "response": "Assistant's response text",
  "conversation_id": "uuid-v4-string",
  "session_id": "session-uuid",
  "project_id": "project-identifier",
  "timestamp": 1705401234567,
  "tool_calls": [
    {
      "tool": "bash",
      "input": "ls -la",
      "output": "total 64\ndrwxr-xr-x ..."
    }
  ]
}
```

**Configuration Example**:
```json
{
  "hooks": {
    "Stop": ".claude/hooks/dist/stop.js"
  }
}
```

## Build Process

### Why Compiled JavaScript?

**Critical**: Hooks MUST use compiled `.js` files, NOT TypeScript via `ts-node`.

**Reasons**:
1. **Performance**: `ts-node` adds 50-200ms startup overhead (exceeds 100ms budget)
2. **Reliability**: No runtime TypeScript compilation errors
3. **Simplicity**: Single Node.js process, no transpiler overhead
4. **Production-ready**: Same execution model as deployed code

**❌ Wrong**:
```json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/src/userPromptSubmit.ts"  // ❌ ts-node too slow
  }
}
```

**✅ Correct**:
```json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/dist/userPromptSubmit.js"  // ✅ Compiled JS
  }
}
```

### TypeScript Configuration

**File**: `.claude/hooks/tsconfig.json`

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "commonjs",
    "moduleResolution": "node",
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "declaration": false,
    "sourceMap": false,
    "removeComments": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist", "**/*.test.ts"]
}
```

**Key settings**:
- `outDir: "./dist"`: Compiled output directory
- `target: "ES2022"`: Modern Node.js features
- `module: "commonjs"`: Node.js compatibility
- `sourceMap: false`: No source maps for performance
- `removeComments: true`: Smaller output files

### Build Scripts

**File**: `.claude/hooks/package.json`

```json
{
  "name": "claude-hooks",
  "version": "1.0.0",
  "private": true,
  "scripts": {
    "build": "tsc",
    "build:watch": "tsc --watch",
    "clean": "rm -rf dist",
    "rebuild": "npm run clean && npm run build",
    "pretest": "npm run build",
    "test": "vitest"
  },
  "devDependencies": {
    "@types/better-sqlite3": "^7.6.9",
    "@types/node": "^20.11.5",
    "typescript": "^5.3.3",
    "vitest": "^1.2.0"
  },
  "dependencies": {
    "better-sqlite3": "^9.2.2",
    "ulid": "^2.3.0"
  }
}
```

### Build Workflow

```bash
# Navigate to hooks directory
cd .claude/hooks

# Install dependencies
npm install

# Build TypeScript → JavaScript
npm run build

# Verify compiled output
ls -l dist/
# Expected output:
# userPromptSubmit.js
# stop.js

# Test hooks (uses compiled .js)
npm test
```

### Watch Mode (Development)

```bash
# Terminal 1: Watch for changes and rebuild
cd .claude/hooks
npm run build:watch

# Terminal 2: Test hooks
npm test
```

### Pre-Commit Hook (Optional)

Ensure hooks are always compiled before commit:

**File**: `.git/hooks/pre-commit`

```bash
#!/bin/bash

# Build hooks before commit
cd .claude/hooks
npm run build

if [ $? -ne 0 ]; then
  echo "Hook compilation failed. Fix errors before committing."
  exit 1
fi

# Add compiled files
git add dist/
```

Make executable:
```bash
chmod +x .git/hooks/pre-commit
```

## Performance Budgets

### Latency Requirements

| Metric | Budget | Measurement Point |
|--------|--------|-------------------|
| Hook execution (p95) | <100ms | Start to process exit |
| Fast sanitization (p95) | <50ms | Regex-based redaction only |
| Database write (p95) | <20ms | SQLite WAL insert |
| Total overhead (p95) | <100ms | User perceivable delay |

### Performance Breakdown

```
Total Hook Execution (100ms budget)
├── Read stdin: ~1-2ms
├── Parse JSON: ~1-2ms
├── Sanitize content: <50ms
├── Enqueue to SQLite: <20ms
├── Write logs: ~5-10ms
└── Overhead: ~10-15ms
```

### Monitoring Performance

**In Hook Code**:

```typescript
import { performance } from 'node:perf_hooks';

const startTime = performance.now();

// ... hook work ...

const duration = performance.now() - startTime;

if (duration > 100) {
  console.warn(`⚠ Hook exceeded budget: ${duration}ms (budget: 100ms)`);
}
```

**Log Analysis**:

```bash
# Check p95 latency
grep "Duration:" logs/hooks.log | \
  awk -F'Duration: ' '{print $2}' | \
  sed 's/ms//' | \
  sort -n | \
  awk 'BEGIN {c=0} {a[c++]=$1} END {print "p95:", a[int(c*0.95)]}'
```

### Optimization Strategies

If hooks exceed 100ms:

1. **Profile execution**:
   ```typescript
   console.time('sanitize');
   const result = fastSanitize(content);
   console.timeEnd('sanitize');
   ```

2. **Optimize regex patterns**:
   - Reduce pattern complexity
   - Limit lookaheads/lookbehinds
   - Use character classes efficiently

3. **Reduce database writes**:
   - Batch small events
   - Use WAL mode (not DELETE journal)
   - Minimize index overhead

4. **Enable sampling** (last resort):
   ```json
   {
     "config": {
       "fallbackToSampling": true,
       "samplingRate": 0.1
     }
   }
   ```

## Environment Variables

### Standard Variables

| Variable | Purpose | Example |
|----------|---------|---------|
| `DB_PATH` | Database location | `/path/to/context.db` |
| `LOG_LEVEL` | Logging verbosity | `error`, `warn`, `info`, `debug` |
| `HOOK_TIMEOUT_MS` | Override timeout | `100` |
| `NODE_ENV` | Environment | `development`, `production` |

### Configuration in Hook

**File**: `.claude/hooks/src/userPromptSubmit.ts`

```typescript
import { join } from 'node:path';

// Environment configuration
const CONFIG = {
  dbPath: process.env.DB_PATH || join(__dirname, '../../.data/context.db'),
  logLevel: (process.env.LOG_LEVEL || 'info') as 'error' | 'warn' | 'info' | 'debug',
  timeoutMs: parseInt(process.env.HOOK_TIMEOUT_MS || '100', 10),
  isDevelopment: process.env.NODE_ENV !== 'production'
};

// Use configuration
const db = new Database(CONFIG.dbPath);
```

### Setting Environment Variables

**Per-session** (testing):
```bash
export DB_PATH="/tmp/test-context.db"
export LOG_LEVEL="debug"
claude code
```

**Project-wide** (`.envrc` with direnv):
```bash
# .envrc
export DB_PATH="${PWD}/.data/context.db"
export LOG_LEVEL="info"
export NODE_ENV="production"
```

**In Claude Code** (if supported):
```bash
# .claude/env
DB_PATH=.data/context.db
LOG_LEVEL=info
```

## Error Handling Configuration

### Failure Modes

Hooks MUST fail silently and never block user interaction:

```typescript
try {
  // Hook work
  const result = await captureEvent(event);
} catch (error) {
  // Log error but DON'T throw
  logger.error('Hook failed', { error, event: sanitizeLogData(event) });

  // Optional: Emit metric
  metrics?.increment('hook.error');

  // NEVER throw to caller
}

// Always exit successfully
process.exit(0);
```

### Error Logging

**File-based logging** (reliable, no network):

```typescript
import { appendFileSync } from 'node:fs';
import { join } from 'node:path';

const LOG_PATH = join(__dirname, '../../logs/hooks.log');

function logError(error: Error, context: Record<string, unknown>): void {
  const timestamp = new Date().toISOString();
  const entry = {
    timestamp,
    level: 'error',
    message: error.message,
    stack: error.stack,
    context
  };

  try {
    appendFileSync(LOG_PATH, JSON.stringify(entry) + '\n');
  } catch {
    // If logging fails, fail completely silently
  }
}
```

### Retry Configuration

**NOT in hooks** (too slow). Use async workers instead:

```json
{
  "config": {
    "asyncWorker": {
      "maxRetries": 3,
      "retryDelayMs": 1000,
      "backoffMultiplier": 2
    }
  }
}
```

### Dead Letter Queue

Failed events move to dead letter queue after max retries:

```sql
CREATE TABLE IF NOT EXISTS dead_letter_queue (
  id TEXT PRIMARY KEY,
  original_event TEXT NOT NULL,
  error TEXT NOT NULL,
  attempts INTEGER NOT NULL,
  failed_at TEXT NOT NULL,
  created_at TEXT NOT NULL
);
```

## Cross-Platform Considerations

### Windows vs. Unix Paths

**Problem**: Path separators differ (`\` vs `/`)

**Solution**: Use `path.join()` and `path.resolve()`

```typescript
import { join, resolve } from 'node:path';

// ✅ Cross-platform
const dbPath = join(__dirname, '..', '..', '.data', 'context.db');

// ❌ Unix-only
const dbPath = `${__dirname}/../../.data/context.db`;
```

### Shebang Lines

**Unix/macOS** (executable):
```typescript
#!/usr/bin/env node

// Hook code...
```

**Windows** (no shebang needed):
```typescript
// Hook code (run with: node userPromptSubmit.js)
```

**Claude Code execution**: Handles platform differences automatically

### File Permissions

**Unix/macOS**:
```bash
chmod +x .claude/hooks/dist/*.js
```

**Windows**: No execution bit; Node.js invoked directly

### Line Endings

**Configure Git**:
```bash
# .gitattributes
*.js text eol=lf
*.ts text eol=lf
*.json text eol=lf
```

**Configure TypeScript**:
```json
{
  "compilerOptions": {
    "newLine": "lf"
  }
}
```

### SQLite Compatibility

`better-sqlite3` works across platforms but requires native bindings:

```bash
# Install with native compilation
npm install better-sqlite3

# Rebuild for current platform (if needed)
npm rebuild better-sqlite3
```

## Complete Examples

### Example 1: Minimal Configuration

**File**: `.claude/hooks.json`

```json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/dist/userPromptSubmit.js",
    "Stop": ".claude/hooks/dist/stop.js"
  }
}
```

**File**: `.claude/hooks/src/userPromptSubmit.ts`

```typescript
#!/usr/bin/env node

import { readFileSync } from 'node:fs';
import { performance } from 'node:perf_hooks';

const startTime = performance.now();

try {
  // Read stdin
  const input = readFileSync(0, 'utf-8');
  const event = JSON.parse(input);

  // Process event (fast)
  console.log(`Captured: ${event.prompt.substring(0, 50)}...`);

  const duration = performance.now() - startTime;
  if (duration > 100) {
    console.warn(`Slow hook: ${duration}ms`);
  }
} catch (error) {
  console.error('Hook error:', error);
}

process.exit(0);
```

**Build**:
```bash
cd .claude/hooks
npm run build
```

### Example 2: Production Configuration

**File**: `.claude/hooks.json`

```json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/dist/userPromptSubmit.js",
    "Stop": ".claude/hooks/dist/stop.js"
  },
  "config": {
    "eventQueuePath": "${PROJECT_ROOT}/.data/context.db",
    "maxBufferSize": 1000,
    "fallbackToSampling": true,
    "samplingRate": 0.1,
    "performanceBudgetMs": 100
  }
}
```

**File**: `.claude/hooks/src/userPromptSubmit.ts`

```typescript
#!/usr/bin/env node

import { readFileSync, appendFileSync } from 'node:fs';
import { join } from 'node:path';
import { performance } from 'node:perf_hooks';
import Database from 'better-sqlite3';
import { ulid } from 'ulid';

// Configuration
const DB_PATH = process.env.DB_PATH || join(__dirname, '../../.data/context.db');
const LOG_PATH = join(__dirname, '../../logs/hooks.log');

interface HookEvent {
  prompt: string;
  conversation_id?: string;
  session_id: string;
  timestamp: number;
}

function log(message: string): void {
  const timestamp = new Date().toISOString();
  try {
    appendFileSync(LOG_PATH, `[${timestamp}] UserPromptSubmit: ${message}\n`);
  } catch {
    // Fail silently
  }
}

function sanitize(text: string): string {
  // Fast regex-based sanitization
  return text
    .replace(/\bsk-[a-zA-Z0-9]{48}\b/g, '[REDACTED_API_KEY]')
    .replace(/\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g, '[REDACTED_EMAIL]')
    .replace(/\/Users\/[^\/\s]+\/[^\s]*/g, '[REDACTED_PATH]');
}

function main(): void {
  const startTime = performance.now();

  try {
    // Read stdin
    const input = readFileSync(0, 'utf-8');
    const event: HookEvent = JSON.parse(input);

    // Sanitize BEFORE storage
    const sanitizedContent = sanitize(event.prompt);

    // Queue event
    const db = new Database(DB_PATH);
    db.pragma('journal_mode = WAL');

    const conversationId = event.conversation_id || ulid();
    const messageId = ulid();

    db.prepare(`
      INSERT INTO messages (id, conversation_id, role, content, created_at)
      VALUES (?, ?, ?, ?, ?)
    `).run(messageId, conversationId, 'user', sanitizedContent, new Date().toISOString());

    db.close();

    const duration = performance.now() - startTime;
    log(`Captured user prompt. Duration: ${duration}ms`);

    if (duration > 100) {
      log(`⚠ WARNING: Hook exceeded budget: ${duration}ms`);
    }
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error);
    log(`Error: ${errorMsg}`);
  }

  process.exit(0);
}

main();
```

**File**: `.claude/hooks/tsconfig.json`

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "commonjs",
    "moduleResolution": "node",
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "declaration": false,
    "sourceMap": false,
    "removeComments": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
```

**Build & Test**:
```bash
cd .claude/hooks
npm install
npm run build
npm test
```

## Troubleshooting

### Hook Not Executing

**Symptoms**: Events not captured, no logs

**Diagnosis**:
```bash
# Check configuration
cat .claude/hooks.json

# Verify compiled files exist
ls -l .claude/hooks/dist/

# Check file permissions (Unix/macOS)
ls -l .claude/hooks/dist/*.js

# Test hook manually
echo '{"prompt":"test"}' | node .claude/hooks/dist/userPromptSubmit.js
```

**Solutions**:
1. Verify `.claude/hooks.json` path (not `.claude/hooks/hooks.json`)
2. Rebuild TypeScript: `cd .claude/hooks && npm run build`
3. Check hook points to `.js` file, not `.ts`
4. Ensure Node.js installed: `node --version`

### Hook Too Slow (>100ms)

**Symptoms**: Laggy user experience, timeout warnings in logs

**Diagnosis**:
```bash
# Check p95 latency
grep "Duration:" logs/hooks.log | \
  awk -F'Duration: ' '{print $2}' | \
  sed 's/ms//' | \
  sort -n | \
  tail -n 5
```

**Solutions**:
1. **Profile execution**:
   ```typescript
   console.time('sanitize');
   const result = sanitize(content);
   console.timeEnd('sanitize');
   ```

2. **Optimize regex** (see [Sanitization Standard](../STANDARDS.md#9-sanitization-standard))

3. **Use WAL mode**:
   ```typescript
   db.pragma('journal_mode = WAL');
   db.pragma('synchronous = NORMAL');
   ```

4. **Reduce logging**:
   ```typescript
   if (process.env.LOG_LEVEL === 'debug') {
     log(details);
   }
   ```

### Database Locked

**Symptoms**: `SQLITE_BUSY` errors

**Diagnosis**:
```bash
# Check WAL mode
sqlite3 .data/context.db "PRAGMA journal_mode;"
# Expected: wal
```

**Solutions**:
1. **Enable WAL mode**:
   ```typescript
   db.pragma('journal_mode = WAL');
   ```

2. **Set busy timeout**:
   ```typescript
   db.pragma('busy_timeout = 5000');
   ```

3. **Close database connections**:
   ```typescript
   db.close();
   ```

### Compilation Errors

**Symptoms**: `tsc` fails, no `.js` files in `dist/`

**Diagnosis**:
```bash
cd .claude/hooks
npm run build
```

**Solutions**:
1. **Fix TypeScript errors** (strict mode required)
2. **Check tsconfig.json** (verify `outDir`, `rootDir`)
3. **Update dependencies**:
   ```bash
   npm install --save-dev typescript@latest
   ```

### Cross-Platform Path Issues

**Symptoms**: Hooks work on macOS/Linux but fail on Windows

**Solutions**:
1. **Use `path.join()`**:
   ```typescript
   import { join } from 'node:path';
   const dbPath = join(__dirname, '..', '..', '.data', 'context.db');
   ```

2. **Normalize paths**:
   ```typescript
   import { normalize } from 'node:path';
   const normalized = normalize(userPath);
   ```

3. **Configure line endings**:
   ```gitattributes
   *.js text eol=lf
   *.ts text eol=lf
   ```

## Related Documents

### Architecture
- [Hooks & Event Capture Architecture](../architecture/architecture-hooks-event-capture-2025-01-16.md)
- [Sanitization Pipeline Architecture](../architecture/architecture-sanitization-pipeline-2025-01-16.md)

### Standards
- [Project Standards](../STANDARDS.md) - Section 5: Hook Configuration Standard
- [Privacy & Data Flow Standard](../STANDARDS.md#1-privacy--data-flow-standard-most-critical)

### Guides
- [Phase 1 Hook Development Guide](../guides/guide-phase-1-hook-development-2025-01-16.md)

---

**Last Updated**: 2025-01-16
**Version**: 1.0.0
**Canonical Path**: `.claude/hooks.json`
**Hook Format**: Compiled `.js` (NOT `ts-node`)
**IO Method**: `stdin` JSON (canonical)
**Performance Budget**: <100ms p95
````

## File: reference/reference-subagent-types-2025-01-16.md
````markdown
# Subagent Types Reference

> Complete catalog of all subagent types with configurations, prompts, and usage patterns

---
title: Subagent Types Reference
category: reference
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [subagents, claude-agent-sdk, development, testing]
applies_to: Claude Agent SDK 1.x, Sonnet 4.5, Haiku 3.5
---

## Overview

This document provides a complete catalog of all subagent types used in the Global Context Network development. Each subagent is specialized for a specific task with defined inputs, outputs, tools, and success criteria.

**Total Subagents**: 22
- **Implementation**: 14 subagents
- **Test Generation**: 3 subagents
- **Test Validation**: 3 subagents
- **Quality Gates**: 3 subagents

## Subagent Contract Template

Each subagent follows this contract:

```typescript
interface SubagentContract {
  name: string;
  description: string;
  phase: number;
  category: 'implementation' | 'test-generation' | 'test-validation' | 'quality-gate';
  model: 'sonnet' | 'haiku';
  tools: string[];
  input_schema: object;
  output_schema: object;
  prompt_template: string;
  success_criteria: string[];
  timeout_ms: number;
  retry_policy: RetryPolicy;
  cost_sensitivity: 'low' | 'medium' | 'high';
  idempotent: boolean;
  concurrency: 'sequential' | 'parallel';
}
```

---

## Phase 0: Foundation

### foundation-setup-agent

**Purpose**: Initialize TypeScript project with Vitest testing infrastructure

**Configuration**:
```typescript
{
  name: "foundation-setup-agent",
  description: "Sets up TypeScript project with strict mode, Vitest, and project structure",
  phase: 0,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Bash", "Read"],
  timeout_ms: 120000,
  cost_sensitivity: "low",
  idempotent: true,
  concurrency: "sequential"
}
```

**Input Schema**:
```typescript
{
  project_name: string;
  node_version: string; // e.g., "20"
  package_manager: "npm" | "pnpm" | "yarn";
}
```

**Output Schema**:
```typescript
{
  files_created: string[];
  dependencies_installed: boolean;
  typescript_version: string;
  vitest_version: string;
}
```

**Prompt Template**:
```
You are a TypeScript project setup expert. Initialize a new TypeScript project with:

1. package.json with dependencies:
   - typescript (latest)
   - vitest (latest)
   - @types/node
   - @typescript-eslint/parser
   - @typescript-eslint/eslint-plugin

2. tsconfig.json with strict mode:
   - "strict": true
   - "esModuleInterop": true
   - "skipLibCheck": true
   - "target": "ES2022"
   - "module": "ESNext"
   - "moduleResolution": "bundler"

3. vitest.config.ts following reference-testing-strategy

4. Project structure:
   - src/
   - tests/
   - .gitignore

5. Install dependencies using {package_manager}

IMPORTANT:
- Use strict TypeScript settings
- Never use "any" types
- All configs must be valid JSON/TypeScript
```

**Success Criteria**:
- ✅ All files created
- ✅ Dependencies installed successfully
- ✅ `pnpm run type-check` passes
- ✅ `pnpm test` can run (even with no tests)

**Common Pitfalls**:
- Forgetting to enable strict mode
- Incorrect module resolution settings
- Missing test setup files

---

### database-schema-agent

**Purpose**: Create SQLite database schema with migrations

**Configuration**:
```typescript
{
  name: "database-schema-agent",
  description: "Designs and implements SQLite database schema with migration system",
  phase: 0,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Bash", "Read"],
  timeout_ms: 90000,
  cost_sensitivity: "medium",
  idempotent: true,
  concurrency: "parallel" // Can run with test-infrastructure
}
```

**Input Schema**:
```typescript
{
  tables: Array<{
    name: string;
    columns: Array<{ name: string; type: string; constraints?: string[] }>;
    indexes?: Array<{ columns: string[]; unique?: boolean }>;
  }>;
  enable_fts: boolean; // Full-text search for learnings
}
```

**Output Schema**:
```typescript
{
  migration_files: string[];
  schema_version: string;
  tables_created: string[];
  indexes_created: string[];
}
```

**Prompt Template**:
```
You are a SQLite database expert. Create a production-ready database schema with:

1. Migration system (migrations/001_initial.sql, 002_add_fts.sql, etc.)

2. All tables from input schema with:
   - Primary keys (id TEXT PRIMARY KEY)
   - Foreign keys (with ON DELETE CASCADE where appropriate)
   - Timestamps (created_at, updated_at with triggers)
   - CHECK constraints for data validation

3. Indexes for all foreign keys and frequently queried columns

4. FTS5 virtual table for learnings.content if enable_fts=true

5. Migration runner in TypeScript (src/database/migrations/runner.ts)

CRITICAL REQUIREMENTS:
- Enable foreign keys: PRAGMA foreign_keys = ON
- Use WAL mode: PRAGMA journal_mode = WAL
- All timestamps as ISO 8601 strings
- Add CHECK constraints for confidence (0.0 to 1.0)
- Ensure ACID compliance

NEVER:
- Store unsanitized data (no "raw_content" columns)
- Use INTEGER for IDs (use TEXT for UUIDs/ULIDs)
- Forget indexes on foreign keys
```

**Success Criteria**:
- ✅ Migration runner executes without errors
- ✅ All tables created with proper constraints
- ✅ Foreign keys enforced
- ✅ Indexes created for performance
- ✅ FTS5 table if requested

**Common Pitfalls**:
- Forgetting PRAGMA foreign_keys = ON
- Not using WAL mode
- Missing indexes on foreign keys
- Incorrect timestamp handling

---

### test-infrastructure-agent

**Purpose**: Set up test utilities, helpers, and fixtures

**Configuration**:
```typescript
{
  name: "test-infrastructure-agent",
  description: "Creates test helpers, fixture factories, and test database utilities",
  phase: 0,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Read"],
  timeout_ms: 60000,
  cost_sensitivity: "low",
  idempotent: true,
  concurrency: "parallel"
}
```

**Input Schema**:
```typescript
{
  test_types: Array<"unit" | "integration" | "e2e">;
  fixture_types: Array<"conversation" | "message" | "learning">;
}
```

**Output Schema**:
```typescript
{
  helper_files: string[];
  fixture_files: string[];
  test_setup_complete: boolean;
}
```

**Prompt Template**:
```
You are a test infrastructure expert. Create comprehensive test utilities:

1. Test setup (tests/setup.ts):
   - In-memory SQLite database
   - beforeEach/afterEach hooks
   - Test database cleanup

2. Fixture factories (tests/helpers/fixture-factory.ts):
   - createConversation()
   - createMessage()
   - createLearning()
   - Use @faker-js/faker for realistic data

3. Test helpers (tests/helpers/):
   - db-helpers.ts (query helpers, assertions)
   - mock-helpers.ts (LLM, MCP, IPFS mocks)

4. Sample fixtures (tests/fixtures/):
   - conversations/with-pii.json
   - sanitization/pii-corpus.json

REQUIREMENTS:
- All factories return valid, type-safe objects
- Fixtures should be realistic but synthetic
- Mock helpers should be deterministic
- Follow reference-testing-strategy guidelines
```

**Success Criteria**:
- ✅ Test setup file creates in-memory database
- ✅ Fixture factories generate valid data
- ✅ Mock helpers provide deterministic results
- ✅ Sample fixtures load successfully

---

## Phase 1: Event Capture

### hook-developer-agent

**Purpose**: Implement Claude Code hooks for event capture

**Configuration**:
```typescript
{
  name: "hook-developer-agent",
  description: "Creates UserPromptSubmit and Stop hooks with <100ms execution",
  phase: 1,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Read", "Bash"],
  timeout_ms: 120000,
  cost_sensitivity: "high", // Critical path
  idempotent: true,
  concurrency: "sequential"
}
```

**Input Schema**:
```typescript
{
  hook_types: Array<"UserPromptSubmit" | "Stop">;
  output_path: string; // Where to write events
  max_execution_ms: number; // Default 100
}
```

**Output Schema**:
```typescript
{
  hook_scripts: string[];
  performance_p95: number; // Measured in tests
  error_handling: "silent" | "logged";
}
```

**Prompt Template**:
```
You are a Claude Code hooks expert. Implement event capture hooks with STRICT performance requirements:

1. UserPromptSubmit hook (.claude/hooks/userPromptSubmit.ts):
   - Captures user input
   - Writes to event queue
   - MUST execute in <100ms P95
   - NEVER block user interaction

2. Stop hook (.claude/hooks/stop.ts):
   - Captures agent response
   - Includes thinking if available
   - MUST execute in <100ms P95
   - Silent error handling (log but don't throw)

3. Event serialization:
   - Correlation ID for conversation tracking
   - Timestamp (ISO 8601)
   - Event type
   - Payload

CRITICAL REQUIREMENTS:
- P95 latency < 100ms (measured in tests)
- Fail silently with logging
- Never throw errors that block user
- Atomic writes to queue
- Handle process crashes gracefully

FORBIDDEN:
- Synchronous I/O in hooks
- Network calls
- Complex processing
- Throwing errors to user
```

**Success Criteria**:
- ✅ Hooks execute in <100ms P95
- ✅ Events persisted atomically
- ✅ Silent error handling
- ✅ No user-blocking behavior
- ✅ Performance tests pass

**Common Pitfalls**:
- Synchronous file I/O
- Complex sanitization in hooks (do async)
- Not handling errors gracefully
- Missing correlation IDs

---

### event-collector-agent

**Purpose**: Aggregate events into conversations

**Configuration**:
```typescript
{
  name: "event-collector-agent",
  description: "Groups events by conversation with session tracking",
  phase: 1,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Read"],
  timeout_ms: 90000,
  cost_sensitivity: "medium",
  idempotent: true,
  concurrency: "parallel"
}
```

**Input Schema**:
```typescript
{
  session_timeout_ms: number; // Default 30 minutes
  correlation_strategy: "session_id" | "time_window" | "hybrid";
}
```

**Output Schema**:
```typescript
{
  collector_module: string;
  conversations_created: number;
  events_processed: number;
}
```

**Prompt Template**:
```
You are an event aggregation expert. Implement conversation collector:

1. Event aggregation (src/events/collector.ts):
   - Group events by correlation_id
   - Detect conversation boundaries
   - Handle session timeouts
   - Support multiple concurrent conversations

2. Conversation assembly:
   - Pair UserPromptSubmit with Stop events
   - Preserve message order
   - Track conversation state (active/complete)

3. Persistence:
   - Write complete conversations to queue
   - Handle partial conversations (session timeout)

REQUIREMENTS:
- Conversations must be ordered correctly
- Handle concurrent sessions
- Graceful session timeout handling
- Idempotent processing (replay-safe)
```

**Success Criteria**:
- ✅ Events correctly grouped by conversation
- ✅ Message order preserved
- ✅ Session timeouts handled
- ✅ Concurrent conversations supported

---

### queue-system-agent

**Purpose**: Implement persistent job queue

**Configuration**:
```typescript
{
  name: "queue-system-agent",
  description: "Creates SQLite-based job queue with priority and retry logic",
  phase: 1,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Read", "Bash"],
  timeout_ms: 90000,
  cost_sensitivity: "high",
  idempotent: true,
  concurrency: "sequential"
}
```

**Input Schema**:
```typescript
{
  job_types: string[]; // e.g., ["sanitize", "extract_learning", "upload"]
  priority_levels: number; // Default 3 (high, medium, low)
  max_retries: number; // Default 3
}
```

**Output Schema**:
```typescript
{
  queue_module: string;
  tables_created: string[];
  worker_ready: boolean;
}
```

**Prompt Template**:
```
You are a job queue expert. Implement a robust, persistent queue system:

1. Queue table (job_queue):
   - id, type, status, priority, run_at, locked_at, locked_by
   - payload (JSON), attempts, max_retries, last_error
   - Indexes for (status, priority, run_at)

2. Queue operations (src/queue/queue.ts):
   - enqueue(type, payload, options)
   - dequeue() with priority + run_at ordering
   - markComplete(id)
   - markFailed(id, error)
   - requeueWithBackoff(id)

3. Concurrency control:
   - Optimistic locking (UPDATE ... WHERE locked_at IS NULL)
   - Worker heartbeats
   - Dead letter queue for max retries exceeded

4. Retry logic:
   - Exponential backoff: 2^attempt * 1000ms
   - Max 3 retries by default
   - Quarantine after max retries

CRITICAL:
- ACID compliance for job claiming
- No duplicate processing
- Handle worker crashes (stale locks)
- Observable metrics (queue depth, latency)
```

**Success Criteria**:
- ✅ Jobs persist across restarts
- ✅ No duplicate processing
- ✅ Priority ordering respected
- ✅ Retry logic with backoff
- ✅ Dead letter queue for failures

---

## Phase 2: Sanitization

### rule-sanitizer-agent

**Purpose**: Regex-based PII detection

**Configuration**:
```typescript
{
  name: "rule-sanitizer-agent",
  description: "Fast regex-based PII detection with <1% false positive rate",
  phase: 2,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Read"],
  timeout_ms: 90000,
  cost_sensitivity: "high",
  idempotent: true,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are a PII detection expert. Implement rule-based sanitization:

1. Detection patterns (src/sanitization/patterns.ts):
   - API keys (AWS, OpenAI, GitHub, etc.)
   - File paths (absolute with usernames)
   - Email addresses
   - IP addresses
   - Phone numbers
   - Credit cards

2. Rule detector (src/sanitization/rule-detector.ts):
   - detectPII(content) → Detection[]
   - Fast execution (<10ms per message)
   - <1% false positive rate on test corpus

3. Replacement strategy:
   - [API_KEY], [FILE_PATH], [EMAIL], etc.
   - Preserve structure for code examples
   - Audit log all detections

REQUIREMENTS:
- All patterns tested against corpus
- Performance: <10ms per 1KB content
- False positive rate <1%
- False negative rate <5%
```

**Success Criteria**:
- ✅ Detects all PII types from corpus
- ✅ Execution time <10ms per message
- ✅ False positive rate <1%
- ✅ Audit log complete

---

### ai-sanitizer-agent

**Purpose**: LLM-powered context-aware sanitization

**Configuration**:
```typescript
{
  name: "ai-sanitizer-agent",
  description: "Context-aware PII detection using LLM with <5% false negative rate",
  phase: 2,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Read"],
  timeout_ms: 120000,
  cost_sensitivity: "high",
  idempotent: false, // LLM calls not deterministic
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are an AI-powered sanitization expert. Implement LLM-based PII detection:

1. AI Sanitizer (src/sanitization/ai-sanitizer.ts):
   - Use Claude API for context-aware detection
   - Distinguish person names from variable names
   - Handle company-specific terminology
   - Provide confidence scores

2. Prompt engineering:
   - "Identify PII in this developer conversation..."
   - Return structured JSON with detections
   - Include confidence (0-1)
   - Explain each detection

3. Error handling:
   - Timeout protection (5s max)
   - Fallback to rule-based if API fails
   - Rate limiting
   - Cost tracking

REQUIREMENTS:
- False negative rate <5%
- Execution time <2s per message
- Graceful degradation on API errors
- Cost awareness (token usage)
```

**Success Criteria**:
- ✅ Detects contextual PII (names vs variables)
- ✅ False negative rate <5%
- ✅ API failures handled gracefully
- ✅ Cost tracking implemented

---

### sanitization-pipeline-agent

**Purpose**: Orchestrate hybrid sanitization

**Configuration**:
```typescript
{
  name: "sanitization-pipeline-agent",
  description: "Combines rule-based and AI sanitization with validation",
  phase: 2,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Read"],
  timeout_ms: 90000,
  cost_sensitivity: "high",
  idempotent: true,
  concurrency: "sequential"
}
```

**Prompt Template**:
```
You are a sanitization pipeline expert. Combine rule-based and AI sanitization:

1. Pipeline orchestration (src/sanitization/pipeline.ts):
   - Run rule-based detector first (fast)
   - Run AI sanitizer for validation
   - Merge results with deduplication
   - Apply replacements
   - Generate audit log

2. Hybrid validation:
   - Rules catch obvious cases
   - AI validates and enhances
   - Confidence scoring
   - Human review queue for low confidence

3. Output:
   - Sanitized content
   - Detection list
   - Audit log
   - Confidence score

CRITICAL:
- NEVER store unsanitized content
- All detections logged
- Replacements deterministic
- Preserve code structure
```

**Success Criteria**:
- ✅ Zero PII leaks in test corpus
- ✅ Combined FP <1%, FN <3%
- ✅ Complete audit trail
- ✅ Execution time <2s per conversation

---

## Phase 3: Database & Storage

### repository-agent

**Purpose**: Implement repository pattern for database access

**Configuration**:
```typescript
{
  name: "repository-agent",
  description: "Creates type-safe repository interfaces with transaction support",
  phase: 3,
  category: "implementation",
  model: "sonnet",
  tools: ["Write", "Read"],
  timeout_ms: 90000,
  cost_sensitivity: "medium",
  idempotent: true,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are a database repository expert. Implement the repository pattern:

1. Base repository (src/database/repositories/base-repository.ts):
   - Transaction support
   - Error handling
   - Type safety
   - Query builders

2. Specific repositories:
   - ConversationRepository
   - MessageRepository
   - LearningRepository
   - JobQueueRepository

3. Repository methods:
   - create(data) → Promise<T>
   - findById(id) → Promise<T | null>
   - update(id, data) → Promise<T>
   - delete(id) → Promise<void>
   - Custom queries (findByConversationId, etc.)

REQUIREMENTS:
- All methods type-safe
- Parameterized queries (no SQL injection)
- Transaction support
- Error wrapping
- NEVER accept unsanitized content
```

**Success Criteria**:
- ✅ All repositories implemented
- ✅ Type safety enforced
- ✅ Transactions working
- ✅ No SQL injection vulnerabilities

---

## Test Generation Subagents

### unit-test-generator

**Configuration**:
```typescript
{
  name: "unit-test-generator",
  description: "Generates comprehensive unit tests with >85% coverage",
  category: "test-generation",
  model: "sonnet",
  tools: ["Write", "Read", "mcp__test-runner__run_unit_tests"],
  timeout_ms: 120000,
  cost_sensitivity: "medium",
  idempotent: false,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are a unit test expert. Generate comprehensive unit tests:

1. Test structure:
   - Arrange-Act-Assert pattern
   - Clear test names (should/when/given)
   - One assertion per test
   - Proper mocking of dependencies

2. Coverage:
   - All functions tested
   - Edge cases (empty, null, undefined, boundary values)
   - Error conditions
   - Happy path and sad path

3. Test quality:
   - Fast (<10ms per test)
   - Isolated (no side effects)
   - Deterministic (no flaky tests)
   - Maintainable (clear, DRY)

TARGET: >85% coverage with high-quality tests
```

**Success Criteria**:
- ✅ Coverage ≥85%
- ✅ All tests pass
- ✅ No flaky tests
- ✅ Test quality score ≥0.8

---

### integration-test-generator

**Configuration**:
```typescript
{
  name: "integration-test-generator",
  description: "Creates integration tests for component interactions",
  category: "test-generation",
  model: "sonnet",
  tools: ["Write", "Read", "Bash"],
  timeout_ms: 120000,
  cost_sensitivity: "medium",
  idempotent: false,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are an integration test expert. Generate integration tests:

1. Component interactions:
   - Hook → Queue
   - Sanitization → Database
   - Learning extraction → Upload
   - MCP server → Database

2. Test scenarios:
   - Multi-step workflows
   - Error propagation
   - Transaction rollback
   - Concurrent operations

3. Test data:
   - Use fixture factories
   - Test database per test
   - Cleanup after each test

TARGET: Cover all component boundaries
```

**Success Criteria**:
- ✅ All component interactions tested
- ✅ Error scenarios covered
- ✅ Transactions validated
- ✅ No test pollution

---

### e2e-test-generator

**Configuration**:
```typescript
{
  name: "e2e-test-generator",
  description: "Creates end-to-end workflow tests",
  category: "test-generation",
  model: "sonnet",
  tools: ["Write", "Read", "Bash"],
  timeout_ms: 180000,
  cost_sensitivity: "low",
  idempotent: false,
  concurrency: "sequential"
}
```

**Prompt Template**:
```
You are an E2E test expert. Generate end-to-end tests:

1. Complete workflows:
   - User prompt → Sanitization → Database → MCP query
   - Conversation → Learning extraction → Network upload
   - Hook capture → Processing → Availability

2. System verification:
   - Real database (ephemeral)
   - Real MCP server (test instance)
   - Mocked external services (IPFS, blockchain)

3. Failure scenarios:
   - Service unavailable
   - Timeout handling
   - Retry logic
   - Recovery procedures

TARGET: Validate complete system behavior
```

**Success Criteria**:
- ✅ Happy path works end-to-end
- ✅ Failure scenarios handled
- ✅ Recovery tested
- ✅ Performance acceptable

---

## Test Validation Subagents

### test-quality-validator

**Configuration**:
```typescript
{
  name: "test-quality-validator",
  description: "Reviews test code quality and completeness",
  category: "test-validation",
  model: "sonnet",
  tools: ["Read", "Grep"],
  timeout_ms: 60000,
  cost_sensitivity: "low",
  idempotent: true,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are a test quality reviewer. Validate test quality:

1. Structure:
   - Proper describe/it nesting
   - Clear test names
   - AAA pattern
   - One assertion per test

2. Coverage:
   - Edge cases covered
   - Error conditions tested
   - Assertions not just truthy

3. Quality:
   - No test.skip without issue reference
   - No disabled assertions
   - Proper cleanup
   - No magic numbers

SCORE: 0-1, require >0.8 to pass
```

**Success Criteria**:
- ✅ Quality score ≥0.8
- ✅ All issues documented
- ✅ Recommendations provided

---

### coverage-validator

**Configuration**:
```typescript
{
  name: "coverage-validator",
  description: "Ensures adequate test coverage",
  category: "test-validation",
  model: "haiku",
  tools: ["Bash", "Read"],
  timeout_ms: 30000,
  cost_sensitivity: "low",
  idempotent: true,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are a coverage validator. Analyze test coverage:

1. Coverage metrics:
   - Lines, statements, functions, branches
   - Per-file coverage
   - Critical path 100% coverage

2. Gaps:
   - Uncovered lines
   - Uncovered branches
   - Missing error handlers

3. Report:
   - Coverage percentage
   - Critical gaps
   - Recommendations

REQUIRE: >85% coverage, critical paths 100%
```

**Success Criteria**:
- ✅ Coverage ≥85%
- ✅ Critical paths 100%
- ✅ Gaps identified

---

### implementation-validator

**Configuration**:
```typescript
{
  name: "implementation-validator",
  description: "Validates implementation against tests",
  category: "test-validation",
  model: "sonnet",
  tools: ["Read", "Bash"],
  timeout_ms: 90000,
  cost_sensitivity: "medium",
  idempotent: true,
  concurrency: "sequential"
}
```

**Prompt Template**:
```
You are an implementation validator. Verify implementation:

1. Test alignment:
   - All tests pass
   - Implementation matches specs
   - No untested edge cases

2. Code quality:
   - TypeScript strict mode
   - No "any" types
   - Proper error handling

3. Security:
   - No SQL injection
   - No command injection
   - No hardcoded secrets

4. Performance:
   - Meets SLAs
   - No obvious bottlenecks

REQUIRE: All tests pass, no security issues
```

**Success Criteria**:
- ✅ All tests pass
- ✅ Security scan clean
- ✅ Performance acceptable
- ✅ Code quality high

---

## Quality Gate Subagents

### code-quality-validator

**Configuration**:
```typescript
{
  name: "code-quality-validator",
  description: "Reviews code quality and standards",
  category: "quality-gate",
  model: "sonnet",
  tools: ["Read", "Bash"],
  timeout_ms: 60000,
  cost_sensitivity: "low",
  idempotent: true,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are a code quality reviewer. Enforce standards:

1. TypeScript:
   - Strict mode compliance
   - No "any" types
   - Proper type annotations

2. Code style:
   - ESLint passes
   - Prettier formatted
   - Consistent naming

3. Best practices:
   - DRY (Don't Repeat Yourself)
   - SOLID principles
   - Clear function names
   - Proper error handling

BLOCK: If any critical issues found
```

**Success Criteria**:
- ✅ ESLint passes
- ✅ TypeScript strict mode
- ✅ No "any" types
- ✅ Formatted correctly

---

### security-validator

**Configuration**:
```typescript
{
  name: "security-validator",
  description: "Scans for security vulnerabilities",
  category: "quality-gate",
  model: "sonnet",
  tools: ["Read", "Bash"],
  timeout_ms: 90000,
  cost_sensitivity: "medium",
  idempotent: true,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are a security auditor. Scan for vulnerabilities:

1. Code analysis:
   - SQL injection (use parameterized queries)
   - Command injection (no shell=true)
   - Path traversal (validate paths)
   - XSS vectors (sanitize output)

2. Dependencies:
   - Known vulnerabilities
   - Outdated packages
   - License issues

3. Secrets:
   - No hardcoded API keys
   - No passwords in code
   - Proper env var usage

BLOCK: If any critical vulnerabilities found
```

**Success Criteria**:
- ✅ No SQL injection
- ✅ No command injection
- ✅ No hardcoded secrets
- ✅ Dependencies clean

---

### performance-validator

**Configuration**:
```typescript
{
  name: "performance-validator",
  description: "Analyzes performance characteristics",
  category: "quality-gate",
  model: "sonnet",
  tools: ["Read", "Bash"],
  timeout_ms: 60000,
  cost_sensitivity: "low",
  idempotent: true,
  concurrency: "parallel"
}
```

**Prompt Template**:
```
You are a performance analyst. Check performance:

1. Algorithm complexity:
   - O(n) acceptable, O(n²) flag
   - No unnecessary loops
   - Proper data structures

2. Database:
   - Indexes on foreign keys
   - No N+1 queries
   - Batch operations where possible

3. Resource usage:
   - No memory leaks
   - Proper stream handling
   - Connection pooling

FLAG: Performance regressions or obvious bottlenecks
```

**Success Criteria**:
- ✅ Algorithm complexity acceptable
- ✅ No N+1 queries
- ✅ No obvious bottlenecks
- ✅ Resource usage reasonable

---

## Orchestration Patterns

### Parallel Execution

```typescript
import { query } from '@anthropic-ai/claude-agent-sdk';

// Phase 0: All can run in parallel
const response = query({
  prompt: 'Implement Phase 0 Foundation',
  options: {
    model: 'claude-sonnet-4-5',
    agents: {
      'foundation-setup-agent': foundationSetupConfig,
      'database-schema-agent': databaseSchemaConfig,
      'test-infrastructure-agent': testInfraConfig
    }
  }
});

for await (const message of response) {
  if (message.type === 'system' && message.subtype === 'subagent_end') {
    console.log(`Completed: ${message.agent_name}`);
  }
}
```

### Sequential with Dependencies

```typescript
// Phase 1: Sequential dependencies
// 1. Setup hooks first
const hooksResult = await query({
  prompt: 'Implement hooks',
  options: {
    agents: { 'hook-developer-agent': hookConfig }
  }
});

// 2. Then event collector (needs hooks to test)
const collectorResult = await query({
  prompt: 'Implement event collector',
  options: {
    agents: { 'event-collector-agent': collectorConfig }
  }
});

// 3. Finally queue system
const queueResult = await query({
  prompt: 'Implement queue system',
  options: {
    agents: { 'queue-system-agent': queueConfig }
  }
});
```

## Related Documents

### Architecture
- [Subagent System Architecture](../architecture/architecture-subagent-system-2025-01-16.md)
- [Global Context Network](../architecture/architecture-global-context-network-2025-01-16.md)

### Reference
- [Claude Agent SDK API](./reference-claude-agent-sdk-api-2025-01-16.md)
- [Testing Strategy](./reference-testing-strategy-2025-01-16.md)
- [Database Schema](./reference-database-schema-2025-01-16.md)

### Guides
- [Using Subagents](../guides/guide-using-subagents-2025-01-16.md)
- [TDD Workflow](../guides/guide-tdd-workflow-2025-01-16.md)
````

## File: reference/reference-testing-strategy-2025-01-16.md
````markdown
# Testing Strategy Reference

> Comprehensive testing strategy, coverage requirements, and test organization for Global Context Network

---
title: Testing Strategy Reference
category: reference
date: 2025-01-16
status: active
authors: Claude + Dennison
tags: [testing, tdd, vitest, coverage, quality]
applies_to: Vitest 1.x, Node.js 20+
---

## Overview

This document defines the complete testing strategy for the Global Context Network MVP. Every component MUST follow TDD (Test-Driven Development) with strict coverage requirements and quality gates.

**Core Principle**: Write failing tests first, then minimal implementation, then refactor.

## Test Pyramid

### Distribution

```
        10%  E2E Tests (Full workflows)
       ────────────────────
      /                    \
     /    20% Integration   \
    /      (Component         \
   /       interactions)        \
  /                              \
 /      70% Unit Tests            \
/    (Isolated functions)          \
────────────────────────────────────
```

### Rationale

- **70% Unit Tests**: Fast feedback, easy debugging, isolated testing
- **20% Integration Tests**: Verify component interactions, database transactions
- **10% E2E Tests**: Validate complete workflows, catch integration issues

### Coverage by Type

| Test Type | Speed | Isolation | Setup Complexity | Debugging |
|-----------|-------|-----------|------------------|-----------|
| Unit | <10ms | High | Low | Easy |
| Integration | <100ms | Medium | Medium | Medium |
| E2E | <5s | Low | High | Hard |

## Coverage Requirements

### Global Thresholds

```json
{
  "coverage": {
    "lines": 85,
    "statements": 85,
    "branches": 70,
    "functions": 85
  }
}
```

### Per-Scope Requirements

**Critical Path Files** (sanitization, hooks, database):
- Lines: ≥ 100%
- Statements: ≥ 100%
- Branches: ≥ 90%
- Functions: ≥ 100%

**Standard Files** (utilities, helpers):
- Lines: ≥ 85%
- Statements: ≥ 85%
- Branches: ≥ 70%
- Functions: ≥ 85%

**Infrastructure Files** (config, types):
- Lines: ≥ 50%
- Statements: ≥ 50%
- Branches: ≥ 30%
- Functions: ≥ 50%

### Critical Path Definition

Files matching these patterns require 100% coverage:

```typescript
// vitest.config.ts
export default defineConfig({
  test: {
    coverage: {
      thresholds: {
        perFile: true,
        'src/sanitization/**/*.ts': {
          lines: 100,
          statements: 100,
          branches: 90,
          functions: 100
        },
        'src/hooks/**/*.ts': {
          lines: 100,
          statements: 100,
          branches: 90,
          functions: 100
        },
        'src/database/repositories/**/*.ts': {
          lines: 100,
          statements: 100,
          branches: 90,
          functions: 100
        }
      }
    }
  }
});
```

### Exclusions Policy

**Excluded from coverage**:
- `**/*.d.ts` - Type definitions
- `**/generated/**` - Generated code
- `**/migrations/**/*.sql` - SQL migrations
- `**/config/**/*.ts` - Configuration files (but test loading logic)
- `**/__mocks__/**` - Test mocks
- `**/test-helpers/**` - Test utilities

**Must be tested**:
- Configuration loading and validation
- Migration application logic
- Generated code usage (integration tests)

## Testing Framework Configuration

### Canonical Vitest Config

```typescript
// vitest.config.ts
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    // Isolation
    isolate: true,
    pool: 'threads',
    poolOptions: {
      threads: {
        singleThread: false,
        isolate: true
      }
    },

    // Test matching
    include: ['**/*.test.ts', '**/*.spec.ts'],
    exclude: [
      '**/node_modules/**',
      '**/dist/**',
      '**/.{git,cache,output,temp}/**'
    ],

    // Environment
    environment: 'node',
    environmentOptions: {},

    // Setup files
    setupFiles: ['./tests/setup.ts'],
    globalSetup: ['./tests/global-setup.ts'],

    // Timeouts
    testTimeout: 5000,
    hookTimeout: 10000,

    // Retry flaky tests once
    retry: 1,

    // Coverage
    coverage: {
      provider: 'v8',
      reporter: ['text', 'json', 'html', 'lcov'],
      exclude: [
        '**/*.d.ts',
        '**/generated/**',
        '**/migrations/**',
        '**/config/**',
        '**/__mocks__/**',
        '**/test-helpers/**'
      ],
      thresholds: {
        lines: 85,
        statements: 85,
        branches: 70,
        functions: 85
      }
    },

    // Fake timers
    fakeTimers: {
      toFake: ['setTimeout', 'clearTimeout', 'setInterval', 'clearInterval', 'Date']
    },

    // Inline snapshots
    resolveSnapshotPath: (testPath, snapExtension) => {
      return testPath.replace(/\.test\.([tj]sx?)/, `.test${snapExtension}.$1`);
    }
  }
});
```

### SQLite Test Database Setup

```typescript
// tests/setup.ts
import { beforeEach, afterEach } from 'vitest';
import Database from 'better-sqlite3';
import { runMigrations } from '../src/database/migrations';

let testDb: Database.Database | null = null;

export function getTestDb(): Database.Database {
  if (!testDb) {
    // In-memory database for fast tests
    testDb = new Database(':memory:');

    // Enable WAL mode (even for in-memory)
    testDb.pragma('journal_mode = WAL');
    testDb.pragma('synchronous = FULL');
    testDb.pragma('foreign_keys = ON');

    // Run all migrations
    runMigrations(testDb);
  }
  return testDb;
}

// Reset database between tests
beforeEach(() => {
  const db = getTestDb();

  // Clear all tables
  db.exec(`
    DELETE FROM sanitization_log;
    DELETE FROM uploads;
    DELETE FROM learnings;
    DELETE FROM messages;
    DELETE FROM conversations;
    DELETE FROM job_queue;
  `);
});

afterEach(() => {
  // Verify no locks
  if (testDb) {
    const locks = testDb.pragma('database_list', { simple: true });
    // Assert no active transactions
  }
});

// Cleanup after all tests
process.on('beforeExit', () => {
  if (testDb) {
    testDb.close();
    testDb = null;
  }
});
```

### CI/CD Configuration

```yaml
# .github/workflows/test.yml
name: Test Suite

on: [push, pull_request]

jobs:
  test:
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        node: [20, 22]

    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node }}
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Lint
        run: pnpm run lint

      - name: Type check
        run: pnpm run type-check

      - name: Unit tests
        run: pnpm run test:unit

      - name: Integration tests
        run: pnpm run test:integration

      - name: E2E tests
        run: pnpm run test:e2e

      - name: Coverage check
        run: pnpm run test:coverage

      - name: Security audit
        run: pnpm audit --audit-level=moderate

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/lcov.info
```

## Test Organization

### Directory Structure

```
tests/
├── unit/                     # Unit tests (70%)
│   ├── sanitization/
│   │   ├── rule-detector.test.ts
│   │   ├── ai-sanitizer.test.ts
│   │   └── hybrid-validator.test.ts
│   ├── hooks/
│   │   ├── user-prompt-submit.test.ts
│   │   └── stop-hook.test.ts
│   ├── database/
│   │   ├── repositories/
│   │   │   ├── conversation-repository.test.ts
│   │   │   └── learning-repository.test.ts
│   │   └── migrations/
│   │       └── migration-runner.test.ts
│   └── utils/
│       └── validators.test.ts
│
├── integration/              # Integration tests (20%)
│   ├── event-capture.int.test.ts
│   ├── sanitization-pipeline.int.test.ts
│   ├── learning-extraction.int.test.ts
│   └── mcp-server.int.test.ts
│
├── e2e/                      # End-to-end tests (10%)
│   ├── conversation-to-learning.e2e.test.ts
│   ├── hook-to-database.e2e.test.ts
│   └── query-via-mcp.e2e.test.ts
│
├── fixtures/                 # Test data
│   ├── conversations/
│   │   ├── with-pii.json
│   │   └── sanitized.json
│   ├── learnings/
│   │   └── examples.json
│   └── sanitization/
│       ├── api-keys.txt
│       ├── file-paths.txt
│       └── pii-corpus.json
│
├── helpers/                  # Test utilities
│   ├── db-helpers.ts
│   ├── fixture-factory.ts
│   └── mock-helpers.ts
│
├── setup.ts                  # Test setup
└── global-setup.ts           # Global setup
```

### Naming Conventions

**Files**:
- Unit tests: `*.test.ts`
- Integration tests: `*.int.test.ts`
- E2E tests: `*.e2e.test.ts`

**Test cases**:
```typescript
describe('RuleBasedDetector', () => {
  describe('detectAPIKeys', () => {
    it('should detect AWS access keys', () => {
      // Test implementation
    });

    it('should detect OpenAI API keys', () => {
      // Test implementation
    });

    it('should return empty array for clean content', () => {
      // Test implementation
    });
  });
});
```

## TDD Workflow

### Red-Green-Refactor with Subagents

```
┌─────────────────────────────────────────────────────────────┐
│ 🔴 RED PHASE (Test Generator Subagent)                      │
├─────────────────────────────────────────────────────────────┤
│ 1. Generate failing test                                    │
│ 2. Validate test quality (Test Quality Validator)           │
│ 3. Run test → confirm proper failure                        │
│ 4. Commit test                                               │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ 🟢 GREEN PHASE (Implementation Subagent)                    │
├─────────────────────────────────────────────────────────────┤
│ 1. Write minimal code to pass test                          │
│ 2. Run tests continuously                                    │
│ 3. Validate implementation (Implementation Validator)       │
│ 4. All tests pass → proceed                                 │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ 🔵 REFACTOR PHASE (Code Quality Validator)                  │
├─────────────────────────────────────────────────────────────┤
│ 1. Improve code quality                                     │
│ 2. Maintain passing tests                                   │
│ 3. Re-validate quality                                      │
│ 4. Commit refactoring                                        │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│ ✅ QUALITY GATE (All Validators in Parallel)                │
├─────────────────────────────────────────────────────────────┤
│ • Coverage ≥ 85%                                             │
│ • Security scan passes                                       │
│ • Performance acceptable                                     │
│ • Lint + TypeScript strict mode                             │
└─────────────────────────────────────────────────────────────┘
```

### Example TDD Cycle

```typescript
// 🔴 RED: Write failing test first
describe('detectAPIKeys', () => {
  it('should detect AWS access keys', () => {
    const content = 'AKIAIOSFODNN7EXAMPLE';
    const result = detectAPIKeys(content);

    expect(result).toHaveLength(1);
    expect(result[0]).toMatchObject({
      type: 'aws_access_key',
      start: 0,
      end: 20,
      confidence: 1.0
    });
  });
});

// Run: pnpm test
// ❌ FAIL: ReferenceError: detectAPIKeys is not defined

// 🟢 GREEN: Minimal implementation
export function detectAPIKeys(content: string): Detection[] {
  const AWS_KEY_REGEX = /AKIA[0-9A-Z]{16}/g;
  const matches: Detection[] = [];

  let match;
  while ((match = AWS_KEY_REGEX.exec(content)) !== null) {
    matches.push({
      type: 'aws_access_key',
      start: match.index,
      end: match.index + match[0].length,
      confidence: 1.0
    });
  }

  return matches;
}

// Run: pnpm test
// ✅ PASS

// 🔵 REFACTOR: Extract regex patterns
const API_KEY_PATTERNS = {
  aws_access_key: /AKIA[0-9A-Z]{16}/g,
  openai: /sk-[a-zA-Z0-9]{48}/g,
  github: /ghp_[a-zA-Z0-9]{36}/g
};

export function detectAPIKeys(content: string): Detection[] {
  const matches: Detection[] = [];

  for (const [type, pattern] of Object.entries(API_KEY_PATTERNS)) {
    let match;
    while ((match = pattern.exec(content)) !== null) {
      matches.push({
        type,
        start: match.index,
        end: match.index + match[0].length,
        confidence: 1.0
      });
    }
  }

  return matches;
}

// Run: pnpm test
// ✅ PASS (all tests still pass after refactor)
```

## Mocking Strategy

### When to Mock vs Stub vs Fake

| Dependency | Strategy | Rationale |
|------------|----------|-----------|
| Database | Fake (in-memory SQLite) | Fast, realistic behavior |
| LLM API | Stub (recorded fixtures) | Deterministic, no API costs |
| MCP Server | Fake (local test server) | Contract testing |
| IPFS | Stub (deterministic CIDs) | No external dependency |
| Blockchain | Stub (fake tx hashes) | No real transactions |
| File System | Real (temp directory) | OS compatibility testing |
| Time | Fake (vitest fake timers) | Deterministic timing |
| Random | Seeded RNG | Reproducible tests |

### LLM Call Mocking

```typescript
// tests/helpers/mock-llm.ts
import { vi } from 'vitest';

interface MockCompletion {
  input: string;
  output: string;
  model?: string;
}

export function mockLLMWithFixtures(fixtures: MockCompletion[]) {
  return vi.fn(async (input: string) => {
    const fixture = fixtures.find(f => f.input === input);
    if (!fixture) {
      throw new Error(`No fixture for input: ${input}`);
    }
    return fixture.output;
  });
}

// Usage in tests
import { sanitizeWithAI } from '../src/sanitization/ai-sanitizer';

it('should sanitize PII using AI', async () => {
  const mockLLM = mockLLMWithFixtures([
    {
      input: 'My email is john@example.com',
      output: 'My email is [EMAIL]'
    }
  ]);

  const result = await sanitizeWithAI('My email is john@example.com', mockLLM);
  expect(result).toBe('My email is [EMAIL]');
});
```

### MCP Server Mocking

```typescript
// tests/helpers/mock-mcp-server.ts
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';

export function createMockMCPServer() {
  const server = new Server({
    name: 'test-context-server',
    version: '1.0.0'
  }, {
    capabilities: {
      tools: {},
      resources: {}
    }
  });

  // Mock search_learnings tool
  server.setRequestHandler('tools/call', async (request) => {
    if (request.params.name === 'search_learnings') {
      return {
        content: [
          {
            type: 'text',
            text: JSON.stringify([
              { id: '1', content: 'Test learning', confidence: 0.9 }
            ])
          }
        ]
      };
    }
    throw new Error(`Unknown tool: ${request.params.name}`);
  });

  return server;
}
```

### Deterministic Time and Randomness

```typescript
// Use fake timers
import { vi } from 'vitest';

beforeEach(() => {
  vi.useFakeTimers();
  vi.setSystemTime(new Date('2025-01-16T00:00:00Z'));
});

afterEach(() => {
  vi.useRealTimers();
});

it('should retry after 1 second', async () => {
  const fn = vi.fn()
    .mockRejectedValueOnce(new Error('Fail'))
    .mockResolvedValueOnce('Success');

  const promise = retryWithBackoff(fn, { initialDelay: 1000 });

  await vi.advanceTimersByTimeAsync(1000);

  const result = await promise;
  expect(result).toBe('Success');
  expect(fn).toHaveBeenCalledTimes(2);
});

// Seeded random for reproducibility
import seedrandom from 'seedrandom';

const rng = seedrandom('test-seed-123');

it('should randomly sample with seed', () => {
  const samples = Array.from({ length: 10 }, () => rng());
  // Always produces same sequence
  expect(samples[0]).toBeCloseTo(0.9282578795792454);
});
```

## Performance Testing

### Component SLAs

| Component | P50 | P95 | P99 | Max |
|-----------|-----|-----|-----|-----|
| Hook execution | <50ms | <100ms | <150ms | 200ms |
| Event queueing | <10ms | <50ms | <100ms | 200ms |
| Sanitization | <1s | <2s | <3s | 5s |
| Database queries | <50ms | <100ms | <200ms | 500ms |
| MCP queries | <100ms | <200ms | <500ms | 1s |
| Learning extraction | <2s | <5s | <10s | 30s |

### Measurement Strategy

```typescript
// tests/performance/hook-performance.test.ts
import { performance } from 'perf_hooks';

describe('Hook Performance', () => {
  it('should execute UserPromptSubmit hook under 100ms P95', async () => {
    const samples: number[] = [];

    for (let i = 0; i < 100; i++) {
      const start = performance.now();
      await userPromptSubmitHook({ prompt: 'Test prompt' });
      const duration = performance.now() - start;
      samples.push(duration);
    }

    samples.sort((a, b) => a - b);
    const p95 = samples[Math.floor(samples.length * 0.95)];

    expect(p95).toBeLessThan(100);
  });
});
```

### Load Testing

```javascript
// tests/load/k6-load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '30s', target: 10 },  // Ramp up to 10 users
    { duration: '1m', target: 10 },   // Stay at 10 users
    { duration: '30s', target: 0 },   // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<200'], // 95% under 200ms
  },
};

export default function () {
  const res = http.get('http://localhost:3000/api/learnings/search?q=test');

  check(res, {
    'status is 200': (r) => r.status === 200,
    'response time < 200ms': (r) => r.timings.duration < 200,
  });

  sleep(1);
}
```

## Security Testing

### Static Analysis

```json
// .eslintrc.json
{
  "extends": [
    "eslint:recommended",
    "plugin:@typescript-eslint/recommended",
    "plugin:security/recommended"
  ],
  "plugins": ["security"],
  "rules": {
    "security/detect-object-injection": "error",
    "security/detect-non-literal-regexp": "warn",
    "security/detect-unsafe-regex": "error",
    "security/detect-buffer-noassert": "error",
    "security/detect-child-process": "error",
    "security/detect-disable-mustache-escape": "error",
    "security/detect-eval-with-expression": "error",
    "security/detect-no-csrf-before-method-override": "error",
    "security/detect-non-literal-fs-filename": "warn",
    "security/detect-non-literal-require": "error",
    "security/detect-possible-timing-attacks": "warn",
    "security/detect-pseudoRandomBytes": "error"
  }
}
```

### Dependency Scanning

```bash
# Run in CI
pnpm audit --audit-level=moderate
pnpm outdated

# Use npm-check-updates
npx ncu -u
```

### Secret Scanning

```yaml
# .github/workflows/security.yml
- name: Gitleaks
  uses: gitleaks/gitleaks-action@v2
  env:
    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

### Sanitization Red-Team Tests

```typescript
// tests/security/sanitization-redteam.test.ts
import { sanitizationPipeline } from '../src/sanitization/pipeline';
import { piiCorpus } from '../fixtures/sanitization/pii-corpus';

describe('Sanitization Red Team Tests', () => {
  // Test against curated corpus of PII
  it.each(piiCorpus.api_keys)('should detect API key: %s', async (apiKey) => {
    const content = `Here is my key: ${apiKey}`;
    const result = await sanitizationPipeline(content);

    expect(result.content).not.toContain(apiKey);
    expect(result.detections).toHaveLength(1);
    expect(result.detections[0].type).toBe('api_key');
  });

  // Adversarial prompts
  it('should handle obfuscated API keys', async () => {
    const content = 'My key is: A K I A I O S F O D N N 7 E X A M P L E';
    const result = await sanitizationPipeline(content);

    // Should still detect when spaces are normalized
    expect(result.content).not.toContain('AKIAIOSFODNN7EXAMPLE');
  });

  // False positive checks
  it('should not flag variable names as API keys', async () => {
    const content = 'const apiKey = getEnvVar("API_KEY");';
    const result = await sanitizationPipeline(content);

    expect(result.detections).toHaveLength(0);
  });
});
```

### PII Corpus Structure

```json
// fixtures/sanitization/pii-corpus.json
{
  "api_keys": {
    "aws": ["AKIAIOSFODNN7EXAMPLE", "AKIAI44QH8DHBEXAMPLE"],
    "openai": ["sk-proj-abc123..."],
    "github": ["ghp_1234567890abcdef..."]
  },
  "file_paths": {
    "absolute_with_username": [
      "/Users/john/projects/app",
      "C:\\Users\\jane\\Documents"
    ],
    "relative_safe": [
      "./src/index.ts",
      "../utils/helpers.ts"
    ]
  },
  "emails": [
    "john.doe@example.com",
    "test+filter@domain.co.uk"
  ],
  "ip_addresses": [
    "192.168.1.1",
    "10.0.0.1",
    "2001:0db8:85a3::8a2e:0370:7334"
  ],
  "person_names": [
    "John Doe",
    "Jane Smith"
  ],
  "phone_numbers": [
    "+1-555-123-4567",
    "(555) 987-6543"
  ]
}
```

## Test Data Management

### Fixture Factories

```typescript
// tests/helpers/fixture-factory.ts
import { faker } from '@faker-js/faker';

export function createConversation(overrides?: Partial<Conversation>): Conversation {
  return {
    id: faker.string.uuid(),
    created_at: new Date().toISOString(),
    updated_at: new Date().toISOString(),
    session_id: faker.string.alphanumeric(32),
    correlation_id: faker.string.uuid(),
    sanitized: true,
    ...overrides
  };
}

export function createMessage(overrides?: Partial<Message>): Message {
  return {
    id: faker.string.uuid(),
    conversation_id: faker.string.uuid(),
    role: faker.helpers.arrayElement(['user', 'assistant']),
    content: faker.lorem.paragraph(),
    created_at: new Date().toISOString(),
    sequence: faker.number.int({ min: 0, max: 100 }),
    ...overrides
  };
}

export function createLearning(overrides?: Partial<Learning>): Learning {
  return {
    id: faker.string.uuid(),
    conversation_id: faker.string.uuid(),
    category: faker.helpers.arrayElement(['pattern', 'best_practice', 'bug_fix']),
    content: faker.lorem.sentences(3),
    confidence: faker.number.float({ min: 0.6, max: 1.0 }),
    tags: faker.helpers.arrayElements(['typescript', 'testing', 'database'], { min: 1, max: 3 }),
    created_at: new Date().toISOString(),
    dedupe_hash: faker.string.alphanumeric(64),
    ...overrides
  };
}
```

### Synthetic vs Real Data

**Unit Tests**: 100% synthetic data via faker
**Integration Tests**: Synthetic data + realistic fixtures
**E2E Tests**: Sanitized real-world examples

### Data Versioning

```typescript
// fixtures/conversations/v1/with-pii.json
{
  "version": "1.0.0",
  "schema": "conversation-with-messages",
  "data": {
    // ...
  }
}
```

## Canonical Commands

```json
// package.json
{
  "scripts": {
    "test": "vitest",
    "test:unit": "vitest run tests/unit",
    "test:integration": "vitest run tests/integration",
    "test:e2e": "vitest run tests/e2e",
    "test:watch": "vitest watch",
    "test:coverage": "vitest run --coverage",
    "test:perf": "k6 run tests/load/k6-load-test.js",
    "test:mutation": "stryker run",
    "test:ci": "pnpm run lint && pnpm run type-check && pnpm run test:coverage && pnpm run test:perf"
  }
}
```

## Flaky Test Policy

### Rules

1. **No disabled tests in main branch**
2. **Quarantine flaky tests** to separate file
3. **Fix within 7 days** or remove
4. **Track flakiness** in test metadata

### Quarantine Process

```typescript
// tests/quarantine/flaky-tests.test.ts
import { describe, it, skip } from 'vitest';

describe.skip('Quarantined Tests', () => {
  it.skip('flaky test - issue #123', () => {
    // Test that needs fixing
  });
});
```

## Mutation Testing

```javascript
// stryker.config.mjs
export default {
  packageManager: 'pnpm',
  reporters: ['html', 'clear-text', 'progress'],
  testRunner: 'vitest',
  coverageAnalysis: 'perTest',
  mutate: [
    'src/sanitization/**/*.ts',
    'src/hooks/**/*.ts',
    'src/database/repositories/**/*.ts'
  ],
  thresholds: { high: 80, low: 60, break: 50 }
};
```

## Related Documents

### Architecture
- [Global Context Network Architecture](../architecture/architecture-global-context-network-2025-01-16.md)
- [Subagent System](../architecture/architecture-subagent-system-2025-01-16.md)

### Guides
- [TDD Workflow Guide](../guides/guide-tdd-workflow-2025-01-16.md)
- [Testing Harness Usage](../guides/guide-testing-harness-usage-2025-01-16.md)

### Reference
- [Subagent Types](./reference-subagent-types-2025-01-16.md)
- [Database Schema](./reference-database-schema-2025-01-16.md)
````

## File: reviews/gpt5-sanitization-plan-review-request.md
````markdown
# GPT-5 Review Request: Sanitization Pipeline Architecture Plan

## Context

You are reviewing an architecture plan for a **2-stage sanitization pipeline** that is the MOST CRITICAL component of a privacy-first AI conversation capture system.

### Critical Background

**External Reviews Identified Critical Issue #1:**
- GPT-5 holistic review + Gemini 2.5 Pro both flagged: "Privacy contradiction - sanitize before storage vs persisting raw events"
- This is a BLOCKER for implementation
- This architecture doc must resolve the contradiction

**Canonical Standards (STANDARDS.md):**
1. **Privacy flow**: Pre-sanitize in hook (<50ms) → Persist ONLY sanitized → Optional AI validation (async)
2. **NEVER persist raw content to disk**
3. **Schema**: Write to `messages` table (no events/event_queue tables)
4. **Redaction format**: `[REDACTED_API_KEY]`, `[REDACTED_EMAIL]`, etc.
5. **Performance**: Fast sanitization <50ms, AI validation <2s

**ADR-004: Sanitize Before Storage**
- Zero-trust principle: "PII never touches disk"
- Rule: Sanitize ALL data BEFORE database insertion
- Architecture: Event Capture → Sanitization Pipeline → Database (PII never passes this gate)

### The Contradiction to Resolve

**Some existing docs show:**
- Persisting raw "content" to events table
- Generating idempotency keys using raw content
- Writing to queue before sanitization

**This plan must:**
- Align 100% with STANDARDS.md privacy flow
- Provide implementable architecture for zero-PII-leakage guarantee
- Show concrete integration with hooks, database, and job queue
- Include performance budgets and testing strategy

## Review Request

Please review the attached architecture plan for the sanitization pipeline.

### Critical Questions

1. **Alignment**: Does this architecture align 100% with STANDARDS.md and ADR-004?
   - Is the privacy flow correct? (Pre-sanitize → Persist sanitized → Optional AI validation)
   - Are there any contradictions with canonical standards?

2. **Privacy Guarantee**: Will this architecture guarantee zero PII leakage to disk?
   - Is the "raw content never persists" invariant maintained?
   - Are there any edge cases where PII could leak?
   - Is the in-memory-only approach for raw content sound?

3. **Completeness**: Are there missing components or specifications?
   - PII taxonomy comprehensive?
   - Regex patterns sufficient?
   - AI validation prompt adequate?
   - Error handling complete?

4. **Performance**: Are the budgets realistic and achievable?
   - <50ms for fast regex sanitization?
   - <100ms total hook execution?
   - <2s for AI validation?
   - Are there optimization opportunities?

5. **Integration**: Are the integration points clear?
   - How hook receives event → sanitizes → persists?
   - How job_queue triggers AI validation?
   - How messages table stores only sanitized content?
   - Database schema alignment?

6. **Testing**: Is the testing strategy sufficient?
   - Privacy invariant tests?
   - Performance benchmarks?
   - Adversarial tests?
   - End-to-end validation?

7. **Implementation Ready**: Can developers implement from this spec?
   - Are code samples complete and correct?
   - Are diagrams clear?
   - Are acceptance criteria explicit?

### Review Format

Please provide:

**1. Alignment Assessment** (Pass/Fail)
- Does it match STANDARDS.md? Specific issues if not.
- Does it match ADR-004? Specific issues if not.
- Are there any contradictions?

**2. Gap Analysis**
- What's missing from the architecture?
- What edge cases aren't covered?
- What needs more detail?

**3. Risk Assessment**
- What could go wrong with this design?
- Where could PII leak?
- What are performance risks?
- What are operational risks?

**4. Recommendations**
- Critical fixes before document creation
- Important improvements
- Nice-to-have enhancements

**5. Implementation Readiness Score** (1-10)
- Can developers implement Stage 1 (fast sanitization)?
- Can developers implement Stage 2 (AI validation)?
- Are all integration points specified?

### Success Criteria

The plan will be approved if:
- ✅ 100% alignment with STANDARDS.md
- ✅ Zero contradictions with ADR-004
- ✅ Provable zero-PII-leakage guarantee
- ✅ Realistic performance budgets
- ✅ Complete integration specifications
- ✅ Implementation-ready code samples
- ✅ Comprehensive testing strategy

---

## Plan to Review

See: `docs/plans/plan-sanitization-architecture-doc-2025-01-16.md`

**Key Sections:**
1. Overview - Privacy flow and principles
2. Stage 1: Fast Pre-Sanitization (synchronous, <50ms, regex)
3. Stage 2: AI Validation (asynchronous, <2s, context-aware)
4. Data Flow & Integration (how it all fits together)
5. Schema Integration (messages, sanitization_log, job_queue)
6. Performance budgets and benchmarking
7. Security & Privacy Guarantees (invariants and proofs)
8. Error Handling & Edge Cases
9. Testing Strategy (unit, integration, adversarial)

**Critical Code Samples:**
- `fastSanitize()` implementation
- `aiValidate()` implementation
- Database schema DDL
- Privacy invariant tests
- End-to-end integration flow

---

Please be thorough and critical. This is a BLOCKER for implementation, so any issues MUST be caught now.
````

## File: reviews/review-of-reviews-analysis-2025-01-16.md
````markdown
# Global Context Network MVP: Comprehensive Review Analysis & Remediation Plan

**Date:** 2025-01-16
**Analyst:** Technical Architecture & Project Management Review
**Source Reviews:**
- GPT-5 Holistic Review: `docs/reviews/gpt5-holistic-review-2025-01-16.txt`
- Gemini 2.5 Pro Holistic Review: `docs/reviews/gemini-holistic-review-2025-01-16.txt`
- Documentation Analyzed: 126K tokens across 38 files

---

## Executive Summary

Both GPT-5 and Gemini 2.5 Pro reviews converge on a critical assessment: **the project has exceptional architectural vision and documentation quality, but is not implementation-ready** due to critical contradictions, missing core documentation, and an unrealistic timeline.

**Key Consensus:**
- Documentation quality is world-class (structure, naming, consistency)
- Privacy-first architecture is exemplary when consistently applied
- Critical blocker: Privacy contradiction (sanitize before storage vs persisting raw events)
- Critical blocker: Missing 4 core architecture documents
- Timeline of 7-9 weeks is unrealistic; Gemini recommends 4-6 months
- Hook performance budget (<100ms) is a major technical risk

**Overall Verdict:** NOT READY for implementation until critical issues resolved.

**Estimated Remediation Effort:** 3-4 weeks of focused documentation work + timeline restructuring

---

## Consensus Issues (Both Reviews Agree)

### 1. Privacy Guarantee Contradiction (CRITICAL)

**Problem:**
- **ADR-004** and **DB Reference** mandate: NEVER store unsanitized data
- **Architecture-Hooks** shows: persisting raw "content" to events table, generating idempotency keys from raw content
- **Queue code samples** persist "content" before sanitization
- **Phase 1 Guide** implements "fastSanitize" in hook (correct direction) but conflicts with other docs

**Impact if Not Addressed:**
- Policy violations (Anthropic usage policies)
- Legal liability for PII storage
- Complete architectural rework required mid-implementation
- Trust violation with users

**Remediation Steps:**
1. **Establish Single Canonical Policy** (4 hours)
   - Document decision: Pre-sanitize synchronously in hook (fast rules)
   - Only persist sanitized text to any permanent storage
   - Optional: AI post-check downstream for enhancement

2. **Audit All Code Samples** (8 hours)
   - Search for any code writing to "events" table with raw content
   - Remove or rewrite to only persist sanitized_content
   - Update all schema examples: `content` → `sanitized_content` (with column rename)

3. **Update Referenced Documents** (6 hours)
   - ADR-004: Add section 4.2 "Canonical Pre-Sanitization Flow"
   - Architecture-Hooks: Remove raw content persistence, show sanitized-only
   - DB Reference: Remove "events" table or update to sanitized-only schema
   - Phase 1 Guide: Align with canonical flow

4. **Memory-Only Ring Buffer (if needed)** (2 hours)
   - Document: If pre-sanitization needs raw buffer, it must be memory-only
   - Explicit policy: Memory buffers exempt from disk persistence guarantee
   - Risk acceptance: Crash = data loss of in-flight events (acceptable)

**Dependencies:** None (can start immediately)

**Success Criteria:**
- [ ] No code sample shows writing raw content to SQLite
- [ ] ADR-004 section 4.2 defines canonical flow with diagram
- [ ] All schemas use `sanitized_content` or equivalent
- [ ] Grep search for "content" in storage context returns only sanitized references
- [ ] Privacy canary test (raw PII) fails to persist to disk

**Estimated Effort:** 20 hours (2.5 days)

---

### 2. Missing Core Architecture Documents (CRITICAL)

**Problem:**
Both reviews identified references to missing architecture documents:
- `architecture-sanitization-pipeline-2025-01-16.md`
- `architecture-learning-extraction-2025-01-16.md`
- `architecture-mcp-server-2025-01-16.md`
- `architecture-database-schema-2025-01-16.md` (reference exists, architecture version missing)
- Referenced but non-existent: `reference-event-schema.md`, `reference-hook-configuration.md`, `guide-database-setup.md`

**Impact if Not Addressed:**
- Impossible to implement sanitization or learning extraction phases
- Subagents lack specifications to work from
- Integration contracts between components undefined
- Rework required when gaps discovered during implementation

**Remediation Steps:**

#### 2.1 Create `architecture-sanitization-pipeline-2025-01-16.md` (16 hours)
**Must Include:**
- Component diagram: Hook → Pre-sanitizer → Event Queue → AI Validator → Messages
- Pre-sanitization rules (fast, synchronous, regex-based)
- AI validation rules (async, enhanced detection, prompts)
- Data contracts at each stage
- Performance budgets per stage
- Error handling and quarantine flow
- PII taxonomy (full list: emails, phones, IPs, paths, keys, JWTs, URLs with tokens, names, SSNs, credit cards)
- Integration points with job queue
- Success criteria: 98% precision, 95% recall

**Dependencies:** Must resolve privacy contradiction first

#### 2.2 Create `architecture-learning-extraction-2025-01-16.md` (12 hours)
**Must Include:**
- Input: Sanitized message sequences (conversations)
- Processing pipeline: Conversation assembly → Pattern detection → Learning extraction → Validation
- LLM prompt templates (temperature, max tokens, retries)
- Learning schema and taxonomy
- Quality gates (relevance, actionability, generalizability)
- Deduplication strategy
- Integration with job queue and storage
- Success criteria per acceptance tests

**Dependencies:** Requires sanitization pipeline to be defined

#### 2.3 Create `architecture-mcp-server-2025-01-16.md` (10 hours)
**Must Include:**
- MCP SDK version and dependencies
- Tool definitions (search, upload, token-balance)
- Resource definitions (learnings, stats)
- Request/response schemas with examples
- Authentication and authorization model
- Performance targets (<200ms for queries)
- Error responses and codes
- Integration with SQLite database
- Contract validation against MCP SDK schemas

**Dependencies:** Requires database schema to be finalized

#### 2.4 Create `reference-event-schema.md` (4 hours)
**Must Include:**
- Canonical event/message schema
- Field definitions and constraints
- Relationship to messages table
- Idempotency key generation (NOT using raw content)
- Sequence number generation strategy
- Example JSON payloads

#### 2.5 Create `reference-hook-configuration.md` (4 hours)
**Must Include:**
- Canonical config path (.claude/hooks.json vs .claude/hooks/hooks.json)
- Schema for hooks.json
- IO contract (stdin JSON vs process.argv)
- Performance monitoring configuration
- Example configurations

#### 2.6 Create `guide-database-setup.md` (6 hours)
**Must Include:**
- Installation prerequisites
- Schema migration process
- Initial data setup
- WAL mode configuration
- Backup and restore procedures
- Testing database setup

**Dependencies:** All documentation work (create or fix dead links)

**Success Criteria:**
- [ ] All 4 missing architecture docs created and reviewed
- [ ] All dead links in architecture/INDEX.md resolved
- [ ] Each doc includes component diagrams
- [ ] Data contracts explicitly defined
- [ ] Integration points with other components clear
- [ ] Subagents can reference these docs for implementation

**Estimated Effort:** 52 hours (6.5 days)

---

### 3. Event/Queue Schema and Naming Drift (CRITICAL)

**Problem:**
- **Hooks doc**: "events" table with pending/processing status
- **Guides**: "event_queue" table
- **DB Reference**: No events table at all
- **Job queue**: Different status vocabulary (queued/running/succeeded/failed)
- Inconsistent status enums across the stack

**Impact if Not Addressed:**
- Schema conflicts during implementation
- Code references non-existent tables
- Integration failures between components
- Confusion for developers and subagents

**Remediation Steps:**

1. **Decide on Canonical Schema** (4 hours)
   - Option A: Single "messages" table (sanitized-only) + job_queue for work
   - Option B: "event_queue" (sanitized-only) → messages + job_queue
   - **Recommendation:** Option A (simpler, fewer tables)

2. **Unify Status Enums** (6 hours)
   - Define canonical status flow: `queued → in_progress → completed → failed → dead_letter`
   - Apply to job_queue consistently
   - Remove any pending/processing/succeeded variants
   - Update all code samples

3. **Update Database Schema Reference** (8 hours)
   - Remove or clearly define "events" table status
   - Document event_queue if retained (with sanitized-only guarantee)
   - Add all tables to single source of truth
   - Include CREATE TABLE statements
   - Document all indexes and constraints

4. **Update All Code Samples** (8 hours)
   - Search and replace status values
   - Update table references
   - Ensure schema consistency

**Dependencies:** Resolving privacy contradiction

**Success Criteria:**
- [ ] Single canonical schema document
- [ ] All status references use same enum
- [ ] No references to undefined tables
- [ ] CREATE TABLE statements execute without errors
- [ ] Code samples reference only documented tables

**Estimated Effort:** 26 hours (3.25 days)

---

### 4. Hook Performance Risk (<100ms budget) (MAJOR)

**Problem:**
- Sub-100ms budget is extremely tight
- Current samples use ts-node (adds overhead)
- performance.now() used without importing node:perf_hooks
- SQLite WAL writes + regex sanitization must fit in budget
- Any I/O contention violates budget

**GPT-5 Concern:** "Risky for <100ms budget" with TypeScript runtime
**Gemini Concern:** "Primary technical risk" and "very tight budget"

**Impact if Not Addressed:**
- Degraded user experience (Claude Code lags)
- Hook might be disabled by Claude Code if too slow
- Potential rejection by users

**Remediation Steps:**

1. **Mandate Compiled JavaScript** (2 hours)
   - Document: Hooks MUST be compiled .js, not ts-node runtime
   - Provide build steps in guide-database-setup.md
   - Update hooks.json to point to .js scripts

2. **Fix Performance Monitoring** (2 hours)
   - Add import: `import { performance } from 'node:perf_hooks';`
   - Update all code samples using performance.now()

3. **Establish Performance Testing Protocol** (8 hours)
   - Create benchmark suite for hook execution
   - Test with SQLite WAL writes included
   - Measure 95th percentile, not just mean
   - Include realistic regex complexity
   - Test on resource-constrained systems

4. **Define Fast Sanitization Rules** (6 hours)
   - Document maximum regex complexity allowed
   - Set redaction patterns that fit budget
   - Define fallback: if pre-sanitization risks timeout, log and pass-through
   - Document trade-off: completeness vs speed

5. **Add Hook Packaging Guide** (6 hours)
   - Cross-platform build instructions (Windows/macOS/Linux)
   - Verified install steps
   - Performance validation checklist
   - Troubleshooting guide

**Dependencies:** Hook configuration reference must be complete

**Success Criteria:**
- [ ] All hook samples use compiled .js
- [ ] Performance imports correct in all samples
- [ ] Benchmark suite shows p95 < 80ms (20ms buffer)
- [ ] Documentation includes "no ts-node in production" warning
- [ ] Build and packaging guide complete

**Estimated Effort:** 24 hours (3 days)

---

### 5. Data Licensing and Consent Model (CRITICAL)

**Problem:**
- Plan involves uploading to global, public IPFS network
- **Zero documentation** on legal framework
- No defined license for shared learnings (MIT? CC0? Proprietary?)
- No IP ownership model (user? system? network?)
- No user consent mechanism for global, irrevocable publication

**Gemini Assessment:** "Exposes project and users to significant legal and ethical risks"

**Impact if Not Addressed:**
- Legal liability for unauthorized data publication
- Violation of user trust
- Potential GDPR/privacy law violations
- Inability to launch Phase 4 (global network)

**Remediation Steps:**

1. **Create ADR-007: Data Licensing and Consent** (12 hours)
   **Must Define:**
   - License for shared learnings (recommend: CC0 or MIT)
   - IP ownership model (recommend: user retains ownership, grants irrevocable license)
   - User consent mechanism (explicit opt-in required)
   - Default state: opt-out (local-only until user consents)
   - Consent UI/flow (checkbox, legal language)
   - Right to withdraw consent (learning remains published, but future uploads stop)
   - Privacy policy implications

2. **Implement Consent Gating** (8 hours)
   - Add `user_consent_global_upload: boolean` to config
   - Default: false
   - Upload worker checks consent before IPFS push
   - MCP tool enforces consent check
   - Log consent status changes

3. **Update Upload Approval Flow** (4 hours)
   - Manual approval gate PLUS consent check
   - Document approval criteria
   - User notification of pending uploads

4. **Legal Review Requirement** (0 hours for you, but document it)
   - Document: ADR-007 requires legal counsel review before Phase 4
   - Add to Phase 4 prerequisites

**Dependencies:** None (critical path item)

**Success Criteria:**
- [ ] ADR-007 created with all sections complete
- [ ] License choice documented with rationale
- [ ] Consent mechanism designed and documented
- [ ] Upload workers enforce consent check
- [ ] Default is opt-out (safe)
- [ ] Legal review prerequisite documented

**Estimated Effort:** 24 hours (3 days)

---

### 6. Timeline Realism (CRITICAL)

**Problem:**
- Current plan: 7-9 weeks
- **Gemini assessment:** "Unrealistic... more realistic would be 4-6 months"
- **GPT-5 assessment:** "Not ready for implementation until critical issues addressed"

**Underestimated Complexities:**
1. **Subagent workflow overhead:** Novel methodology with steep learning curve, prompt engineering, debugging agents
2. **Phase 2 sanitization:** 7-10 days to build 1000+ example dataset AND develop 98%/95% AI detector is "multi-week data science project"
3. **Testing harness:** Building Claude-powered harness is "project on its own"
4. **Missing architecture docs:** Cannot start implementation without them

**Impact if Not Addressed:**
- Missed deadlines and stakeholder frustration
- Rushed implementation leading to quality issues
- Burnout and team demoralization
- Incomplete features or cut corners

**Remediation Steps:**

1. **Add Phase -1: Proof of Concept** (4-6 weeks)
   - Validate subagent-driven development on single non-critical component
   - Build and stabilize testing harness MVP
   - Measure actual development velocity with subagents
   - Assess prompt engineering overhead
   - Deliverable: PoC report informing final methodology

2. **Revise Phase Durations** (Based on PoC results)
   - Phase 0 (Foundation): 2 weeks → 3 weeks
   - Phase 1 (Event Capture): 2 weeks → 3 weeks
   - Phase 2 (Sanitization): 7-10 days → 4 weeks (data science project)
   - Phase 3 (Learning Extraction): 7-10 days → 3 weeks
   - Phase 4 (Global Network): 7-10 days → 4 weeks (including legal review)
   - Buffer: Add 2 weeks contingency

3. **Updated Timeline**
   - Phase -1 (PoC): 4-6 weeks
   - Phase 0-4: 17 weeks
   - Buffer: 2 weeks
   - **Total: 23-25 weeks (5.5-6 months)**

4. **Document Methodology Risks** (4 hours)
   - Create risk register for subagent development
   - Mitigation: PoC validation phase
   - Fallback: Traditional development if subagents inefficient
   - Success metrics for PoC phase

**Dependencies:** Complete all missing architecture docs first

**Success Criteria:**
- [ ] PoC phase added to roadmap with clear deliverables
- [ ] Phase durations revised based on realistic estimates
- [ ] Total timeline: 5-6 months
- [ ] Risk register created for novel methodology
- [ ] Stakeholder buy-in on revised timeline

**Estimated Effort:** 4 hours documentation + PoC execution

---

### 7. Chain-of-Thought Handling Contradiction (MAJOR)

**Problem:**
- **Original vision** (`plan-original-user-vision`): "ideally all of their thinking processes"
- **Final decisions** (ADR, Phase 2): "Do NOT capture hidden chain-of-thought", "Chain-of-thought excluded from all storage"
- Risk: Original non-compliant intent could influence development

**Impact if Not Addressed:**
- Potential policy violations
- Confusion for developers
- Accidental capture of restricted content

**Remediation Steps:**

1. **Update Original Vision Document** (2 hours)
   - Add prominent disclaimer at top: "NOTE: Original vision included chain-of-thought capture. This was ruled non-compliant with Anthropic policy. Current design excludes all chain-of-thought. See ADR-XXX."
   - Strikethrough or annotate references to "thinking processes"
   - Link to final decision

2. **Audit All Documents** (4 hours)
   - Search for "chain-of-thought", "thinking", "reasoning"
   - Ensure all references align with exclusion policy
   - Remove any ambiguous language

3. **Add to Privacy Canary Tests** (2 hours)
   - Test case: Verify chain-of-thought never captured
   - Use sample with visible thinking tags
   - Assert: Tags not in stored data

**Dependencies:** None

**Success Criteria:**
- [ ] Original vision document annotated with disclaimer
- [ ] No documents suggest capturing chain-of-thought
- [ ] Privacy tests include chain-of-thought exclusion
- [ ] Grep search returns only exclusion references

**Estimated Effort:** 8 hours (1 day)

---

## Unique Issues by Reviewer

### GPT-5 Unique Concerns

#### Identity Strategy Inconsistency (UUID vs ULID) (MAJOR)

**Problem:**
- Some docs use UUID v4
- Others recommend ULID for sortable IDs
- Sequence numbers computed from "events" table that may not exist

**Remediation:**
1. **Decide on ULID globally** (2 hours)
   - Document rationale: Chronological sorting, k-sortable
   - Update all code samples to use ULID
   - Specify library: `ulid` npm package

2. **Define Sequence Number Strategy** (4 hours)
   - If events table removed: Compute per-conversation increment in messages table
   - Use transaction for atomic read-increment-write
   - Document in reference-event-schema.md
   - Update code samples

**Success Criteria:**
- [ ] All ID generation uses ULID
- [ ] Sequence strategy documented without relying on events table
- [ ] Code samples updated

**Estimated Effort:** 6 hours

---

#### Hook IO and Config Path Inconsistencies (MAJOR)

**Problem:**
- Some code reads JSON from stdin
- Other samples use process.argv
- Config path varies: `.claude/hooks.json` vs `.claude/hooks/hooks.json`

**Remediation:**
1. **Standardize on stdin JSON** (2 hours)
   - Document IO contract in reference-hook-configuration.md
   - Update all hook samples to read from stdin

2. **Choose Single Config Path** (2 hours)
   - Recommendation: `.claude/hooks.json` (simpler)
   - Update all references
   - Document in reference-hook-configuration.md

**Success Criteria:**
- [ ] All hooks read from stdin
- [ ] Single config path documented and used
- [ ] No samples use process.argv

**Estimated Effort:** 4 hours

---

#### PII Taxonomy Incompleteness (MODERATE)

**Problem:**
- PII lists vary across documents
- Missing comprehensive taxonomy

**Remediation:**
1. **Create Canonical PII Taxonomy** (6 hours)
   - Unify list: emails, phones, IPs, file paths, API keys, JWTs, URLs with tokens, names, SSNs, credit cards, addresses, dates of birth, biometrics
   - Document in architecture-sanitization-pipeline.md
   - Reference from all ADRs and tests
   - Match in regex patterns and AI prompts

**Success Criteria:**
- [ ] Single PII taxonomy document
- [ ] All references use same list
- [ ] Patterns and tests cover all categories

**Estimated Effort:** 6 hours

---

### Gemini 2.5 Pro Unique Concerns

#### Missing User Management & Configuration Layer (MAJOR)

**Problem:**
- No documentation on how users interact with system
- No UI or CLI for viewing queue status, learnings, token balance
- No settings management (upload approvals, sanitization strictness)

**Remediation:**
1. **Create `architecture-user-interface-2025-01-16.md`** (10 hours)
   - CLI commands for status, configuration, monitoring
   - MCP as primary interface (leverage existing plan)
   - Configuration file schema
   - User flows: opt-in, view learnings, approve uploads, check balance

2. **Extend MCP Server Spec** (4 hours)
   - Add configuration tools
   - Add monitoring tools (queue depth, processing status)
   - Add user preference tools

**Success Criteria:**
- [ ] User interface architecture documented
- [ ] Configuration management specified
- [ ] MCP tools extended for user operations

**Estimated Effort:** 14 hours

---

#### Cost Analysis & Management (MODERATE)

**Problem:**
- No plan for monitoring or controlling Claude API costs
- Sanitization and learning extraction will incur substantial costs

**Remediation:**
1. **Create Cost Model** (6 hours)
   - Estimate tokens per conversation
   - Calculate sanitization cost ($/conversation)
   - Calculate learning extraction cost ($/learning)
   - Project monthly costs for active user

2. **Add Cost Controls** (4 hours)
   - Token budget per user
   - Rate limiting strategy
   - Cost alerts
   - Document in architecture-global-context-network.md

**Success Criteria:**
- [ ] Cost model documented with estimates
- [ ] Budget and rate limiting specified
- [ ] Monitoring plan in place

**Estimated Effort:** 10 hours

---

#### Onboarding & Installation Guide (MODERATE)

**Problem:**
- System involves hooks, background services, database
- No detailed installation guide

**Remediation:**
1. **Create `guide-installation-setup-2025-01-16.md`** (8 hours)
   - Prerequisites checklist
   - Step-by-step installation
   - Hook compilation and configuration
   - Service startup procedures
   - Verification steps
   - Troubleshooting common issues
   - Cross-platform notes (Windows/macOS/Linux)

**Success Criteria:**
- [ ] Complete installation guide
- [ ] Tested on all three platforms
- [ ] Includes troubleshooting

**Estimated Effort:** 8 hours

---

#### Meta-Testing Problem (MODERATE)

**Problem:**
- Strategy doesn't address testing the subagents themselves
- How to verify test-generator-agent produces quality tests?

**Remediation:**
1. **Create `architecture-subagent-validation-2025-01-16.md`** (6 hours)
   - Quality metrics for generated tests
   - Manual review process for agent outputs
   - Agent performance benchmarks
   - Feedback loop for improving agents
   - Acceptance criteria for agent deliverables

**Success Criteria:**
- [ ] Validation strategy for subagents documented
- [ ] Quality gates defined
- [ ] Review process specified

**Estimated Effort:** 6 hours

---

## Prioritized Remediation Plan

### Priority 1: Blockers (MUST FIX - Cannot proceed without)

| Issue | Effort | Owner/Phase | Completion Criteria |
|-------|--------|-------------|-------------------|
| **1. Privacy Contradiction** | 20h (2.5d) | Documentation Lead | All schemas show sanitized-only; ADR-004 section 4.2 added; privacy canary test passes |
| **2. Missing Architecture Docs (4 files)** | 52h (6.5d) | Architecture Team | All 4 docs created, reviewed, and linked from INDEX; data contracts defined |
| **3. Schema & Naming Drift** | 26h (3.25d) | Database Lead | Single canonical schema; status enums unified; no undefined table references |
| **4. Data Licensing & Consent ADR** | 24h (3d) | Legal/Policy Lead | ADR-007 complete; consent mechanism designed; default opt-out enforced |
| **5. Timeline Restructuring** | 4h + 4-6 weeks PoC | PM/Tech Lead | PoC phase added; realistic 5-6 month timeline; stakeholder buy-in |

**Total Priority 1 Effort:** 126 hours (15.75 days) + PoC phase
**Critical Path:** Must complete 1-4 before implementation can start

---

### Priority 2: High Risk (Major issues, address before implementation)

| Issue | Effort | Owner/Phase | Completion Criteria |
|-------|--------|-------------|-------------------|
| **6. Hook Performance Risk** | 24h (3d) | Performance Lead | Compiled JS mandate; benchmark suite shows p95 < 80ms; packaging guide complete |
| **7. Chain-of-Thought Contradiction** | 8h (1d) | Documentation Lead | Original vision annotated; all refs aligned; privacy test added |
| **8. Missing User Interface Spec** | 14h (1.75d) | Product Lead | User interface architecture doc created; MCP tools extended |
| **9. UUID vs ULID Inconsistency** | 6h (0.75d) | Database Lead | ULID chosen; sequence strategy documented; samples updated |
| **10. Hook IO/Config Standardization** | 4h (0.5d) | Hooks Lead | Stdin IO documented; single config path chosen; all samples aligned |

**Total Priority 2 Effort:** 56 hours (7 days)

---

### Priority 3: Important (Should address before implementation)

| Issue | Effort | Owner/Phase | Completion Criteria |
|-------|--------|-------------|-------------------|
| **11. PII Taxonomy Completeness** | 6h (0.75d) | Privacy Lead | Canonical taxonomy created; all refs use same list; patterns updated |
| **12. Cost Analysis & Management** | 10h (1.25d) | Operations Lead | Cost model documented; budgets defined; monitoring specified |
| **13. Installation & Onboarding Guide** | 8h (1d) | DevOps Lead | Installation guide created; tested on 3 platforms; troubleshooting included |
| **14. Subagent Meta-Testing** | 6h (0.75d) | Quality Lead | Validation strategy documented; quality gates defined |

**Total Priority 3 Effort:** 30 hours (3.75 days)

---

### Priority 4: Nice to Have (Quality improvements)

| Issue | Effort | Owner/Phase | Completion Criteria |
|-------|--------|-------------|-------------------|
| Regex pattern review & property-based tests | 8h | Quality Lead | Named patterns; benchmarks; property tests specified |
| Timestamp format normalization | 2h | Standards Lead | ISO-8601 standard chosen; all samples updated |
| WAL in-memory test cleanup | 2h | Test Lead | WAL removed from :memory: tests |
| Metrics abstraction | 4h | Infra Lead | Pluggable metrics interface; no-op default provided |
| Operational runbook | 6h | Operations Lead | Restart procedures; queue recovery; backup/restore documented |

**Total Priority 4 Effort:** 22 hours (2.75 days)

---

## Total Remediation Summary

| Priority | Effort | Timeline Impact |
|----------|--------|-----------------|
| **Priority 1 (Blockers)** | 126h + PoC | 15.75 days + 4-6 weeks |
| **Priority 2 (High Risk)** | 56h | 7 days |
| **Priority 3 (Important)** | 30h | 3.75 days |
| **Priority 4 (Nice to Have)** | 22h | 2.75 days |
| **TOTAL** | 234h (29.25 days) | ~6 weeks + PoC |

**Recommended Approach:**
1. **Weeks 1-3:** Complete all Priority 1 blockers (parallel workstreams)
2. **Weeks 4-9:** Execute Phase -1 Proof of Concept
3. **Week 10:** Address Priority 2 issues based on PoC learnings
4. **Week 11:** Address Priority 3 issues
5. **Week 12:** Priority 4 cleanup + final documentation review
6. **Week 13+:** Begin Phase 0 implementation with confidence

---

## Timeline Recommendation

### Current Plan Assessment
- **Current Timeline:** 7-9 weeks for full MVP
- **GPT-5 Verdict:** "Not ready for implementation until critical issues addressed"
- **Gemini Verdict:** "Highly unrealistic... more realistic would be 4-6 months"

### Revised Recommended Timeline

#### Phase -1: Proof of Concept & Remediation (10 weeks)
- **Weeks 1-3:** Critical documentation remediation (Priority 1)
- **Weeks 4-9:** PoC for subagent-driven development
  - Select single non-critical component (e.g., token balance tracking)
  - Build testing harness MVP
  - Measure actual velocity with subagents
  - Validate methodology assumptions
  - **Deliverable:** PoC report with go/no-go recommendation

#### Phase 0: Foundation (3 weeks)
- Database setup
- Hook infrastructure
- Testing framework
- **Revised from 2 weeks based on PoC overhead**

#### Phase 1: Event Capture (3 weeks)
- Hook implementation
- Event queue
- Initial sanitization
- **Revised from 2 weeks**

#### Phase 2: Sanitization Pipeline (4 weeks)
- Gold dataset creation (1000+ examples)
- AI/rule hybrid detector
- 98% precision / 95% recall target
- **Revised from 7-10 days** (Gemini: "multi-week data science project")

#### Phase 3: Learning Extraction (3 weeks)
- Pattern detection
- LLM-based extraction
- Quality validation
- **Revised from 7-10 days**

#### Phase 4: Global Network (4 weeks)
- IPFS integration
- Upload approval flow
- Legal review checkpoint
- **Revised from 7-10 days** (includes consent implementation)

#### Buffer & Stabilization (2 weeks)
- Bug fixes
- Performance optimization
- Documentation updates

### Total Revised Timeline: **29 weeks (7 months)**
- **Remediation + PoC:** 10 weeks
- **Implementation (Phases 0-4):** 17 weeks
- **Buffer:** 2 weeks

### Stakeholder Communication
**Key Message:** "The project has exceptional architectural foundations, but rushing a 7-9 week timeline would compromise quality and create legal/privacy risks. The revised 7-month timeline includes critical remediation, methodology validation, and realistic phase durations. This investment ensures a robust, compliant, and maintainable system."

---

## Documentation Updates Needed

### New Documents to Create

| Document | Priority | Effort | Purpose |
|----------|----------|--------|---------|
| `architecture-sanitization-pipeline-2025-01-16.md` | P1 | 16h | Define complete sanitization flow, rules, AI validation |
| `architecture-learning-extraction-2025-01-16.md` | P1 | 12h | Specify learning extraction pipeline, prompts, quality gates |
| `architecture-mcp-server-2025-01-16.md` | P1 | 10h | MCP server contract, tools, resources, schemas |
| `architecture-database-schema-2025-01-16.md` | P1 | 6h | Unified schema architecture document |
| `reference-event-schema.md` | P1 | 4h | Canonical event/message schema reference |
| `reference-hook-configuration.md` | P1 | 4h | Hook config contract and examples |
| `guide-database-setup.md` | P1 | 6h | Database installation and setup guide |
| `decision-data-licensing-consent-2025-01-16.md` (ADR-007) | P1 | 12h | Legal framework for data sharing |
| `architecture-user-interface-2025-01-16.md` | P2 | 10h | User interaction model and CLI/MCP interface |
| `architecture-subagent-validation-2025-01-16.md` | P3 | 6h | Meta-testing and quality assurance for subagents |
| `guide-installation-setup-2025-01-16.md` | P3 | 8h | End-to-end installation and onboarding |
| `reference-cost-model-2025-01-16.md` | P3 | 6h | Cost analysis and budget management |

**Total New Documents:** 12 documents, 100 hours effort

---

### Existing Documents Requiring Updates

| Document | Updates Needed | Effort |
|----------|----------------|--------|
| `decision-privacy-guarantees.md` (ADR-004) | Add section 4.2: Canonical Pre-Sanitization Flow; resolve raw storage contradictions | 6h |
| `architecture-hooks-event-capture.md` | Remove raw content persistence; align with sanitized-only policy; fix IO/config refs | 8h |
| `reference-database-schema.md` | Unify schema; remove/clarify events table; add event_queue if retained; unify status enums | 8h |
| `guide-phase-1-implementation.md` | Align with canonical sanitization flow; update schema references | 4h |
| `plan-original-user-vision.md` | Add disclaimer about chain-of-thought exclusion; annotate non-compliant original intent | 2h |
| `architecture/INDEX.md` | Add links to new architecture docs; fix broken references | 2h |
| `plan-implementation-roadmap.md` | Revise timeline to 7 months; add Phase -1 PoC; update phase durations | 4h |
| All code samples (multiple files) | Update to compiled JS; fix performance imports; unify schemas; ULID migration | 16h |

**Total Update Effort:** 50 hours

---

### Cross-Reference Integrity

**Actions Required:**
1. **Link Audit** (4h): Scan all markdown files for broken internal links
2. **Schema Reference Sweep** (4h): Ensure all table/column references valid
3. **Status Enum Normalization** (4h): Replace all status values with canonical set
4. **Config Path Standardization** (2h): Replace all hook config paths with chosen canonical path

**Total Cross-Reference Effort:** 14 hours

---

## Concrete Next Steps (Week 1 Action Plan)

### Day 1: Privacy Remediation
- [ ] **Morning:** Create ADR-004 section 4.2 "Canonical Pre-Sanitization Flow"
  - Define: Hook → fastSanitize (sync) → persist sanitized → AI validate (async)
  - Diagram the flow
  - Document memory-only buffer policy
- [ ] **Afternoon:** Audit all code samples for raw content persistence
  - Search for: `.content`, `raw_content`, writes to "events" table
  - Create fix list with file locations

### Day 2: Privacy Remediation (Continued)
- [ ] **Morning:** Update architecture-hooks-event-capture.md
  - Remove raw content persistence code
  - Update to sanitized-only flow
- [ ] **Afternoon:** Update database schema reference
  - Remove "events" table or convert to sanitized-only
  - Update all CREATE TABLE statements

### Day 3: Schema Unification
- [ ] **Morning:** Decide canonical schema model (messages + job_queue)
  - Document decision
  - Create unified status enum
- [ ] **Afternoon:** Create reference-event-schema.md
  - Define message schema
  - Specify ULID usage
  - Document sequence strategy

### Day 4: Missing Architecture Docs (Start)
- [ ] **Full Day:** Create architecture-sanitization-pipeline-2025-01-16.md
  - Component diagram
  - Pre-sanitization rules (fast, regex)
  - AI validation rules (prompts, budgets)
  - PII taxonomy (canonical list)
  - Integration points

### Day 5: Missing Architecture Docs (Continue)
- [ ] **Morning:** Create reference-hook-configuration.md
  - Canonical config path decision
  - IO contract (stdin)
  - Schema and examples
- [ ] **Afternoon:** Create guide-database-setup.md
  - Installation steps
  - Schema migration
  - WAL configuration

---

### Week 1 Deliverables Checklist
- [ ] Privacy contradiction resolved (ADR-004 updated, code audited, schema fixed)
- [ ] Canonical schema decided and documented
- [ ] 3 of 6 missing reference docs created (event schema, hook config, DB setup)
- [ ] Sanitization pipeline architecture complete
- [ ] Status report prepared for stakeholders with timeline revision proposal

---

## Success Metrics for Remediation

### Documentation Completeness
- [ ] All referenced documents exist (zero broken links)
- [ ] All architecture components have detailed specs
- [ ] All code samples executable and aligned with specs
- [ ] Cross-references validated and consistent

### Privacy Compliance
- [ ] Privacy canary test: Raw PII fails to persist to disk (pass)
- [ ] Grep audit: No "raw" content in storage paths (pass)
- [ ] ADR-004 canonical flow section complete
- [ ] All schemas show sanitized_content only

### Technical Consistency
- [ ] Single status enum used everywhere
- [ ] Single ID strategy (ULID) documented and implemented
- [ ] Single hook IO method (stdin) in all samples
- [ ] Single config path in all references
- [ ] All hooks use compiled JavaScript

### Legal/Policy Readiness
- [ ] ADR-007 (consent/licensing) complete
- [ ] Default opt-out enforced in code
- [ ] Upload approval gate documented
- [ ] Legal review checkpoint added to Phase 4

### Timeline Realism
- [ ] PoC phase added to roadmap
- [ ] Phase durations revised to 5-6 months total
- [ ] Risk register for subagent methodology created
- [ ] Stakeholder approval obtained for revised timeline

---

## Risk Assessment Post-Remediation

### Residual Risks After Remediation

#### High Risk (Monitor Closely)
1. **Subagent Methodology Unproven**
   - **Mitigation:** PoC phase validates before full commitment
   - **Fallback:** Traditional development if agents inefficient

2. **Hook Performance Budget**
   - **Mitigation:** Compiled JS, benchmark suite, measured budgets
   - **Fallback:** Async event capture if sync budget unachievable

#### Medium Risk (Manage)
3. **AI Sanitization Quality (98%/95% targets)**
   - **Mitigation:** Gold dataset, hybrid approach, ongoing tuning
   - **Monitoring:** Precision/recall metrics, canary tests

4. **Claude API Costs**
   - **Mitigation:** Cost model, budgets, rate limiting
   - **Monitoring:** Monthly spend tracking

#### Low Risk (Accept)
5. **SQLite Performance at Scale**
   - **Mitigation:** WAL mode, reasonable single-user limits
   - **Future:** Migrate to PostgreSQL post-MVP

6. **IPFS Network Availability**
   - **Mitigation:** Local-first architecture, graceful degradation
   - **Monitoring:** Upload success rates

---

## Recommendations for Project Leadership

### Immediate Actions (This Week)
1. **Halt any implementation work** until Priority 1 blockers resolved
2. **Assign owners** to each Priority 1 remediation task
3. **Schedule daily standups** for remediation team
4. **Communicate timeline revision** to stakeholders with rationale
5. **Begin PoC planning** (scope, success criteria, resources)

### Strategic Decisions Required
1. **Schema Model:** Approve canonical schema (messages + job_queue)
2. **Config Path:** Choose .claude/hooks.json as standard
3. **ID Strategy:** Approve ULID globally
4. **License Choice:** Decide CC0 vs MIT for shared learnings
5. **PoC Go/No-Go Criteria:** Define what validates subagent approach

### Resource Allocation
- **Documentation Team:** 2-3 people for 3 weeks (Priority 1)
- **PoC Team:** 1-2 developers + 1 QA for 4-6 weeks
- **Legal Counsel:** Review ADR-007 (consent/licensing)
- **Technical Architect:** Schema and architecture doc reviews

### Communication Plan
- **Internal:** Weekly progress reports on remediation
- **Stakeholders:** Timeline revision presentation (with rationale)
- **Legal:** Consent model review meeting
- **Development Team:** "Hold" until documentation complete

---

## Conclusion

### What's Exceptional
Both reviewers praised:
- **World-class documentation quality:** Structure, naming, consistency
- **Privacy-first architecture:** When consistently applied, it's exemplary
- **Pragmatic MVP scoping:** Local-first approach de-risks the project
- **Innovative vision:** Subagent development and learning mining concepts are forward-thinking
- **Strong testing culture:** Coverage targets, adversarial tests, quality gates

### What's Broken
Both reviewers flagged as critical:
- **Privacy contradiction:** Raw vs sanitized storage must be resolved
- **Missing core docs:** 4 architecture documents blocking implementation
- **Schema inconsistency:** Events/event_queue/job_queue naming drift
- **Unrealistic timeline:** 7-9 weeks impossible, 5-6 months realistic
- **Legal/consent gap:** No licensing or consent model for global uploads

### The Path Forward
**This project can succeed**, but only with disciplined remediation:

1. **3 weeks:** Fix Priority 1 blockers (documentation work)
2. **4-6 weeks:** Validate methodology with PoC
3. **17 weeks:** Implement Phases 0-4 with realistic durations
4. **2 weeks:** Buffer and stabilization

**Total: 26-28 weeks (6-7 months)** from today to MVP launch.

The alternative—rushing a 7-9 week timeline with unresolved contradictions—risks:
- Privacy policy violations
- Legal liability
- Architectural rework mid-implementation
- Developer burnout
- Project failure

### Final Verdict
**Status:** NOT READY for implementation
**Recommended Action:** Execute 3-week remediation sprint, then proceed to PoC
**Confidence After Remediation:** HIGH (if timeline revised and PoC validates methodology)

---

## Appendix: Reviewer Agreement Matrix

| Issue | GPT-5 | Gemini | Severity | Consensus |
|-------|-------|--------|----------|-----------|
| Privacy contradiction (raw vs sanitized) | ✅ Critical | ✅ Critical | CRITICAL | **FULL** |
| Missing architecture docs (4 files) | ✅ Critical | ✅ Critical | CRITICAL | **FULL** |
| Schema/naming drift | ✅ Critical | ⚠️ Implicit | CRITICAL | **STRONG** |
| Timeline unrealistic | ⚠️ Implicit | ✅ Major | CRITICAL | **STRONG** |
| Hook performance risk | ✅ Major | ✅ Major | MAJOR | **FULL** |
| Data licensing/consent missing | ✅ Major | ✅ Critical | CRITICAL | **FULL** |
| Chain-of-thought contradiction | ✅ Critical | ✅ Major | MAJOR | **FULL** |
| UUID vs ULID inconsistency | ✅ Critical | ❌ Not mentioned | MAJOR | GPT-5 only |
| Hook IO/config inconsistency | ✅ Critical | ❌ Not mentioned | MAJOR | GPT-5 only |
| User management missing | ❌ Not mentioned | ✅ Major | MAJOR | Gemini only |
| Cost analysis missing | ❌ Not mentioned | ✅ Moderate | MODERATE | Gemini only |
| Installation guide missing | ❌ Not mentioned | ✅ Moderate | MODERATE | Gemini only |
| Meta-testing problem | ❌ Not mentioned | ✅ Moderate | MODERATE | Gemini only |

**Consensus Rate:** 7 of 13 issues (54%) identified by both reviewers
**Critical Consensus:** 4 of 5 critical blockers agreed upon (80%)

---

**Document Version:** 1.0
**Last Updated:** 2025-01-16
**Next Review:** After Priority 1 remediation complete (Week 4)
**Owner:** Technical Architecture & Project Management Team

---

## Quick Reference: Top 5 Critical Actions

1. **Resolve Privacy Contradiction** → ADR-004 section 4.2 + code audit (2.5 days)
2. **Create 4 Missing Architecture Docs** → Sanitization, Learning, MCP, Database (6.5 days)
3. **Unify Schema & Status Enums** → Single canonical schema (3.25 days)
4. **Create Consent/Licensing ADR** → ADR-007 with legal framework (3 days)
5. **Revise Timeline to 5-6 Months** → Add PoC phase, realistic durations (4h + PoC execution)

**Total Critical Path:** ~16 days documentation + 4-6 weeks PoC = **10 weeks to implementation-ready state**
````

## File: INDEX.md
````markdown
# Documentation Index

> Last updated: 2025-01-16

## Overview

This directory contains all project documentation for the Global Context Network MVP - a system that captures Claude Code conversations, sanitizes them for PII, stores learnings, and shares them globally via blockchain/IPFS with token rewards.

**Core Philosophy**: Subagent-driven development with Claude-powered testing harness.

## Quick Links

- [Architecture](./architecture/INDEX.md) - System design, components, and data flow
- [Decisions](./decisions/INDEX.md) - Architecture Decision Records (ADRs)
- [Plans](./plans/INDEX.md) - Implementation plans, roadmaps, and task breakdowns
- [Guides](./guides/INDEX.md) - How-to guides for development and testing
- [Reference](./reference/INDEX.md) - Technical reference materials and APIs
- [Learnings](./learnings/INDEX.md) - Insights, retrospectives, and discoveries

## Project Goals

1. **Capture**: Hook into Claude Code to capture conversations
2. **Sanitize**: Remove ALL PII before storage (privacy-first)
3. **Extract**: Generate valuable, reusable learnings
4. **Share**: Upload to global network (IPFS + blockchain)
5. **Query**: MCP server for agents to access learnings
6. **Reward**: Token rewards for quality contributions

## Recent Documents

| Date | Category | Document | Description |
|------|----------|----------|-------------|
| 2025-01-16 | decision | [ADR-001: Use Claude Hooks](./decisions/decision-use-claude-hooks-2025-01-16.md) | Event capture via Claude Code hooks |
| 2025-01-16 | decision | [ADR-002: Subagent-Driven Development](./decisions/decision-subagent-driven-development-2025-01-16.md) | All implementation via specialized subagents |
| 2025-01-16 | decision | [ADR-003: Claude Testing Harness](./decisions/decision-claude-testing-harness-2025-01-16.md) | AI-powered test generation and validation |
| 2025-01-16 | decision | [ADR-004: Sanitize Before Storage](./decisions/decision-sanitize-before-storage-2025-01-16.md) | Privacy-first architecture decision |
| 2025-01-16 | decision | [ADR-005: Use SQLite](./decisions/decision-use-sqlite-2025-01-16.md) | SQLite for MVP storage |
| 2025-01-16 | decision | [ADR-006: Async Processing Model](./decisions/decision-async-processing-model-2025-01-16.md) | SQLite-based job queue |
| 2025-01-16 | decision | [ADR-007: Data Licensing & Consent](./decisions/decision-data-licensing-consent-2025-01-16.md) | GDPR/CCPA compliance, opt-in model ⭐ |
| 2025-01-16 | decision | [ADR-010: PII Detection Strategy](./decisions/decision-pii-detection-strategy-2025-01-16.md) | 72+ categories, 2-stage detection |
| 2025-01-16 | decision | [ADR-011: Security & Provenance](./decisions/decision-security-provenance-2025-01-16.md) | Signing, attestations, key management |
| 2025-01-16 | decision | [ADR-013: Observability & Cost](./decisions/decision-observability-cost-slo-governance-2025-01-16.md) | SLO governance, cost management |
| 2025-01-16 | plans | [Global Context Network MVP](./plans/plan-global-context-network-mvp-2025-01-16.md) | Complete MVP implementation plan |
| 2025-01-16 | plans | [Implementation Roadmap](./plans/plan-implementation-roadmap-2025-01-16.md) | 7-phase roadmap with timeline |
| 2025-01-16 | plans | [Iterative Build Strategy](./plans/plan-iterative-build-strategy-2025-01-16.md) | 15-level iteration plan ⭐ |
| 2025-01-16 | guides | [Database Setup Guide](./guides/guide-database-setup-2025-01-16.md) | Step-by-step SQLite setup |
| 2025-01-16 | reference | [Event Schema Reference](./reference/reference-event-schema-2025-01-16.md) | Complete event schemas (ULID, ISO-8601) |
| 2025-01-16 | reference | [Hook Configuration Reference](./reference/reference-hook-configuration-2025-01-16.md) | Canonical .claude/hooks.json |
| 2025-01-16 | architecture | [Global Context Network](./architecture/architecture-global-context-network-2025-01-16.md) | System architecture overview |
| 2025-01-16 | architecture | [Subagent System](./architecture/architecture-subagent-system-2025-01-16.md) | Subagent-driven development architecture |
| 2025-01-16 | architecture | [Async Processing](./architecture/architecture-async-processing-2025-01-16.md) | Complete async job queue architecture |

## Document Count by Category

- **Architecture**: 9 documents (system design, components, data flow) ✅ **COMPLETE**
- **Decisions**: 13 ADRs (major architectural decisions) ✅ **COMPLETE**
- **Plans**: 13 plans (MVP plan, roadmap, phase tasks, iterative strategy) ✅ **COMPLETE**
- **Guides**: 7 guides (foundation, database, SDK, subagents, testing, TDD, hooks) ✅ **COMPLETE**
- **Reference**: 6 references (testing, subagents, database, SDK, events, hooks) ✅ **COMPLETE**
- **Learnings**: 0 (will be populated during implementation)

**Total**: 48 documents (all core documentation complete)

## Key Architectural Decisions

1. **Hooks-Based Capture** - Use Claude Code hooks (UserPromptSubmit, Stop)
2. **Privacy-First** - Sanitize BEFORE database storage, never store raw PII
3. **Subagent-Driven** - ALL implementation via specialized subagents
4. **Claude Testing** - Self-validating system using Claude Agent SDK
5. **Async Processing** - Job queue for sanitization and learning extraction
6. **SQLite Storage** - Local persistence with migration system
7. **MCP Interface** - Standard protocol for agent queries
8. **IPFS + Blockchain** - Decentralized global storage with rewards

## Development Phases

| Phase | Focus | Duration | Status | Plan |
|-------|-------|----------|--------|------|
| 0 | Foundation (TypeScript, DB, Tests) | 2-3 days | Planned | [Phase 0 Tasks](./plans/plan-phase-0-tasks-2025-01-16.md) |
| 1 | Event Capture (Hooks, Queue) | 3-4 days | Planned | [Phase 1 Tasks](./plans/plan-phase-1-tasks-2025-01-16.md) |
| 2 | Sanitization (PII Removal) | 7-10 days | Planned | [Phase 2 Tasks](./plans/plan-phase-2-tasks-2025-01-16.md) |
| 3 | Database & Storage | 2-3 days | Planned | [Phase 3 Tasks](./plans/plan-phase-3-tasks-2025-01-16.md) |
| 4 | Async Processing (Job Queue) | 5-7 days | Planned | [Phase 4 Tasks](./plans/plan-phase-4-tasks-2025-01-16.md) |
| 5 | Learning Extraction | 6-8 days | Planned | [Phase 5 Tasks](./plans/plan-phase-5-tasks-2025-01-16.md) |
| 6 | MCP Server | 3-4 days | Planned | [Phase 6 Tasks](./plans/plan-phase-6-tasks-2025-01-16.md) |
| 7 | Mining & Upload (IPFS/Blockchain) | 4-10 days | MVP+ | [Phase 7 Tasks](./plans/plan-phase-7-tasks-2025-01-16.md) |

## How to Use This Documentation

### For Implementation
1. Start with [Implementation Roadmap](./plans/plan-implementation-roadmap-2025-01-16.md)
2. **Follow [Iterative Build Strategy](./plans/plan-iterative-build-strategy-2025-01-16.md) for step-by-step implementation**
3. Review [Subagent Workflow](./plans/plan-subagent-workflow-2025-01-16.md)
4. Follow phase-specific guides in `guides/`
5. Reference architecture docs as needed

### For Understanding the System
1. Read [Global Context Network Architecture](./architecture/architecture-global-context-network-2025-01-16.md)
2. Review [Architectural Decisions](./decisions/) for rationale
3. Explore component-specific architecture docs

### For Testing
1. Follow [TDD Workflow Guide](./guides/guide-tdd-workflow-2025-01-16.md)
2. Use [Testing Harness Usage](./guides/guide-testing-harness-usage-2025-01-16.md)
3. Reference [Testing Strategy](./reference/reference-testing-strategy-2025-01-16.md)

## Contributing

When adding documentation:

1. **Choose Category**: architecture, decisions, plans, guides, reference, or learnings
2. **Name Properly**: `category-topic-2025-01-16.md` format
3. **Use Template**: Follow markdown-organizer templates
4. **Add Frontmatter**: Include all required metadata
5. **Update Category INDEX**: Add entry to category's INDEX.md
6. **Update This File**: Add to "Recent Documents" and update counts
7. **Cross-Link**: Add "Related Documents" section with links

## Templates

All documentation follows standardized templates:
- **Architecture**: System design with diagrams, components, trade-offs
- **ADR**: Context, decision, consequences, alternatives
- **Plan**: Goals, tasks, risks, success criteria
- **Guide**: Step-by-step instructions with examples
- **Reference**: Technical specifications and API docs
- **Learning**: Insights with context and application

## Navigation Tips

- Use category INDEX.md files for complete listings
- Search by topic using your editor's find function
- Follow cross-links in "Related Documents" sections
- Check this master index for recent additions
- All paths are relative for easy navigation

## Project Links

- **GitHub**: [Repository URL when created]
- **Documentation**: This directory
- **Tests**: `/tests/` directory
- **Source**: `/src/` directory

## Status Legend

- 📝 **Planned**: Not yet started
- 🚧 **In Progress**: Currently being implemented
- ✅ **Complete**: Finished and validated
- 📦 **Archived**: Superseded or no longer active

---

*This documentation follows the markdown-organizer skill guidelines for consistent, discoverable, well-organized documentation.*
````

## File: STANDARDS.md
````markdown
# Global Context Network - Project Standards

> **CRITICAL**: All documentation and code MUST follow these standards to ensure consistency across the entire system.

---
**Date**: 2025-01-16
**Status**: CANONICAL - These are the single source of truth for all project standards
**Authority**: Established from review feedback (GPT-5, Gemini 2.5 Pro) to resolve contradictions

---

## 1. Privacy & Data Flow Standard (MOST CRITICAL)

### Rule: NEVER Persist Raw Data

**Canonical Privacy Flow**:
```
User Input → Hook Receives Event (in-memory)
           → Fast Pre-Sanitization (<50ms, rule-based)
           → Persist ONLY Sanitized Content to Database
           → Optional AI Validation (async, downstream)
```

**What This Means**:
- ✅ **DO**: Pre-sanitize synchronously in hook using fast regex rules
- ✅ **DO**: Only write sanitized content to disk (messages table)
- ✅ **DO**: Use in-memory buffers if needed (exempt from persistence)
- ❌ **NEVER**: Persist raw content to SQLite (not even temporarily)
- ❌ **NEVER**: Write raw content to event_queue, events table, or any disk storage
- ❌ **NEVER**: Log raw content before sanitization

**Why**: Zero-trust PII handling. If raw data never touches disk, we can guarantee no PII leaks.

---

## 2. Schema Standard (CANONICAL)

### Official Tables

**Production Tables**:
1. `conversations` - Conversation metadata
2. `messages` - Individual messages (SANITIZED content only)
3. `learnings` - Extracted insights
4. `job_queue` - Async job processing
5. `uploads` - Network upload status
6. `sanitization_log` - Audit trail

**Eliminated Tables**:
- ❌ NO `events` table
- ❌ NO `event_queue` table
- **Rationale**: These implied raw content persistence. We write sanitized directly to `messages`.

### Data Flow

```
Hook (sanitize) → messages table (sanitized content)
                → job_queue (jobs for AI validation, learning extraction, upload)
```

---

## 3. Status Enum Standard (CANONICAL)

All async operations use this vocabulary:

```typescript
type JobStatus =
  | 'queued'        // Initial state
  | 'in_progress'   // Worker claimed
  | 'completed'     // Succeeded
  | 'failed'        // Failed but retriable
  | 'dead_letter';  // Failed permanently
```

**Usage**:
- `job_queue.status`: Uses this enum
- Upload status: Uses this enum
- Worker states: Uses this enum

**Eliminated Alternatives**:
- ❌ NO "pending/processing" (use queued/in_progress)
- ❌ NO "running/succeeded" (use in_progress/completed)
- ❌ NO "quarantined" (use dead_letter)

---

## 4. ID Strategy Standard (CANONICAL)

### Use ULID Globally

```typescript
import { ulid } from 'ulid';

// All IDs use ULID
const conversationId = ulid(); // Lexicographically sortable, chronological
const messageId = ulid();
const learningId = ulid();
```

**Why ULID over UUID**:
- Chronologically sortable
- Lexicographic ordering matches creation time
- No need for separate timestamps in indexes
- Better database index performance

**Eliminated**: UUID v4 (not sortable)

---

## 5. Hook Configuration Standard (CANONICAL)

### Config File Path
```
.claude/hooks.json
```
**NOT**: `.claude/hooks/hooks.json` (eliminated)

### Hook IO Method
```typescript
// Hooks receive events via stdin (JSON)
const event = JSON.parse(await readStdin());
```
**NOT**: `process.argv` (eliminated)

### Hook Format
```json
{
  "hooks": {
    "UserPromptSubmit": ".claude/hooks/dist/userPromptSubmit.js",
    "Stop": ".claude/hooks/dist/stop.js"
  }
}
```

**CRITICAL**:
- ✅ Point to compiled `.js` files (in `dist/` or `build/`)
- ❌ NEVER use ts-node at runtime (too slow for <100ms budget)
- ✅ Build step: `tsc` compiles TypeScript to JavaScript
- ✅ Hooks run compiled JS only

---

## 6. Chain-of-Thought Standard (CANONICAL)

### Rule: NEVER Capture Chain-of-Thought

**What We Capture**:
- ✅ User prompts
- ✅ Assistant responses (visible output)
- ✅ Tool calls and results
- ✅ File operations
- ✅ Error messages

**What We NEVER Capture**:
- ❌ Hidden chain-of-thought (internal reasoning)
- ❌ "Thinking" blocks (if present)
- ❌ Internal model reasoning traces

**Why**:
1. Provider policy compliance
2. Privacy concerns
3. No reliable access anyway (Claude Code doesn't expose it)

**Action**: Remove all references to "thinking processes" or "reasoning capture" from docs.

---

## 7. Timestamp Standard (CANONICAL)

### Use ISO-8601 Strings

```typescript
// All timestamps in database
created_at: '2025-01-16T12:00:00.000Z'  // ISO-8601 UTC string

// Generated via:
new Date().toISOString()
```

**Why**:
- Human-readable
- Standard format
- Timezone-aware (always UTC with Z)
- SQLite text column compatible

**Eliminated**: Unix epoch milliseconds (not human-readable)

---

## 8. Import Standards

### Performance Timing
```typescript
import { performance } from 'node:perf_hooks';

const start = performance.now();
// ... work ...
const duration = performance.now() - start;
```
**NOT**: `performance.now()` without import (will error)

### Database
```typescript
import Database from 'better-sqlite3';

const db = new Database('context.db');
db.pragma('journal_mode = WAL');
db.pragma('synchronous = NORMAL');
db.pragma('foreign_keys = ON');
```

### IDs
```typescript
import { ulid } from 'ulid';

const id = ulid();
```

---

## 9. Sanitization Standard

### Fast Pre-Sanitization (In Hook)

**Budget**: <50ms (synchronous, rule-based)

**What to Redact**:
1. API keys (OpenAI, Anthropic, AWS, Google, GitHub, etc.)
2. Absolute file paths with usernames
3. Email addresses
4. IP addresses
5. Phone numbers
6. URLs with tokens/secrets
7. JWT tokens
8. Environment variable values
9. SSH keys, PEM blocks
10. Credit card numbers, SSNs

**Redaction Format**:
```typescript
"[REDACTED_API_KEY]"
"[REDACTED_EMAIL]"
"[REDACTED_PATH]"
"[REDACTED_IP]"
```

**Pseudonymization** (optional, session-scoped):
```typescript
"<EMAIL_1>", "<EMAIL_2>"  // Same email = same placeholder within session
"<PATH_1>", "<PATH_2>"    // Same path = same placeholder
```

### AI Validation (Async, Downstream)

**Budget**: <2s per conversation (async job)

**Purpose**:
- Catch context-aware PII (names that look like variables)
- Validate pre-sanitization caught everything
- Handle edge cases

**Not a replacement**: AI runs AFTER pre-sanitization, not instead of.

---

## 10. File Path Standards

### Hook Scripts
```
.claude/hooks/
  src/
    userPromptSubmit.ts
    stop.ts
  dist/              # Compiled output
    userPromptSubmit.js
    stop.js
  tsconfig.json
  package.json
```

### Project Structure
```
docs/
  architecture/
  decisions/
  plans/
  guides/
  reference/
  learnings/
  reviews/
src/
  hooks/
  sanitization/
  learning/
  mcp/
  database/
tests/
  unit/
  integration/
  e2e/
```

---

## 11. Testing Standards

### Coverage Requirements
- **Global**: ≥85% line coverage
- **Critical paths** (sanitization, hooks): ≥95% line + branch coverage
- **Learnings extraction**: ≥80%

### Test Types
- **70% Unit tests**: Isolated, fast, mocked
- **20% Integration tests**: Component interactions
- **10% E2E tests**: Full workflows

### Naming
```typescript
describe('sanitizeContent', () => {
  it('should redact API keys', () => {
    // arrange
    const input = 'key: sk-1234567890';

    // act
    const result = sanitizeContent(input);

    // assert
    expect(result).toBe('key: [REDACTED_API_KEY]');
  });
});
```

---

## 12. Performance Budgets

| Component | Budget | Measurement |
|-----------|--------|-------------|
| Hook execution | <100ms p95 | End-to-end (receive → sanitize → persist) |
| Fast sanitization | <50ms p95 | Regex-based rules only |
| Database writes | <20ms p95 | WAL-mode insert |
| Database queries | <100ms p95 | Indexed lookups |
| MCP queries | <200ms p95 | Search + serialization |
| AI sanitization | <2s p95 | Claude API call |
| Learning extraction | <5s p95 | Claude API call |

---

## 13. Error Handling Standards

### Hooks
```typescript
try {
  // Hook work
} catch (error) {
  // NEVER throw or block user
  logger.error('Hook failed', { error, event });
  // Fail silently
}
```

### Workers
```typescript
// Retry with exponential backoff
const maxAttempts = 3;
for (let attempt = 1; attempt <= maxAttempts; attempt++) {
  try {
    await processJob(job);
    break;
  } catch (error) {
    if (attempt === maxAttempts) {
      await moveToDeadLetter(job, error);
    } else {
      await sleep(2 ** attempt * 1000);
    }
  }
}
```

---

## 14. Logging Standards

### Structured Logging
```typescript
logger.info('Event captured', {
  conversation_id: conversationId,
  message_id: messageId,
  role: 'user',
  content_length: sanitizedContent.length,  // NEVER log raw content
  duration_ms: duration
});
```

### Privacy in Logs
- ❌ NEVER log raw content
- ❌ NEVER log PII before sanitization
- ✅ Log sanitized content (optional, for debugging)
- ✅ Log metadata (IDs, lengths, durations)

---

## 15. Blockchain Standard (Clarified)

### EVM Chain Selection (Not Celestia)

**For MVP**:
- Target: Ethereum L2 (Base, Arbitrum, Optimism, or Polygon)
- **NOT Celestia**: Celestia is data availability layer, not EVM-compatible

**Celestia Usage** (optional, future):
- Can use for data availability (content commitments)
- Requires separate integration

**Clarification**:
- Smart contracts = EVM chain
- Data availability = Celestia
- Don't conflate the two

---

## 16. Consent & Licensing Standard

### Default Behavior
- **Default**: Local-only mode (no uploads)
- **Opt-in required**: User must explicitly enable global sharing
- **Manual approval gate**: Each upload requires confirmation (MVP)

### License for Learnings
- **Recommended**: CC BY 4.0 or ODC-By for shared learnings
- **Prohibited**: Sharing raw conversations (only derived learnings)

**ADR Required**: ADR-007 must formalize this

---

## 17. Documentation Standards

### File Naming
```
category-topic-YYYY-MM-DD.md

Examples:
- architecture-sanitization-pipeline-2025-01-16.md
- decision-use-ulid-2025-01-16.md
- guide-hook-development-2025-01-16.md
```

### Frontmatter (Required)
```yaml
---
title: Document Title
category: architecture|decision|plan|guide|reference|learning
date: 2025-01-16
status: active|draft|archived
authors: Name(s)
tags: [tag1, tag2]
---
```

### Cross-Linking
- Always link to related docs in "Related Documents" section
- Use relative paths: `../architecture/file.md`
- Update category INDEX.md when adding files

---

## 18. Code Standards

### TypeScript Strict Mode
```json
{
  "compilerOptions": {
    "strict": true,
    "noImplicitAny": true,
    "strictNullChecks": true,
    "strictFunctionTypes": true
  }
}
```

### No `any` Types
```typescript
// ❌ BAD
function process(data: any) { }

// ✅ GOOD
function process(data: unknown) {
  if (typeof data === 'string') {
    // Type guard
  }
}

// ✅ BETTER
interface EventData {
  role: string;
  content: string;
}
function process(data: EventData) { }
```

---

## Enforcement

### All New Documents MUST:
1. Reference this STANDARDS.md file
2. Use canonical schema (messages + job_queue)
3. Use canonical status enums (queued → in_progress → completed → failed → dead_letter)
4. Use ULID for all IDs
5. Follow privacy flow (pre-sanitize, never persist raw)
6. Exclude chain-of-thought universally
7. Use .claude/hooks.json path
8. Use compiled .js hooks
9. Use ISO-8601 timestamps
10. Follow performance budgets

### Review Checklist

Before any document is finalized:
- [ ] Uses canonical schema (no events/event_queue tables)
- [ ] Uses canonical status enums
- [ ] Uses ULID for IDs
- [ ] Pre-sanitizes in hook (never persists raw)
- [ ] Excludes chain-of-thought
- [ ] Uses .claude/hooks.json config path
- [ ] References compiled .js hooks (not ts-node)
- [ ] Uses ISO-8601 timestamps
- [ ] Performance budgets specified
- [ ] Privacy guarantees maintained

---

## Related Documents

- [ADR-004: Sanitize Before Storage](./decisions/decision-sanitize-before-storage-2025-01-16.md)
- [Database Schema Reference](./reference/reference-database-schema-2025-01-16.md)
- [Architecture: Hooks & Event Capture](./architecture/architecture-hooks-event-capture-2025-01-16.md)

---

*This document is the canonical source of truth for all project standards. When in doubt, refer here. If standards conflict with other docs, THIS document wins.*
````
